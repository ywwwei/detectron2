srun: job 316296 queued and waiting for resources
srun: job 316296 has been allocated resources
[02/27 19:16:40 detectron2]: Rank of current process: 0. World size: 4
[02/27 19:16:44 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/27 19:16:44 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/27 19:16:44 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader


model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
train.output_dir = f"/srv/home/pmorgado/yibing/output/mae2cl_detection/bs{dataloader.train.total_batch_size}_mae"
train.init_checkpoint = (
   "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
)
train.num_workers = 2


# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}

[02/27 19:16:45 detectron2]: Full config saved to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/config.yaml
[02/27 19:16:45 d2.utils.env]: Using a generated random seed 47501817
[02/27 19:16:57 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
slurmstepd: error: *** STEP 316296.0 ON euler22 CANCELLED AT 2023-02-27T19:17:16 ***
srun: error: euler22: task 0: Terminated
srun: Force Terminated StepId=316296.0
srun: job 316297 queued and waiting for resources
srun: job 316297 has been allocated resources
[02/27 19:17:55 detectron2]: Rank of current process: 0. World size: 4
[02/27 19:17:58 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/27 19:17:58 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/27 19:17:58 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader


model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
train.output_dir = f"/srv/home/pmorgado/yibing/output/mae2cl_detection/bs{dataloader.train.total_batch_size}_mae"
train.init_checkpoint = (
   "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
)
train.num_workers = 2


# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}

[02/27 19:17:58 detectron2]: Full config saved to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/config.yaml
[02/27 19:17:58 d2.utils.env]: Using a generated random seed 60912777
[02/27 19:18:05 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[02/27 19:18:42 d2.data.datasets.coco]: Loading /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json takes 37.12 seconds.
[02/27 19:18:44 d2.data.datasets.coco]: Loaded 118287 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json
[02/27 19:19:02 d2.data.build]: Removed 1021 images with no usable annotations. 117266 images left.
[02/27 19:19:14 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |
[02/27 19:19:14 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=[1024, 1024], pad=False)]
[02/27 19:19:14 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/27 19:19:14 d2.data.common]: Serializing 117266 elements to byte tensors and concatenating them all ...
[02/27 19:19:21 d2.data.common]: Serialized dataset takes 453.34 MiB
[02/27 19:19:30 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest.pth ...
[02/27 19:19:30 fvcore.common.checkpoint]: [Checkpointer] Loading from /srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest.pth ...
checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])
after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])
WARNING [02/27 19:19:31 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
backbone.net.blocks.0.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.1.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.10.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.11.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.2.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.3.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.4.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.5.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.6.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.7.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.8.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.9.attn.{rel_pos_h, rel_pos_w}
backbone.simfp_2.0.{bias, weight}
backbone.simfp_2.1.{bias, weight}
backbone.simfp_2.3.{bias, weight}
backbone.simfp_2.4.norm.{bias, weight}
backbone.simfp_2.4.weight
backbone.simfp_2.5.norm.{bias, weight}
backbone.simfp_2.5.weight
backbone.simfp_3.0.{bias, weight}
backbone.simfp_3.1.norm.{bias, weight}
backbone.simfp_3.1.weight
backbone.simfp_3.2.norm.{bias, weight}
backbone.simfp_3.2.weight
backbone.simfp_4.0.norm.{bias, weight}
backbone.simfp_4.0.weight
backbone.simfp_4.1.norm.{bias, weight}
backbone.simfp_4.1.weight
backbone.simfp_5.1.norm.{bias, weight}
backbone.simfp_5.1.weight
backbone.simfp_5.2.norm.{bias, weight}
backbone.simfp_5.2.weight
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.conv0.{bias, weight}
proposal_generator.rpn_head.conv.conv1.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_head.conv1.norm.{bias, weight}
roi_heads.box_head.conv1.weight
roi_heads.box_head.conv2.norm.{bias, weight}
roi_heads.box_head.conv2.weight
roi_heads.box_head.conv3.norm.{bias, weight}
roi_heads.box_head.conv3.weight
roi_heads.box_head.conv4.norm.{bias, weight}
roi_heads.box_head.conv4.weight
roi_heads.box_head.fc1.{bias, weight}
roi_heads.box_predictor.bbox_pred.{bias, weight}
roi_heads.box_predictor.cls_score.{bias, weight}
roi_heads.mask_head.deconv.{bias, weight}
roi_heads.mask_head.mask_fcn1.norm.{bias, weight}
roi_heads.mask_head.mask_fcn1.weight
roi_heads.mask_head.mask_fcn2.norm.{bias, weight}
roi_heads.mask_head.mask_fcn2.weight
roi_heads.mask_head.mask_fcn3.norm.{bias, weight}
roi_heads.mask_head.mask_fcn3.weight
roi_heads.mask_head.mask_fcn4.norm.{bias, weight}
roi_heads.mask_head.mask_fcn4.weight
roi_heads.mask_head.predictor.{bias, weight}
WARNING [02/27 19:19:31 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  backbone.net.{cls_token, mask_token}
  backbone.net.norm.{bias, weight}

/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))

/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[02/27 19:19:31 d2.engine.train_loop]: Starting training from iteration 0

/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[02/27 19:20:09 d2.utils.events]:  eta: 1 day, 17:01:08  iter: 19  total_loss: 5.424  loss_cls: 3.766  loss_box_reg: 0.03237  loss_mask: 0.6932  loss_rpn_cls: 0.6879  loss_rpn_loc: 0.2217  time: 0.8033  last_time: 0.7673  data_time: 0.9172  last_data_time: 0.0312   lr: 7.6924e-06  max_mem: 10908M
[02/27 19:20:24 d2.utils.events]:  eta: 1 day, 16:43:33  iter: 39  total_loss: 2.341  loss_cls: 0.5753  loss_box_reg: 0.08227  loss_mask: 0.693  loss_rpn_cls: 0.6736  loss_rpn_loc: 0.2029  time: 0.7933  last_time: 0.7168  data_time: 0.0365  last_data_time: 0.0345   lr: 1.5684e-05  max_mem: 11001M
[02/27 19:20:40 d2.utils.events]:  eta: 1 day, 16:35:46  iter: 59  total_loss: 2.066  loss_cls: 0.3975  loss_box_reg: 0.1398  loss_mask: 0.6927  loss_rpn_cls: 0.6191  loss_rpn_loc: 0.2156  time: 0.7941  last_time: 0.7943  data_time: 0.0344  last_data_time: 0.0121   lr: 2.3676e-05  max_mem: 11101M
slurmstepd: error: *** STEP 316297.0 ON euler22 CANCELLED AT 2023-02-27T19:20:53 ***
srun: error: euler22: task 0: Terminated
srun: Force Terminated StepId=316297.0
srun: error: Invalid numeric value "64tools/lazyconfig_train_net.py" for --cpus-per-task.
srun: job 316298 queued and waiting for resources
srun: job 316298 has been allocated resources
[02/27 19:21:44 detectron2]: Rank of current process: 0. World size: 4
[02/27 19:21:46 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/27 19:21:46 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/27 19:21:46 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader


model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
train.output_dir = f"/srv/home/pmorgado/yibing/output/mae2cl_detection/bs{dataloader.train.total_batch_size}_mae"
train.init_checkpoint = (
   "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
)
train.num_workers = 2


# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}

[02/27 19:21:46 detectron2]: Full config saved to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/config.yaml
[02/27 19:21:46 d2.utils.env]: Using a generated random seed 48581364
[02/27 19:21:48 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[02/27 19:22:02 d2.data.datasets.coco]: Loading /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json takes 13.11 seconds.
[02/27 19:22:02 d2.data.datasets.coco]: Loaded 118287 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json
[02/27 19:22:09 d2.data.build]: Removed 1021 images with no usable annotations. 117266 images left.
[02/27 19:22:12 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |
[02/27 19:22:12 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=[1024, 1024], pad=False)]
[02/27 19:22:12 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/27 19:22:12 d2.data.common]: Serializing 117266 elements to byte tensors and concatenating them all ...
[02/27 19:22:15 d2.data.common]: Serialized dataset takes 453.34 MiB
[02/27 19:22:24 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest.pth ...
[02/27 19:22:24 fvcore.common.checkpoint]: [Checkpointer] Loading from /srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest.pth ...
checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])
after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])

checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['module.cls_token', 'module.pos_embed', 'module.mask_token', 'module.decoder_pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias', 'module.blocks.0.attn.qkv.weight', 'module.blocks.0.attn.qkv.bias', 'module.blocks.0.attn.proj.weight', 'module.blocks.0.attn.proj.bias', 'module.blocks.0.norm2.weight', 'module.blocks.0.norm2.bias', 'module.blocks.0.mlp.fc1.weight', 'module.blocks.0.mlp.fc1.bias', 'module.blocks.0.mlp.fc2.weight', 'module.blocks.0.mlp.fc2.bias', 'module.blocks.1.norm1.weight', 'module.blocks.1.norm1.bias', 'module.blocks.1.attn.qkv.weight', 'module.blocks.1.attn.qkv.bias', 'module.blocks.1.attn.proj.weight', 'module.blocks.1.attn.proj.bias', 'module.blocks.1.norm2.weight', 'module.blocks.1.norm2.bias', 'module.blocks.1.mlp.fc1.weight', 'module.blocks.1.mlp.fc1.bias', 'module.blocks.1.mlp.fc2.weight', 'module.blocks.1.mlp.fc2.bias', 'module.blocks.2.norm1.weight', 'module.blocks.2.norm1.bias', 'module.blocks.2.attn.qkv.weight', 'module.blocks.2.attn.qkv.bias', 'module.blocks.2.attn.proj.weight', 'module.blocks.2.attn.proj.bias', 'module.blocks.2.norm2.weight', 'module.blocks.2.norm2.bias', 'module.blocks.2.mlp.fc1.weight', 'module.blocks.2.mlp.fc1.bias', 'module.blocks.2.mlp.fc2.weight', 'module.blocks.2.mlp.fc2.bias', 'module.blocks.3.norm1.weight', 'module.blocks.3.norm1.bias', 'module.blocks.3.attn.qkv.weight', 'module.blocks.3.attn.qkv.bias', 'module.blocks.3.attn.proj.weight', 'module.blocks.3.attn.proj.bias', 'module.blocks.3.norm2.weight', 'module.blocks.3.norm2.bias', 'module.blocks.3.mlp.fc1.weight', 'module.blocks.3.mlp.fc1.bias', 'module.blocks.3.mlp.fc2.weight', 'module.blocks.3.mlp.fc2.bias', 'module.blocks.4.norm1.weight', 'module.blocks.4.norm1.bias', 'module.blocks.4.attn.qkv.weight', 'module.blocks.4.attn.qkv.bias', 'module.blocks.4.attn.proj.weight', 'module.blocks.4.attn.proj.bias', 'module.blocks.4.norm2.weight', 'module.blocks.4.norm2.bias', 'module.blocks.4.mlp.fc1.weight', 'module.blocks.4.mlp.fc1.bias', 'module.blocks.4.mlp.fc2.weight', 'module.blocks.4.mlp.fc2.bias', 'module.blocks.5.norm1.weight', 'module.blocks.5.norm1.bias', 'module.blocks.5.attn.qkv.weight', 'module.blocks.5.attn.qkv.bias', 'module.blocks.5.attn.proj.weight', 'module.blocks.5.attn.proj.bias', 'module.blocks.5.norm2.weight', 'module.blocks.5.norm2.bias', 'module.blocks.5.mlp.fc1.weight', 'module.blocks.5.mlp.fc1.bias', 'module.blocks.5.mlp.fc2.weight', 'module.blocks.5.mlp.fc2.bias', 'module.blocks.6.norm1.weight', 'module.blocks.6.norm1.bias', 'module.blocks.6.attn.qkv.weight', 'module.blocks.6.attn.qkv.bias', 'module.blocks.6.attn.proj.weight', 'module.blocks.6.attn.proj.bias', 'module.blocks.6.norm2.weight', 'module.blocks.6.norm2.bias', 'module.blocks.6.mlp.fc1.weight', 'module.blocks.6.mlp.fc1.bias', 'module.blocks.6.mlp.fc2.weight', 'module.blocks.6.mlp.fc2.bias', 'module.blocks.7.norm1.weight', 'module.blocks.7.norm1.bias', 'module.blocks.7.attn.qkv.weight', 'module.blocks.7.attn.qkv.bias', 'module.blocks.7.attn.proj.weight', 'module.blocks.7.attn.proj.bias', 'module.blocks.7.norm2.weight', 'module.blocks.7.norm2.bias', 'module.blocks.7.mlp.fc1.weight', 'module.blocks.7.mlp.fc1.bias', 'module.blocks.7.mlp.fc2.weight', 'module.blocks.7.mlp.fc2.bias', 'module.blocks.8.norm1.weight', 'module.blocks.8.norm1.bias', 'module.blocks.8.attn.qkv.weight', 'module.blocks.8.attn.qkv.bias', 'module.blocks.8.attn.proj.weight', 'module.blocks.8.attn.proj.bias', 'module.blocks.8.norm2.weight', 'module.blocks.8.norm2.bias', 'module.blocks.8.mlp.fc1.weight', 'module.blocks.8.mlp.fc1.bias', 'module.blocks.8.mlp.fc2.weight', 'module.blocks.8.mlp.fc2.bias', 'module.blocks.9.norm1.weight', 'module.blocks.9.norm1.bias', 'module.blocks.9.attn.qkv.weight', 'module.blocks.9.attn.qkv.bias', 'module.blocks.9.attn.proj.weight', 'module.blocks.9.attn.proj.bias', 'module.blocks.9.norm2.weight', 'module.blocks.9.norm2.bias', 'module.blocks.9.mlp.fc1.weight', 'module.blocks.9.mlp.fc1.bias', 'module.blocks.9.mlp.fc2.weight', 'module.blocks.9.mlp.fc2.bias', 'module.blocks.10.norm1.weight', 'module.blocks.10.norm1.bias', 'module.blocks.10.attn.qkv.weight', 'module.blocks.10.attn.qkv.bias', 'module.blocks.10.attn.proj.weight', 'module.blocks.10.attn.proj.bias', 'module.blocks.10.norm2.weight', 'module.blocks.10.norm2.bias', 'module.blocks.10.mlp.fc1.weight', 'module.blocks.10.mlp.fc1.bias', 'module.blocks.10.mlp.fc2.weight', 'module.blocks.10.mlp.fc2.bias', 'module.blocks.11.norm1.weight', 'module.blocks.11.norm1.bias', 'module.blocks.11.attn.qkv.weight', 'module.blocks.11.attn.qkv.bias', 'module.blocks.11.attn.proj.weight', 'module.blocks.11.attn.proj.bias', 'module.blocks.11.norm2.weight', 'module.blocks.11.norm2.bias', 'module.blocks.11.mlp.fc1.weight', 'module.blocks.11.mlp.fc1.bias', 'module.blocks.11.mlp.fc2.weight', 'module.blocks.11.mlp.fc2.bias', 'module.norm.weight', 'module.norm.bias', 'module.decoder_embed.weight', 'module.decoder_embed.bias', 'module.decoder_blocks.0.norm1.weight', 'module.decoder_blocks.0.norm1.bias', 'module.decoder_blocks.0.attn.qkv.weight', 'module.decoder_blocks.0.attn.qkv.bias', 'module.decoder_blocks.0.attn.proj.weight', 'module.decoder_blocks.0.attn.proj.bias', 'module.decoder_blocks.0.norm2.weight', 'module.decoder_blocks.0.norm2.bias', 'module.decoder_blocks.0.mlp.fc1.weight', 'module.decoder_blocks.0.mlp.fc1.bias', 'module.decoder_blocks.0.mlp.fc2.weight', 'module.decoder_blocks.0.mlp.fc2.bias', 'module.decoder_blocks.1.norm1.weight', 'module.decoder_blocks.1.norm1.bias', 'module.decoder_blocks.1.attn.qkv.weight', 'module.decoder_blocks.1.attn.qkv.bias', 'module.decoder_blocks.1.attn.proj.weight', 'module.decoder_blocks.1.attn.proj.bias', 'module.decoder_blocks.1.norm2.weight', 'module.decoder_blocks.1.norm2.bias', 'module.decoder_blocks.1.mlp.fc1.weight', 'module.decoder_blocks.1.mlp.fc1.bias', 'module.decoder_blocks.1.mlp.fc2.weight', 'module.decoder_blocks.1.mlp.fc2.bias', 'module.decoder_blocks.2.norm1.weight', 'module.decoder_blocks.2.norm1.bias', 'module.decoder_blocks.2.attn.qkv.weight', 'module.decoder_blocks.2.attn.qkv.bias', 'module.decoder_blocks.2.attn.proj.weight', 'module.decoder_blocks.2.attn.proj.bias', 'module.decoder_blocks.2.norm2.weight', 'module.decoder_blocks.2.norm2.bias', 'module.decoder_blocks.2.mlp.fc1.weight', 'module.decoder_blocks.2.mlp.fc1.bias', 'module.decoder_blocks.2.mlp.fc2.weight', 'module.decoder_blocks.2.mlp.fc2.bias', 'module.decoder_blocks.3.norm1.weight', 'module.decoder_blocks.3.norm1.bias', 'module.decoder_blocks.3.attn.qkv.weight', 'module.decoder_blocks.3.attn.qkv.bias', 'module.decoder_blocks.3.attn.proj.weight', 'module.decoder_blocks.3.attn.proj.bias', 'module.decoder_blocks.3.norm2.weight', 'module.decoder_blocks.3.norm2.bias', 'module.decoder_blocks.3.mlp.fc1.weight', 'module.decoder_blocks.3.mlp.fc1.bias', 'module.decoder_blocks.3.mlp.fc2.weight', 'module.decoder_blocks.3.mlp.fc2.bias', 'module.decoder_blocks.4.norm1.weight', 'module.decoder_blocks.4.norm1.bias', 'module.decoder_blocks.4.attn.qkv.weight', 'module.decoder_blocks.4.attn.qkv.bias', 'module.decoder_blocks.4.attn.proj.weight', 'module.decoder_blocks.4.attn.proj.bias', 'module.decoder_blocks.4.norm2.weight', 'module.decoder_blocks.4.norm2.bias', 'module.decoder_blocks.4.mlp.fc1.weight', 'module.decoder_blocks.4.mlp.fc1.bias', 'module.decoder_blocks.4.mlp.fc2.weight', 'module.decoder_blocks.4.mlp.fc2.bias', 'module.decoder_blocks.5.norm1.weight', 'module.decoder_blocks.5.norm1.bias', 'module.decoder_blocks.5.attn.qkv.weight', 'module.decoder_blocks.5.attn.qkv.bias', 'module.decoder_blocks.5.attn.proj.weight', 'module.decoder_blocks.5.attn.proj.bias', 'module.decoder_blocks.5.norm2.weight', 'module.decoder_blocks.5.norm2.bias', 'module.decoder_blocks.5.mlp.fc1.weight', 'module.decoder_blocks.5.mlp.fc1.bias', 'module.decoder_blocks.5.mlp.fc2.weight', 'module.decoder_blocks.5.mlp.fc2.bias', 'module.decoder_blocks.6.norm1.weight', 'module.decoder_blocks.6.norm1.bias', 'module.decoder_blocks.6.attn.qkv.weight', 'module.decoder_blocks.6.attn.qkv.bias', 'module.decoder_blocks.6.attn.proj.weight', 'module.decoder_blocks.6.attn.proj.bias', 'module.decoder_blocks.6.norm2.weight', 'module.decoder_blocks.6.norm2.bias', 'module.decoder_blocks.6.mlp.fc1.weight', 'module.decoder_blocks.6.mlp.fc1.bias', 'module.decoder_blocks.6.mlp.fc2.weight', 'module.decoder_blocks.6.mlp.fc2.bias', 'module.decoder_blocks.7.norm1.weight', 'module.decoder_blocks.7.norm1.bias', 'module.decoder_blocks.7.attn.qkv.weight', 'module.decoder_blocks.7.attn.qkv.bias', 'module.decoder_blocks.7.attn.proj.weight', 'module.decoder_blocks.7.attn.proj.bias', 'module.decoder_blocks.7.norm2.weight', 'module.decoder_blocks.7.norm2.bias', 'module.decoder_blocks.7.mlp.fc1.weight', 'module.decoder_blocks.7.mlp.fc1.bias', 'module.decoder_blocks.7.mlp.fc2.weight', 'module.decoder_blocks.7.mlp.fc2.bias', 'module.decoder_norm.weight', 'module.decoder_norm.bias', 'module.decoder_pred.weight', 'module.decoder_pred.bias']

after checkpoint_state_dict ['backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.patch_embed.proj.weight', 'backbone.net.pos_embed']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.bias', 'backbone.net.norm.weight'], incorrect_shapes=[])
WARNING [02/27 19:22:25 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
backbone.net.blocks.0.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.1.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.10.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.11.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.2.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.3.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.4.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.5.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.6.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.7.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.8.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.9.attn.{rel_pos_h, rel_pos_w}
backbone.simfp_2.0.{bias, weight}
backbone.simfp_2.1.{bias, weight}
backbone.simfp_2.3.{bias, weight}
backbone.simfp_2.4.norm.{bias, weight}
backbone.simfp_2.4.weight
backbone.simfp_2.5.norm.{bias, weight}
backbone.simfp_2.5.weight
backbone.simfp_3.0.{bias, weight}
backbone.simfp_3.1.norm.{bias, weight}
backbone.simfp_3.1.weight
backbone.simfp_3.2.norm.{bias, weight}
backbone.simfp_3.2.weight
backbone.simfp_4.0.norm.{bias, weight}
backbone.simfp_4.0.weight
backbone.simfp_4.1.norm.{bias, weight}
backbone.simfp_4.1.weight
backbone.simfp_5.1.norm.{bias, weight}
backbone.simfp_5.1.weight
backbone.simfp_5.2.norm.{bias, weight}
backbone.simfp_5.2.weight
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.conv0.{bias, weight}
proposal_generator.rpn_head.conv.conv1.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_head.conv1.norm.{bias, weight}
roi_heads.box_head.conv1.weight
roi_heads.box_head.conv2.norm.{bias, weight}
roi_heads.box_head.conv2.weight
roi_heads.box_head.conv3.norm.{bias, weight}
roi_heads.box_head.conv3.weight
roi_heads.box_head.conv4.norm.{bias, weight}
roi_heads.box_head.conv4.weight
roi_heads.box_head.fc1.{bias, weight}
roi_heads.box_predictor.bbox_pred.{bias, weight}
roi_heads.box_predictor.cls_score.{bias, weight}
roi_heads.mask_head.deconv.{bias, weight}
roi_heads.mask_head.mask_fcn1.norm.{bias, weight}
roi_heads.mask_head.mask_fcn1.weight
roi_heads.mask_head.mask_fcn2.norm.{bias, weight}
roi_heads.mask_head.mask_fcn2.weight
roi_heads.mask_head.mask_fcn3.norm.{bias, weight}
roi_heads.mask_head.mask_fcn3.weight
roi_heads.mask_head.mask_fcn4.norm.{bias, weight}
roi_heads.mask_head.mask_fcn4.weight
roi_heads.mask_head.predictor.{bias, weight}
WARNING [02/27 19:22:25 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  backbone.net.{cls_token, mask_token}
  backbone.net.norm.{bias, weight}
[02/27 19:22:25 d2.engine.train_loop]: Starting training from iteration 0
[02/27 19:22:44 d2.utils.events]:  eta: 1 day, 5:13:03  iter: 19  total_loss: 5.476  loss_cls: 3.843  loss_box_reg: 0.02227  loss_mask: 0.6931  loss_rpn_cls: 0.692  loss_rpn_loc: 0.1048  time: 0.5632  last_time: 0.5763  data_time: 0.3101  last_data_time: 0.1115   lr: 7.6924e-06  max_mem: 10995M
[02/27 19:22:56 d2.utils.events]:  eta: 1 day, 5:15:52  iter: 39  total_loss: 2.484  loss_cls: 0.7984  loss_box_reg: 0.09513  loss_mask: 0.6929  loss_rpn_cls: 0.6763  loss_rpn_loc: 0.1596  time: 0.5753  last_time: 0.7026  data_time: 0.0354  last_data_time: 0.1364   lr: 1.5684e-05  max_mem: 11149M
[02/27 19:23:08 d2.utils.events]:  eta: 1 day, 5:19:51  iter: 59  total_loss: 2.003  loss_cls: 0.3375  loss_box_reg: 0.1263  loss_mask: 0.6926  loss_rpn_cls: 0.6257  loss_rpn_loc: 0.1799  time: 0.5859  last_time: 0.5645  data_time: 0.0298  last_data_time: 0.0056   lr: 2.3676e-05  max_mem: 11149M
[02/27 19:23:19 d2.utils.events]:  eta: 1 day, 5:08:18  iter: 79  total_loss: 1.724  loss_cls: 0.3538  loss_box_reg: 0.132  loss_mask: 0.6924  loss_rpn_cls: 0.4279  loss_rpn_loc: 0.1679  time: 0.5789  last_time: 0.5677  data_time: 0.0118  last_data_time: 0.0457   lr: 3.1668e-05  max_mem: 11278M
[02/27 19:23:30 d2.utils.events]:  eta: 1 day, 4:55:01  iter: 99  total_loss: 1.619  loss_cls: 0.3897  loss_box_reg: 0.1849  loss_mask: 0.6917  loss_rpn_cls: 0.2013  loss_rpn_loc: 0.1269  time: 0.5701  last_time: 0.5420  data_time: 0.0081  last_data_time: 0.0128   lr: 3.966e-05  max_mem: 11278M
[02/27 19:23:41 d2.utils.events]:  eta: 1 day, 4:52:42  iter: 119  total_loss: 1.764  loss_cls: 0.4325  loss_box_reg: 0.2324  loss_mask: 0.6908  loss_rpn_cls: 0.2675  loss_rpn_loc: 0.2138  time: 0.5664  last_time: 0.5576  data_time: 0.0220  last_data_time: 0.0476   lr: 4.7652e-05  max_mem: 11355M
[02/27 19:23:52 d2.utils.events]:  eta: 1 day, 4:35:47  iter: 139  total_loss: 1.784  loss_cls: 0.4171  loss_box_reg: 0.2004  loss_mask: 0.6892  loss_rpn_cls: 0.2042  loss_rpn_loc: 0.2217  time: 0.5629  last_time: 0.5349  data_time: 0.0145  last_data_time: 0.0052   lr: 5.5644e-05  max_mem: 11355M
[02/27 19:24:02 d2.utils.events]:  eta: 1 day, 4:26:26  iter: 159  total_loss: 1.762  loss_cls: 0.4368  loss_box_reg: 0.2339  loss_mask: 0.6823  loss_rpn_cls: 0.1621  loss_rpn_loc: 0.2043  time: 0.5593  last_time: 0.5499  data_time: 0.0055  last_data_time: 0.0040   lr: 6.3636e-05  max_mem: 11356M
[02/27 19:24:13 d2.utils.events]:  eta: 1 day, 4:08:19  iter: 179  total_loss: 1.839  loss_cls: 0.5091  loss_box_reg: 0.2519  loss_mask: 0.6781  loss_rpn_cls: 0.1588  loss_rpn_loc: 0.1688  time: 0.5562  last_time: 0.5400  data_time: 0.0140  last_data_time: 0.0123   lr: 7.1628e-05  max_mem: 11356M
[02/27 19:24:24 d2.utils.events]:  eta: 1 day, 3:59:19  iter: 199  total_loss: 1.587  loss_cls: 0.4059  loss_box_reg: 0.1999  loss_mask: 0.6724  loss_rpn_cls: 0.1412  loss_rpn_loc: 0.2063  time: 0.5541  last_time: 0.5365  data_time: 0.0295  last_data_time: 0.0246   lr: 7.962e-05  max_mem: 11369M
[02/27 19:24:34 d2.utils.events]:  eta: 1 day, 3:49:26  iter: 219  total_loss: 1.612  loss_cls: 0.3669  loss_box_reg: 0.1968  loss_mask: 0.6492  loss_rpn_cls: 0.1517  loss_rpn_loc: 0.2036  time: 0.5515  last_time: 0.4988  data_time: 0.0076  last_data_time: 0.0048   lr: 8.7612e-05  max_mem: 11369M
[02/27 19:24:45 d2.utils.events]:  eta: 1 day, 3:44:03  iter: 239  total_loss: 1.618  loss_cls: 0.3802  loss_box_reg: 0.1998  loss_mask: 0.6458  loss_rpn_cls: 0.125  loss_rpn_loc: 0.283  time: 0.5497  last_time: 0.5485  data_time: 0.0187  last_data_time: 0.0135   lr: 9.5604e-05  max_mem: 11369M
[02/27 19:24:56 d2.utils.events]:  eta: 1 day, 3:39:39  iter: 259  total_loss: 1.659  loss_cls: 0.4375  loss_box_reg: 0.236  loss_mask: 0.6197  loss_rpn_cls: 0.1326  loss_rpn_loc: 0.1802  time: 0.5485  last_time: 0.5188  data_time: 0.0134  last_data_time: 0.0117   lr: 0.0001  max_mem: 11369M
[02/27 19:25:07 d2.utils.events]:  eta: 1 day, 3:38:41  iter: 279  total_loss: 1.527  loss_cls: 0.3351  loss_box_reg: 0.1848  loss_mask: 0.6228  loss_rpn_cls: 0.123  loss_rpn_loc: 0.2251  time: 0.5474  last_time: 0.5206  data_time: 0.0161  last_data_time: 0.0034   lr: 0.0001  max_mem: 11369M
[02/27 19:25:17 d2.utils.events]:  eta: 1 day, 3:36:15  iter: 299  total_loss: 1.65  loss_cls: 0.4033  loss_box_reg: 0.1901  loss_mask: 0.6274  loss_rpn_cls: 0.1344  loss_rpn_loc: 0.184  time: 0.5459  last_time: 0.5294  data_time: 0.0134  last_data_time: 0.0044   lr: 0.0001  max_mem: 11369M
[02/27 19:25:28 d2.utils.events]:  eta: 1 day, 3:32:50  iter: 319  total_loss: 1.427  loss_cls: 0.3728  loss_box_reg: 0.204  loss_mask: 0.6191  loss_rpn_cls: 0.1389  loss_rpn_loc: 0.1829  time: 0.5439  last_time: 0.4961  data_time: 0.0138  last_data_time: 0.0191   lr: 0.0001  max_mem: 11521M
[02/27 19:25:38 d2.utils.events]:  eta: 1 day, 3:27:04  iter: 339  total_loss: 1.444  loss_cls: 0.3532  loss_box_reg: 0.1567  loss_mask: 0.5819  loss_rpn_cls: 0.141  loss_rpn_loc: 0.2298  time: 0.5418  last_time: 0.5231  data_time: 0.0136  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 19:25:48 d2.utils.events]:  eta: 1 day, 3:25:58  iter: 359  total_loss: 1.375  loss_cls: 0.2983  loss_box_reg: 0.1534  loss_mask: 0.562  loss_rpn_cls: 0.1333  loss_rpn_loc: 0.1607  time: 0.5409  last_time: 0.5052  data_time: 0.0144  last_data_time: 0.0095   lr: 0.0001  max_mem: 11521M
[02/27 19:25:59 d2.utils.events]:  eta: 1 day, 3:24:58  iter: 379  total_loss: 1.518  loss_cls: 0.3825  loss_box_reg: 0.2127  loss_mask: 0.5952  loss_rpn_cls: 0.1385  loss_rpn_loc: 0.1598  time: 0.5402  last_time: 0.5435  data_time: 0.0095  last_data_time: 0.0052   lr: 0.0001  max_mem: 11521M
[02/27 19:26:09 d2.utils.events]:  eta: 1 day, 3:19:11  iter: 399  total_loss: 1.418  loss_cls: 0.3436  loss_box_reg: 0.2009  loss_mask: 0.5369  loss_rpn_cls: 0.1299  loss_rpn_loc: 0.1133  time: 0.5392  last_time: 0.5041  data_time: 0.0171  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 19:26:20 d2.utils.events]:  eta: 1 day, 3:17:40  iter: 419  total_loss: 1.485  loss_cls: 0.37  loss_box_reg: 0.2065  loss_mask: 0.5704  loss_rpn_cls: 0.1148  loss_rpn_loc: 0.1539  time: 0.5385  last_time: 0.5133  data_time: 0.0216  last_data_time: 0.0541   lr: 0.0001  max_mem: 11521M
[02/27 19:26:30 d2.utils.events]:  eta: 1 day, 3:15:50  iter: 439  total_loss: 1.538  loss_cls: 0.4355  loss_box_reg: 0.2481  loss_mask: 0.5477  loss_rpn_cls: 0.1166  loss_rpn_loc: 0.158  time: 0.5374  last_time: 0.5167  data_time: 0.0207  last_data_time: 0.0136   lr: 0.0001  max_mem: 11521M
[02/27 19:26:41 d2.utils.events]:  eta: 1 day, 3:15:24  iter: 459  total_loss: 1.529  loss_cls: 0.3359  loss_box_reg: 0.1973  loss_mask: 0.5506  loss_rpn_cls: 0.1497  loss_rpn_loc: 0.1465  time: 0.5367  last_time: 0.5446  data_time: 0.0132  last_data_time: 0.0139   lr: 0.0001  max_mem: 11521M
[02/27 19:26:51 d2.utils.events]:  eta: 1 day, 3:14:45  iter: 479  total_loss: 1.444  loss_cls: 0.402  loss_box_reg: 0.216  loss_mask: 0.5546  loss_rpn_cls: 0.1259  loss_rpn_loc: 0.1776  time: 0.5361  last_time: 0.4933  data_time: 0.0213  last_data_time: 0.0025   lr: 0.0001  max_mem: 11521M
[02/27 19:27:02 d2.utils.events]:  eta: 1 day, 3:12:37  iter: 499  total_loss: 1.296  loss_cls: 0.3424  loss_box_reg: 0.1792  loss_mask: 0.5525  loss_rpn_cls: 0.09803  loss_rpn_loc: 0.1115  time: 0.5354  last_time: 0.5236  data_time: 0.0142  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 19:27:12 d2.utils.events]:  eta: 1 day, 3:07:13  iter: 519  total_loss: 1.486  loss_cls: 0.4121  loss_box_reg: 0.2048  loss_mask: 0.5717  loss_rpn_cls: 0.1376  loss_rpn_loc: 0.1601  time: 0.5347  last_time: 0.5253  data_time: 0.0150  last_data_time: 0.0113   lr: 0.0001  max_mem: 11521M
[02/27 19:27:23 d2.utils.events]:  eta: 1 day, 3:06:06  iter: 539  total_loss: 1.38  loss_cls: 0.3908  loss_box_reg: 0.2155  loss_mask: 0.5393  loss_rpn_cls: 0.1082  loss_rpn_loc: 0.1243  time: 0.5342  last_time: 0.5315  data_time: 0.0208  last_data_time: 0.0106   lr: 0.0001  max_mem: 11521M
[02/27 19:27:33 d2.utils.events]:  eta: 1 day, 3:06:22  iter: 559  total_loss: 1.601  loss_cls: 0.4088  loss_box_reg: 0.2527  loss_mask: 0.5512  loss_rpn_cls: 0.1173  loss_rpn_loc: 0.1944  time: 0.5341  last_time: 0.5402  data_time: 0.0246  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 19:27:44 d2.utils.events]:  eta: 1 day, 3:03:49  iter: 579  total_loss: 1.501  loss_cls: 0.3993  loss_box_reg: 0.1963  loss_mask: 0.553  loss_rpn_cls: 0.1661  loss_rpn_loc: 0.1581  time: 0.5335  last_time: 0.5295  data_time: 0.0235  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 19:27:54 d2.utils.events]:  eta: 1 day, 3:02:11  iter: 599  total_loss: 1.546  loss_cls: 0.3877  loss_box_reg: 0.2088  loss_mask: 0.5315  loss_rpn_cls: 0.1161  loss_rpn_loc: 0.1729  time: 0.5331  last_time: 0.5219  data_time: 0.0192  last_data_time: 0.0304   lr: 0.0001  max_mem: 11521M
[02/27 19:28:05 d2.utils.events]:  eta: 1 day, 3:01:44  iter: 619  total_loss: 1.472  loss_cls: 0.4023  loss_box_reg: 0.1862  loss_mask: 0.547  loss_rpn_cls: 0.1028  loss_rpn_loc: 0.1604  time: 0.5329  last_time: 0.5278  data_time: 0.0168  last_data_time: 0.0029   lr: 0.0001  max_mem: 11521M
[02/27 19:28:15 d2.utils.events]:  eta: 1 day, 3:00:17  iter: 639  total_loss: 1.532  loss_cls: 0.359  loss_box_reg: 0.2056  loss_mask: 0.5477  loss_rpn_cls: 0.1383  loss_rpn_loc: 0.1993  time: 0.5324  last_time: 0.4879  data_time: 0.0139  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 19:28:25 d2.utils.events]:  eta: 1 day, 2:58:36  iter: 659  total_loss: 1.483  loss_cls: 0.3939  loss_box_reg: 0.2027  loss_mask: 0.5535  loss_rpn_cls: 0.09464  loss_rpn_loc: 0.1655  time: 0.5320  last_time: 0.4900  data_time: 0.0094  last_data_time: 0.0293   lr: 0.0001  max_mem: 11521M
[02/27 19:28:36 d2.utils.events]:  eta: 1 day, 2:58:20  iter: 679  total_loss: 1.606  loss_cls: 0.4387  loss_box_reg: 0.22  loss_mask: 0.5128  loss_rpn_cls: 0.1628  loss_rpn_loc: 0.201  time: 0.5319  last_time: 0.5326  data_time: 0.0272  last_data_time: 0.0111   lr: 0.0001  max_mem: 11521M
[02/27 19:28:47 d2.utils.events]:  eta: 1 day, 2:57:51  iter: 699  total_loss: 1.545  loss_cls: 0.4109  loss_box_reg: 0.2658  loss_mask: 0.5365  loss_rpn_cls: 0.128  loss_rpn_loc: 0.1865  time: 0.5317  last_time: 0.5150  data_time: 0.0183  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 19:28:57 d2.utils.events]:  eta: 1 day, 2:57:02  iter: 719  total_loss: 1.473  loss_cls: 0.4122  loss_box_reg: 0.2146  loss_mask: 0.5425  loss_rpn_cls: 0.1199  loss_rpn_loc: 0.1307  time: 0.5313  last_time: 0.4835  data_time: 0.0233  last_data_time: 0.0133   lr: 0.0001  max_mem: 11521M
[02/27 19:29:07 d2.utils.events]:  eta: 1 day, 2:55:36  iter: 739  total_loss: 1.645  loss_cls: 0.4925  loss_box_reg: 0.2541  loss_mask: 0.5668  loss_rpn_cls: 0.1338  loss_rpn_loc: 0.2119  time: 0.5308  last_time: 0.5000  data_time: 0.0088  last_data_time: 0.0108   lr: 0.0001  max_mem: 11521M
[02/27 19:29:18 d2.utils.events]:  eta: 1 day, 2:54:28  iter: 759  total_loss: 1.526  loss_cls: 0.4352  loss_box_reg: 0.2871  loss_mask: 0.5249  loss_rpn_cls: 0.1308  loss_rpn_loc: 0.1501  time: 0.5303  last_time: 0.4992  data_time: 0.0172  last_data_time: 0.0133   lr: 0.0001  max_mem: 11521M
[02/27 19:29:28 d2.utils.events]:  eta: 1 day, 2:53:37  iter: 779  total_loss: 1.377  loss_cls: 0.3554  loss_box_reg: 0.2062  loss_mask: 0.5172  loss_rpn_cls: 0.101  loss_rpn_loc: 0.1916  time: 0.5299  last_time: 0.5189  data_time: 0.0148  last_data_time: 0.0118   lr: 0.0001  max_mem: 11521M
[02/27 19:29:39 d2.utils.events]:  eta: 1 day, 2:52:46  iter: 799  total_loss: 1.585  loss_cls: 0.4611  loss_box_reg: 0.2823  loss_mask: 0.5455  loss_rpn_cls: 0.08548  loss_rpn_loc: 0.1773  time: 0.5297  last_time: 0.5339  data_time: 0.0160  last_data_time: 0.0053   lr: 0.0001  max_mem: 11521M
[02/27 19:29:49 d2.utils.events]:  eta: 1 day, 2:52:11  iter: 819  total_loss: 1.555  loss_cls: 0.4509  loss_box_reg: 0.247  loss_mask: 0.5466  loss_rpn_cls: 0.108  loss_rpn_loc: 0.1444  time: 0.5296  last_time: 0.5511  data_time: 0.0132  last_data_time: 0.0270   lr: 0.0001  max_mem: 11521M
[02/27 19:30:00 d2.utils.events]:  eta: 1 day, 2:51:38  iter: 839  total_loss: 1.514  loss_cls: 0.4738  loss_box_reg: 0.2835  loss_mask: 0.5271  loss_rpn_cls: 0.07115  loss_rpn_loc: 0.1671  time: 0.5293  last_time: 0.5306  data_time: 0.0145  last_data_time: 0.0086   lr: 0.0001  max_mem: 11521M
[02/27 19:30:10 d2.utils.events]:  eta: 1 day, 2:50:21  iter: 859  total_loss: 1.491  loss_cls: 0.4359  loss_box_reg: 0.2347  loss_mask: 0.5229  loss_rpn_cls: 0.1176  loss_rpn_loc: 0.1715  time: 0.5289  last_time: 0.5485  data_time: 0.0195  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 19:30:20 d2.utils.events]:  eta: 1 day, 2:49:27  iter: 879  total_loss: 1.55  loss_cls: 0.4377  loss_box_reg: 0.2531  loss_mask: 0.5001  loss_rpn_cls: 0.13  loss_rpn_loc: 0.1744  time: 0.5287  last_time: 0.5023  data_time: 0.0145  last_data_time: 0.0055   lr: 0.0001  max_mem: 11521M
[02/27 19:30:31 d2.utils.events]:  eta: 1 day, 2:47:41  iter: 899  total_loss: 1.407  loss_cls: 0.3854  loss_box_reg: 0.2046  loss_mask: 0.5024  loss_rpn_cls: 0.1108  loss_rpn_loc: 0.1951  time: 0.5285  last_time: 0.4948  data_time: 0.0236  last_data_time: 0.0328   lr: 0.0001  max_mem: 11521M
[02/27 19:30:41 d2.utils.events]:  eta: 1 day, 2:46:04  iter: 919  total_loss: 1.449  loss_cls: 0.3807  loss_box_reg: 0.2491  loss_mask: 0.522  loss_rpn_cls: 0.1023  loss_rpn_loc: 0.192  time: 0.5282  last_time: 0.5045  data_time: 0.0201  last_data_time: 0.0203   lr: 0.0001  max_mem: 11521M
[02/27 19:30:52 d2.utils.events]:  eta: 1 day, 2:46:07  iter: 939  total_loss: 1.521  loss_cls: 0.4637  loss_box_reg: 0.2493  loss_mask: 0.5139  loss_rpn_cls: 0.1181  loss_rpn_loc: 0.2321  time: 0.5283  last_time: 0.4997  data_time: 0.0222  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 19:31:02 d2.utils.events]:  eta: 1 day, 2:45:18  iter: 959  total_loss: 1.309  loss_cls: 0.3475  loss_box_reg: 0.2061  loss_mask: 0.4895  loss_rpn_cls: 0.0987  loss_rpn_loc: 0.1201  time: 0.5280  last_time: 0.5268  data_time: 0.0098  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 19:31:12 d2.utils.events]:  eta: 1 day, 2:44:54  iter: 979  total_loss: 1.378  loss_cls: 0.3868  loss_box_reg: 0.21  loss_mask: 0.4498  loss_rpn_cls: 0.1055  loss_rpn_loc: 0.1128  time: 0.5277  last_time: 0.5294  data_time: 0.0264  last_data_time: 0.0639   lr: 0.0001  max_mem: 11521M
[02/27 19:31:23 d2.utils.events]:  eta: 1 day, 2:44:16  iter: 999  total_loss: 1.47  loss_cls: 0.3942  loss_box_reg: 0.1987  loss_mask: 0.5104  loss_rpn_cls: 0.1008  loss_rpn_loc: 0.1044  time: 0.5275  last_time: 0.5457  data_time: 0.0181  last_data_time: 0.0372   lr: 0.0001  max_mem: 11521M
[02/27 19:31:33 d2.utils.events]:  eta: 1 day, 2:41:34  iter: 1019  total_loss: 1.495  loss_cls: 0.414  loss_box_reg: 0.2314  loss_mask: 0.4981  loss_rpn_cls: 0.1384  loss_rpn_loc: 0.1614  time: 0.5273  last_time: 0.5502  data_time: 0.0117  last_data_time: 0.0156   lr: 0.0001  max_mem: 11521M
[02/27 19:31:44 d2.utils.events]:  eta: 1 day, 2:39:51  iter: 1039  total_loss: 1.509  loss_cls: 0.3447  loss_box_reg: 0.2098  loss_mask: 0.5189  loss_rpn_cls: 0.1319  loss_rpn_loc: 0.187  time: 0.5271  last_time: 0.4866  data_time: 0.0181  last_data_time: 0.0051   lr: 0.0001  max_mem: 11521M
[02/27 19:31:54 d2.utils.events]:  eta: 1 day, 2:38:16  iter: 1059  total_loss: 1.3  loss_cls: 0.369  loss_box_reg: 0.2248  loss_mask: 0.4851  loss_rpn_cls: 0.1216  loss_rpn_loc: 0.1727  time: 0.5270  last_time: 0.5156  data_time: 0.0170  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 19:32:05 d2.utils.events]:  eta: 1 day, 2:36:31  iter: 1079  total_loss: 1.497  loss_cls: 0.3934  loss_box_reg: 0.2395  loss_mask: 0.5134  loss_rpn_cls: 0.0957  loss_rpn_loc: 0.2066  time: 0.5269  last_time: 0.4899  data_time: 0.0156  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 19:32:15 d2.utils.events]:  eta: 1 day, 2:34:35  iter: 1099  total_loss: 1.349  loss_cls: 0.3797  loss_box_reg: 0.2232  loss_mask: 0.4907  loss_rpn_cls: 0.07945  loss_rpn_loc: 0.171  time: 0.5268  last_time: 0.5291  data_time: 0.0180  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 19:32:25 d2.utils.events]:  eta: 1 day, 2:32:38  iter: 1119  total_loss: 1.457  loss_cls: 0.4301  loss_box_reg: 0.2205  loss_mask: 0.4861  loss_rpn_cls: 0.1192  loss_rpn_loc: 0.1658  time: 0.5265  last_time: 0.5311  data_time: 0.0087  last_data_time: 0.0061   lr: 0.0001  max_mem: 11521M
[02/27 19:32:36 d2.utils.events]:  eta: 1 day, 2:30:23  iter: 1139  total_loss: 1.381  loss_cls: 0.3858  loss_box_reg: 0.2359  loss_mask: 0.5013  loss_rpn_cls: 0.09359  loss_rpn_loc: 0.168  time: 0.5261  last_time: 0.5018  data_time: 0.0092  last_data_time: 0.0028   lr: 0.0001  max_mem: 11521M
[02/27 19:32:46 d2.utils.events]:  eta: 1 day, 2:29:08  iter: 1159  total_loss: 1.56  loss_cls: 0.4105  loss_box_reg: 0.2789  loss_mask: 0.4799  loss_rpn_cls: 0.1158  loss_rpn_loc: 0.1232  time: 0.5259  last_time: 0.5030  data_time: 0.0148  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 19:32:56 d2.utils.events]:  eta: 1 day, 2:28:27  iter: 1179  total_loss: 1.424  loss_cls: 0.3925  loss_box_reg: 0.2377  loss_mask: 0.4798  loss_rpn_cls: 0.1145  loss_rpn_loc: 0.131  time: 0.5257  last_time: 0.4776  data_time: 0.0150  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 19:33:07 d2.utils.events]:  eta: 1 day, 2:26:36  iter: 1199  total_loss: 1.5  loss_cls: 0.3818  loss_box_reg: 0.2303  loss_mask: 0.5161  loss_rpn_cls: 0.1339  loss_rpn_loc: 0.1746  time: 0.5255  last_time: 0.5322  data_time: 0.0130  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 19:33:17 d2.utils.events]:  eta: 1 day, 2:25:11  iter: 1219  total_loss: 1.393  loss_cls: 0.3809  loss_box_reg: 0.2073  loss_mask: 0.4581  loss_rpn_cls: 0.09589  loss_rpn_loc: 0.1094  time: 0.5253  last_time: 0.5490  data_time: 0.0145  last_data_time: 0.0050   lr: 0.0001  max_mem: 11521M
[02/27 19:33:27 d2.utils.events]:  eta: 1 day, 2:25:01  iter: 1239  total_loss: 1.523  loss_cls: 0.4235  loss_box_reg: 0.2529  loss_mask: 0.5056  loss_rpn_cls: 0.1177  loss_rpn_loc: 0.2066  time: 0.5254  last_time: 0.5319  data_time: 0.0172  last_data_time: 0.0309   lr: 0.0001  max_mem: 11521M
[02/27 19:33:38 d2.utils.events]:  eta: 1 day, 2:23:41  iter: 1259  total_loss: 1.461  loss_cls: 0.4238  loss_box_reg: 0.2414  loss_mask: 0.5003  loss_rpn_cls: 0.09915  loss_rpn_loc: 0.1269  time: 0.5252  last_time: 0.5328  data_time: 0.0144  last_data_time: 0.0102   lr: 0.0001  max_mem: 11521M
[02/27 19:33:48 d2.utils.events]:  eta: 1 day, 2:22:29  iter: 1279  total_loss: 1.428  loss_cls: 0.4106  loss_box_reg: 0.1964  loss_mask: 0.4806  loss_rpn_cls: 0.129  loss_rpn_loc: 0.2057  time: 0.5251  last_time: 0.5479  data_time: 0.0198  last_data_time: 0.0987   lr: 0.0001  max_mem: 11521M
[02/27 19:33:58 d2.utils.events]:  eta: 1 day, 2:20:17  iter: 1299  total_loss: 1.524  loss_cls: 0.4339  loss_box_reg: 0.2724  loss_mask: 0.4749  loss_rpn_cls: 0.1154  loss_rpn_loc: 0.1274  time: 0.5248  last_time: 0.5210  data_time: 0.0192  last_data_time: 0.0448   lr: 0.0001  max_mem: 11521M
[02/27 19:34:09 d2.utils.events]:  eta: 1 day, 2:20:07  iter: 1319  total_loss: 1.51  loss_cls: 0.508  loss_box_reg: 0.2904  loss_mask: 0.5021  loss_rpn_cls: 0.1309  loss_rpn_loc: 0.18  time: 0.5247  last_time: 0.5291  data_time: 0.0102  last_data_time: 0.0167   lr: 0.0001  max_mem: 11521M
[02/27 19:34:19 d2.utils.events]:  eta: 1 day, 2:20:35  iter: 1339  total_loss: 1.461  loss_cls: 0.4567  loss_box_reg: 0.3011  loss_mask: 0.4795  loss_rpn_cls: 0.07278  loss_rpn_loc: 0.1565  time: 0.5244  last_time: 0.4833  data_time: 0.0224  last_data_time: 0.0090   lr: 0.0001  max_mem: 11521M
[02/27 19:34:29 d2.utils.events]:  eta: 1 day, 2:20:04  iter: 1359  total_loss: 1.659  loss_cls: 0.5164  loss_box_reg: 0.3192  loss_mask: 0.4941  loss_rpn_cls: 0.1475  loss_rpn_loc: 0.2155  time: 0.5243  last_time: 0.5111  data_time: 0.0176  last_data_time: 0.0138   lr: 0.0001  max_mem: 11521M
[02/27 19:34:40 d2.utils.events]:  eta: 1 day, 2:19:27  iter: 1379  total_loss: 1.27  loss_cls: 0.3426  loss_box_reg: 0.2006  loss_mask: 0.4743  loss_rpn_cls: 0.1241  loss_rpn_loc: 0.1426  time: 0.5242  last_time: 0.5236  data_time: 0.0158  last_data_time: 0.0266   lr: 0.0001  max_mem: 11521M
[02/27 19:34:50 d2.utils.events]:  eta: 1 day, 2:19:57  iter: 1399  total_loss: 1.446  loss_cls: 0.4343  loss_box_reg: 0.2864  loss_mask: 0.4598  loss_rpn_cls: 0.1176  loss_rpn_loc: 0.1798  time: 0.5242  last_time: 0.5138  data_time: 0.0202  last_data_time: 0.0149   lr: 0.0001  max_mem: 11521M
[02/27 19:35:01 d2.utils.events]:  eta: 1 day, 2:19:47  iter: 1419  total_loss: 1.512  loss_cls: 0.4828  loss_box_reg: 0.3054  loss_mask: 0.4702  loss_rpn_cls: 0.1069  loss_rpn_loc: 0.1495  time: 0.5241  last_time: 0.5316  data_time: 0.0181  last_data_time: 0.0053   lr: 0.0001  max_mem: 11521M
[02/27 19:35:11 d2.utils.events]:  eta: 1 day, 2:19:05  iter: 1439  total_loss: 1.441  loss_cls: 0.3829  loss_box_reg: 0.2092  loss_mask: 0.4799  loss_rpn_cls: 0.08574  loss_rpn_loc: 0.194  time: 0.5239  last_time: 0.5050  data_time: 0.0176  last_data_time: 0.0240   lr: 0.0001  max_mem: 11521M
[02/27 19:35:22 d2.utils.events]:  eta: 1 day, 2:18:46  iter: 1459  total_loss: 1.6  loss_cls: 0.4442  loss_box_reg: 0.2722  loss_mask: 0.4679  loss_rpn_cls: 0.1236  loss_rpn_loc: 0.1611  time: 0.5239  last_time: 0.5021  data_time: 0.0173  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 19:35:32 d2.utils.events]:  eta: 1 day, 2:18:34  iter: 1479  total_loss: 1.418  loss_cls: 0.3842  loss_box_reg: 0.2375  loss_mask: 0.4364  loss_rpn_cls: 0.1117  loss_rpn_loc: 0.1904  time: 0.5237  last_time: 0.5288  data_time: 0.0117  last_data_time: 0.0072   lr: 0.0001  max_mem: 11521M
[02/27 19:35:42 d2.utils.events]:  eta: 1 day, 2:18:23  iter: 1499  total_loss: 1.39  loss_cls: 0.4215  loss_box_reg: 0.2224  loss_mask: 0.4632  loss_rpn_cls: 0.08826  loss_rpn_loc: 0.1449  time: 0.5236  last_time: 0.5177  data_time: 0.0182  last_data_time: 0.0154   lr: 0.0001  max_mem: 11521M
[02/27 19:35:53 d2.utils.events]:  eta: 1 day, 2:18:14  iter: 1519  total_loss: 1.391  loss_cls: 0.363  loss_box_reg: 0.2298  loss_mask: 0.4604  loss_rpn_cls: 0.09305  loss_rpn_loc: 0.1444  time: 0.5236  last_time: 0.5809  data_time: 0.0198  last_data_time: 0.0029   lr: 0.0001  max_mem: 11521M
[02/27 19:36:03 d2.utils.events]:  eta: 1 day, 2:17:15  iter: 1539  total_loss: 1.387  loss_cls: 0.4416  loss_box_reg: 0.2819  loss_mask: 0.4458  loss_rpn_cls: 0.1002  loss_rpn_loc: 0.1175  time: 0.5234  last_time: 0.5498  data_time: 0.0176  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 19:36:14 d2.utils.events]:  eta: 1 day, 2:15:47  iter: 1559  total_loss: 1.516  loss_cls: 0.477  loss_box_reg: 0.2781  loss_mask: 0.4902  loss_rpn_cls: 0.101  loss_rpn_loc: 0.1924  time: 0.5233  last_time: 0.5136  data_time: 0.0100  last_data_time: 0.0369   lr: 0.0001  max_mem: 11521M
[02/27 19:36:24 d2.utils.events]:  eta: 1 day, 2:15:30  iter: 1579  total_loss: 1.28  loss_cls: 0.3691  loss_box_reg: 0.2063  loss_mask: 0.4516  loss_rpn_cls: 0.1004  loss_rpn_loc: 0.1279  time: 0.5233  last_time: 0.4943  data_time: 0.0247  last_data_time: 0.0408   lr: 0.0001  max_mem: 11521M
[02/27 19:36:34 d2.utils.events]:  eta: 1 day, 2:14:04  iter: 1599  total_loss: 1.389  loss_cls: 0.3404  loss_box_reg: 0.1964  loss_mask: 0.4359  loss_rpn_cls: 0.1069  loss_rpn_loc: 0.1623  time: 0.5232  last_time: 0.5392  data_time: 0.0204  last_data_time: 0.0104   lr: 0.0001  max_mem: 11521M
[02/27 19:36:45 d2.utils.events]:  eta: 1 day, 2:12:57  iter: 1619  total_loss: 1.337  loss_cls: 0.3603  loss_box_reg: 0.2111  loss_mask: 0.4732  loss_rpn_cls: 0.1062  loss_rpn_loc: 0.2135  time: 0.5231  last_time: 0.5166  data_time: 0.0177  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 19:36:55 d2.utils.events]:  eta: 1 day, 2:12:05  iter: 1639  total_loss: 1.459  loss_cls: 0.4284  loss_box_reg: 0.2554  loss_mask: 0.4907  loss_rpn_cls: 0.1012  loss_rpn_loc: 0.1256  time: 0.5229  last_time: 0.5077  data_time: 0.0059  last_data_time: 0.0145   lr: 0.0001  max_mem: 11521M
[02/27 19:37:05 d2.utils.events]:  eta: 1 day, 2:11:17  iter: 1659  total_loss: 1.484  loss_cls: 0.4035  loss_box_reg: 0.2389  loss_mask: 0.4872  loss_rpn_cls: 0.09784  loss_rpn_loc: 0.2012  time: 0.5228  last_time: 0.5257  data_time: 0.0111  last_data_time: 0.0077   lr: 0.0001  max_mem: 11521M
[02/27 19:37:16 d2.utils.events]:  eta: 1 day, 2:11:02  iter: 1679  total_loss: 1.371  loss_cls: 0.3752  loss_box_reg: 0.2444  loss_mask: 0.4737  loss_rpn_cls: 0.07927  loss_rpn_loc: 0.2205  time: 0.5228  last_time: 0.5349  data_time: 0.0166  last_data_time: 0.0202   lr: 0.0001  max_mem: 11521M
[02/27 19:37:26 d2.utils.events]:  eta: 1 day, 2:09:51  iter: 1699  total_loss: 1.47  loss_cls: 0.4679  loss_box_reg: 0.2621  loss_mask: 0.4442  loss_rpn_cls: 0.1136  loss_rpn_loc: 0.1553  time: 0.5227  last_time: 0.5110  data_time: 0.0091  last_data_time: 0.0109   lr: 0.0001  max_mem: 11521M
[02/27 19:37:36 d2.utils.events]:  eta: 1 day, 2:09:36  iter: 1719  total_loss: 1.506  loss_cls: 0.4538  loss_box_reg: 0.2783  loss_mask: 0.4591  loss_rpn_cls: 0.08654  loss_rpn_loc: 0.1805  time: 0.5225  last_time: 0.5183  data_time: 0.0167  last_data_time: 0.0131   lr: 0.0001  max_mem: 11521M
[02/27 19:37:47 d2.utils.events]:  eta: 1 day, 2:09:30  iter: 1739  total_loss: 1.502  loss_cls: 0.4177  loss_box_reg: 0.2842  loss_mask: 0.4894  loss_rpn_cls: 0.1127  loss_rpn_loc: 0.1378  time: 0.5224  last_time: 0.5325  data_time: 0.0233  last_data_time: 0.0439   lr: 0.0001  max_mem: 11521M
[02/27 19:37:57 d2.utils.events]:  eta: 1 day, 2:09:50  iter: 1759  total_loss: 1.318  loss_cls: 0.3736  loss_box_reg: 0.2491  loss_mask: 0.4477  loss_rpn_cls: 0.1181  loss_rpn_loc: 0.1865  time: 0.5224  last_time: 0.5136  data_time: 0.0179  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 19:38:08 d2.utils.events]:  eta: 1 day, 2:09:37  iter: 1779  total_loss: 1.378  loss_cls: 0.4168  loss_box_reg: 0.2232  loss_mask: 0.4432  loss_rpn_cls: 0.1032  loss_rpn_loc: 0.133  time: 0.5224  last_time: 0.5404  data_time: 0.0183  last_data_time: 0.0995   lr: 0.0001  max_mem: 11521M
[02/27 19:38:18 d2.utils.events]:  eta: 1 day, 2:08:55  iter: 1799  total_loss: 1.411  loss_cls: 0.3887  loss_box_reg: 0.242  loss_mask: 0.4502  loss_rpn_cls: 0.1372  loss_rpn_loc: 0.1262  time: 0.5223  last_time: 0.5055  data_time: 0.0094  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 19:38:29 d2.utils.events]:  eta: 1 day, 2:08:33  iter: 1819  total_loss: 1.397  loss_cls: 0.4197  loss_box_reg: 0.2471  loss_mask: 0.438  loss_rpn_cls: 0.09205  loss_rpn_loc: 0.141  time: 0.5223  last_time: 0.5179  data_time: 0.0322  last_data_time: 0.0520   lr: 0.0001  max_mem: 11521M
[02/27 19:38:39 d2.utils.events]:  eta: 1 day, 2:07:53  iter: 1839  total_loss: 1.41  loss_cls: 0.425  loss_box_reg: 0.2579  loss_mask: 0.434  loss_rpn_cls: 0.09343  loss_rpn_loc: 0.1324  time: 0.5221  last_time: 0.5028  data_time: 0.0150  last_data_time: 0.0029   lr: 0.0001  max_mem: 11521M
[02/27 19:38:49 d2.utils.events]:  eta: 1 day, 2:08:12  iter: 1859  total_loss: 1.449  loss_cls: 0.3943  loss_box_reg: 0.2453  loss_mask: 0.4261  loss_rpn_cls: 0.1013  loss_rpn_loc: 0.2102  time: 0.5221  last_time: 0.5297  data_time: 0.0168  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 19:39:00 d2.utils.events]:  eta: 1 day, 2:08:06  iter: 1879  total_loss: 1.504  loss_cls: 0.3932  loss_box_reg: 0.265  loss_mask: 0.4793  loss_rpn_cls: 0.1255  loss_rpn_loc: 0.148  time: 0.5221  last_time: 0.5355  data_time: 0.0067  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 19:39:10 d2.utils.events]:  eta: 1 day, 2:08:24  iter: 1899  total_loss: 1.559  loss_cls: 0.4741  loss_box_reg: 0.3309  loss_mask: 0.4373  loss_rpn_cls: 0.08496  loss_rpn_loc: 0.1794  time: 0.5221  last_time: 0.5112  data_time: 0.0166  last_data_time: 0.0078   lr: 0.0001  max_mem: 11521M
[02/27 19:39:20 d2.utils.events]:  eta: 1 day, 2:08:27  iter: 1919  total_loss: 1.323  loss_cls: 0.3975  loss_box_reg: 0.2429  loss_mask: 0.4204  loss_rpn_cls: 0.1142  loss_rpn_loc: 0.08632  time: 0.5220  last_time: 0.5151  data_time: 0.0161  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 19:39:31 d2.utils.events]:  eta: 1 day, 2:07:44  iter: 1939  total_loss: 1.433  loss_cls: 0.4065  loss_box_reg: 0.2678  loss_mask: 0.4585  loss_rpn_cls: 0.09659  loss_rpn_loc: 0.1847  time: 0.5219  last_time: 0.4979  data_time: 0.0094  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 19:39:41 d2.utils.events]:  eta: 1 day, 2:08:12  iter: 1959  total_loss: 1.399  loss_cls: 0.3672  loss_box_reg: 0.2215  loss_mask: 0.4854  loss_rpn_cls: 0.08999  loss_rpn_loc: 0.1823  time: 0.5218  last_time: 0.4960  data_time: 0.0139  last_data_time: 0.0056   lr: 0.0001  max_mem: 11521M
[02/27 19:39:52 d2.utils.events]:  eta: 1 day, 2:08:02  iter: 1979  total_loss: 1.409  loss_cls: 0.3905  loss_box_reg: 0.252  loss_mask: 0.4588  loss_rpn_cls: 0.1048  loss_rpn_loc: 0.1617  time: 0.5218  last_time: 0.5438  data_time: 0.0082  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 19:40:02 d2.utils.events]:  eta: 1 day, 2:08:06  iter: 1999  total_loss: 1.417  loss_cls: 0.3515  loss_box_reg: 0.2454  loss_mask: 0.4601  loss_rpn_cls: 0.1007  loss_rpn_loc: 0.1828  time: 0.5219  last_time: 0.5086  data_time: 0.0175  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 19:40:12 d2.utils.events]:  eta: 1 day, 2:07:47  iter: 2019  total_loss: 1.397  loss_cls: 0.3924  loss_box_reg: 0.2713  loss_mask: 0.459  loss_rpn_cls: 0.08495  loss_rpn_loc: 0.1126  time: 0.5217  last_time: 0.4949  data_time: 0.0114  last_data_time: 0.0024   lr: 0.0001  max_mem: 11521M
[02/27 19:40:23 d2.utils.events]:  eta: 1 day, 2:07:39  iter: 2039  total_loss: 1.372  loss_cls: 0.3972  loss_box_reg: 0.2486  loss_mask: 0.454  loss_rpn_cls: 0.09105  loss_rpn_loc: 0.1504  time: 0.5217  last_time: 0.5204  data_time: 0.0174  last_data_time: 0.0415   lr: 0.0001  max_mem: 11521M
[02/27 19:40:33 d2.utils.events]:  eta: 1 day, 2:07:21  iter: 2059  total_loss: 1.181  loss_cls: 0.3245  loss_box_reg: 0.2021  loss_mask: 0.4879  loss_rpn_cls: 0.07373  loss_rpn_loc: 0.1298  time: 0.5217  last_time: 0.5346  data_time: 0.0189  last_data_time: 0.0118   lr: 0.0001  max_mem: 11521M
[02/27 19:40:44 d2.utils.events]:  eta: 1 day, 2:07:49  iter: 2079  total_loss: 1.451  loss_cls: 0.429  loss_box_reg: 0.2917  loss_mask: 0.4479  loss_rpn_cls: 0.1102  loss_rpn_loc: 0.1881  time: 0.5217  last_time: 0.5207  data_time: 0.0083  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 19:40:54 d2.utils.events]:  eta: 1 day, 2:07:48  iter: 2099  total_loss: 1.576  loss_cls: 0.469  loss_box_reg: 0.3162  loss_mask: 0.4559  loss_rpn_cls: 0.08768  loss_rpn_loc: 0.1455  time: 0.5217  last_time: 0.5543  data_time: 0.0146  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 19:41:05 d2.utils.events]:  eta: 1 day, 2:07:49  iter: 2119  total_loss: 1.338  loss_cls: 0.3886  loss_box_reg: 0.2475  loss_mask: 0.4264  loss_rpn_cls: 0.0974  loss_rpn_loc: 0.1228  time: 0.5216  last_time: 0.4568  data_time: 0.0163  last_data_time: 0.0052   lr: 0.0001  max_mem: 11521M
[02/27 19:41:15 d2.utils.events]:  eta: 1 day, 2:08:07  iter: 2139  total_loss: 1.429  loss_cls: 0.3899  loss_box_reg: 0.2731  loss_mask: 0.4565  loss_rpn_cls: 0.1014  loss_rpn_loc: 0.118  time: 0.5215  last_time: 0.5255  data_time: 0.0145  last_data_time: 0.0032   lr: 0.0001  max_mem: 11521M
[02/27 19:41:25 d2.utils.events]:  eta: 1 day, 2:08:05  iter: 2159  total_loss: 1.58  loss_cls: 0.4471  loss_box_reg: 0.3047  loss_mask: 0.455  loss_rpn_cls: 0.1104  loss_rpn_loc: 0.2247  time: 0.5214  last_time: 0.5215  data_time: 0.0138  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 19:41:36 d2.utils.events]:  eta: 1 day, 2:08:43  iter: 2179  total_loss: 1.379  loss_cls: 0.4308  loss_box_reg: 0.2746  loss_mask: 0.4314  loss_rpn_cls: 0.07573  loss_rpn_loc: 0.1408  time: 0.5215  last_time: 0.5495  data_time: 0.0175  last_data_time: 0.0067   lr: 0.0001  max_mem: 11521M
[02/27 19:41:46 d2.utils.events]:  eta: 1 day, 2:07:55  iter: 2199  total_loss: 1.381  loss_cls: 0.4533  loss_box_reg: 0.271  loss_mask: 0.446  loss_rpn_cls: 0.07377  loss_rpn_loc: 0.1244  time: 0.5213  last_time: 0.5166  data_time: 0.0092  last_data_time: 0.0177   lr: 0.0001  max_mem: 11521M
[02/27 19:41:56 d2.utils.events]:  eta: 1 day, 2:07:58  iter: 2219  total_loss: 1.32  loss_cls: 0.3942  loss_box_reg: 0.2672  loss_mask: 0.458  loss_rpn_cls: 0.07804  loss_rpn_loc: 0.1201  time: 0.5213  last_time: 0.5407  data_time: 0.0159  last_data_time: 0.0675   lr: 0.0001  max_mem: 11521M
[02/27 19:42:07 d2.utils.events]:  eta: 1 day, 2:07:30  iter: 2239  total_loss: 1.555  loss_cls: 0.4279  loss_box_reg: 0.2988  loss_mask: 0.4533  loss_rpn_cls: 0.1052  loss_rpn_loc: 0.1596  time: 0.5213  last_time: 0.5105  data_time: 0.0157  last_data_time: 0.0068   lr: 0.0001  max_mem: 11521M
[02/27 19:42:17 d2.utils.events]:  eta: 1 day, 2:08:08  iter: 2259  total_loss: 1.404  loss_cls: 0.3907  loss_box_reg: 0.2421  loss_mask: 0.4533  loss_rpn_cls: 0.1034  loss_rpn_loc: 0.1529  time: 0.5213  last_time: 0.5171  data_time: 0.0165  last_data_time: 0.0664   lr: 0.0001  max_mem: 11521M
[02/27 19:42:28 d2.utils.events]:  eta: 1 day, 2:07:58  iter: 2279  total_loss: 1.389  loss_cls: 0.3841  loss_box_reg: 0.2686  loss_mask: 0.4535  loss_rpn_cls: 0.09497  loss_rpn_loc: 0.1448  time: 0.5213  last_time: 0.4979  data_time: 0.0100  last_data_time: 0.0254   lr: 0.0001  max_mem: 11521M
[02/27 19:42:38 d2.utils.events]:  eta: 1 day, 2:08:18  iter: 2299  total_loss: 1.398  loss_cls: 0.4024  loss_box_reg: 0.2393  loss_mask: 0.4354  loss_rpn_cls: 0.07918  loss_rpn_loc: 0.1532  time: 0.5212  last_time: 0.5336  data_time: 0.0134  last_data_time: 0.0068   lr: 0.0001  max_mem: 11521M
[02/27 19:42:49 d2.utils.events]:  eta: 1 day, 2:08:54  iter: 2319  total_loss: 1.331  loss_cls: 0.3607  loss_box_reg: 0.2518  loss_mask: 0.434  loss_rpn_cls: 0.1073  loss_rpn_loc: 0.09663  time: 0.5213  last_time: 0.5366  data_time: 0.0168  last_data_time: 0.0100   lr: 0.0001  max_mem: 11521M
[02/27 19:42:59 d2.utils.events]:  eta: 1 day, 2:08:53  iter: 2339  total_loss: 1.384  loss_cls: 0.3942  loss_box_reg: 0.2566  loss_mask: 0.4486  loss_rpn_cls: 0.09446  loss_rpn_loc: 0.1537  time: 0.5212  last_time: 0.4945  data_time: 0.0151  last_data_time: 0.0051   lr: 0.0001  max_mem: 11521M
[02/27 19:43:10 d2.utils.events]:  eta: 1 day, 2:08:24  iter: 2359  total_loss: 1.341  loss_cls: 0.3286  loss_box_reg: 0.2275  loss_mask: 0.4184  loss_rpn_cls: 0.1101  loss_rpn_loc: 0.1424  time: 0.5211  last_time: 0.5353  data_time: 0.0208  last_data_time: 0.0277   lr: 0.0001  max_mem: 11521M
[02/27 19:43:20 d2.utils.events]:  eta: 1 day, 2:08:14  iter: 2379  total_loss: 1.271  loss_cls: 0.375  loss_box_reg: 0.236  loss_mask: 0.4503  loss_rpn_cls: 0.08815  loss_rpn_loc: 0.1236  time: 0.5211  last_time: 0.5378  data_time: 0.0157  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 19:43:31 d2.utils.events]:  eta: 1 day, 2:07:50  iter: 2399  total_loss: 1.331  loss_cls: 0.4002  loss_box_reg: 0.231  loss_mask: 0.4445  loss_rpn_cls: 0.09889  loss_rpn_loc: 0.1518  time: 0.5211  last_time: 0.5798  data_time: 0.0115  last_data_time: 0.0287   lr: 0.0001  max_mem: 11521M
[02/27 19:43:41 d2.utils.events]:  eta: 1 day, 2:07:46  iter: 2419  total_loss: 1.372  loss_cls: 0.4266  loss_box_reg: 0.2622  loss_mask: 0.4342  loss_rpn_cls: 0.09881  loss_rpn_loc: 0.1513  time: 0.5211  last_time: 0.5012  data_time: 0.0131  last_data_time: 0.0067   lr: 0.0001  max_mem: 11521M
[02/27 19:43:52 d2.utils.events]:  eta: 1 day, 2:07:52  iter: 2439  total_loss: 1.148  loss_cls: 0.3512  loss_box_reg: 0.2228  loss_mask: 0.4233  loss_rpn_cls: 0.07936  loss_rpn_loc: 0.08437  time: 0.5211  last_time: 0.5308  data_time: 0.0184  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 19:44:02 d2.utils.events]:  eta: 1 day, 2:06:31  iter: 2459  total_loss: 1.313  loss_cls: 0.3808  loss_box_reg: 0.2349  loss_mask: 0.4369  loss_rpn_cls: 0.1061  loss_rpn_loc: 0.1374  time: 0.5209  last_time: 0.4735  data_time: 0.0100  last_data_time: 0.0233   lr: 0.0001  max_mem: 11521M
[02/27 19:44:12 d2.utils.events]:  eta: 1 day, 2:06:15  iter: 2479  total_loss: 1.307  loss_cls: 0.3559  loss_box_reg: 0.2142  loss_mask: 0.4329  loss_rpn_cls: 0.1073  loss_rpn_loc: 0.1185  time: 0.5208  last_time: 0.5274  data_time: 0.0162  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 19:44:23 d2.utils.events]:  eta: 1 day, 2:05:58  iter: 2499  total_loss: 1.422  loss_cls: 0.3958  loss_box_reg: 0.261  loss_mask: 0.4478  loss_rpn_cls: 0.1012  loss_rpn_loc: 0.1103  time: 0.5208  last_time: 0.5014  data_time: 0.0212  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 19:44:33 d2.utils.events]:  eta: 1 day, 2:05:46  iter: 2519  total_loss: 1.311  loss_cls: 0.3894  loss_box_reg: 0.2402  loss_mask: 0.42  loss_rpn_cls: 0.0874  loss_rpn_loc: 0.1346  time: 0.5208  last_time: 0.5091  data_time: 0.0198  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 19:44:44 d2.utils.events]:  eta: 1 day, 2:05:46  iter: 2539  total_loss: 1.658  loss_cls: 0.5117  loss_box_reg: 0.3251  loss_mask: 0.4643  loss_rpn_cls: 0.08766  loss_rpn_loc: 0.1762  time: 0.5207  last_time: 0.5258  data_time: 0.0145  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 19:44:54 d2.utils.events]:  eta: 1 day, 2:06:04  iter: 2559  total_loss: 1.297  loss_cls: 0.4015  loss_box_reg: 0.2451  loss_mask: 0.391  loss_rpn_cls: 0.08584  loss_rpn_loc: 0.1168  time: 0.5207  last_time: 0.5300  data_time: 0.0141  last_data_time: 0.0120   lr: 0.0001  max_mem: 11521M
[02/27 19:45:04 d2.utils.events]:  eta: 1 day, 2:05:25  iter: 2579  total_loss: 1.263  loss_cls: 0.3477  loss_box_reg: 0.2288  loss_mask: 0.4436  loss_rpn_cls: 0.07724  loss_rpn_loc: 0.1485  time: 0.5207  last_time: 0.5098  data_time: 0.0125  last_data_time: 0.0296   lr: 0.0001  max_mem: 11521M
[02/27 19:45:15 d2.utils.events]:  eta: 1 day, 2:05:21  iter: 2599  total_loss: 1.242  loss_cls: 0.317  loss_box_reg: 0.2  loss_mask: 0.407  loss_rpn_cls: 0.07807  loss_rpn_loc: 0.1649  time: 0.5207  last_time: 0.5060  data_time: 0.0132  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 19:45:25 d2.utils.events]:  eta: 1 day, 2:05:20  iter: 2619  total_loss: 1.264  loss_cls: 0.3097  loss_box_reg: 0.1788  loss_mask: 0.4618  loss_rpn_cls: 0.1098  loss_rpn_loc: 0.1481  time: 0.5206  last_time: 0.5251  data_time: 0.0173  last_data_time: 0.0665   lr: 0.0001  max_mem: 11521M
[02/27 19:45:36 d2.utils.events]:  eta: 1 day, 2:06:30  iter: 2639  total_loss: 1.182  loss_cls: 0.3021  loss_box_reg: 0.1599  loss_mask: 0.4078  loss_rpn_cls: 0.1042  loss_rpn_loc: 0.1219  time: 0.5206  last_time: 0.5018  data_time: 0.0196  last_data_time: 0.0246   lr: 0.0001  max_mem: 11521M
[02/27 19:45:46 d2.utils.events]:  eta: 1 day, 2:06:20  iter: 2659  total_loss: 1.391  loss_cls: 0.4021  loss_box_reg: 0.2897  loss_mask: 0.4231  loss_rpn_cls: 0.08995  loss_rpn_loc: 0.1888  time: 0.5205  last_time: 0.5192  data_time: 0.0105  last_data_time: 0.0026   lr: 0.0001  max_mem: 11521M
[02/27 19:45:56 d2.utils.events]:  eta: 1 day, 2:06:34  iter: 2679  total_loss: 1.312  loss_cls: 0.4465  loss_box_reg: 0.2735  loss_mask: 0.4315  loss_rpn_cls: 0.09575  loss_rpn_loc: 0.1295  time: 0.5206  last_time: 0.5257  data_time: 0.0173  last_data_time: 0.0066   lr: 0.0001  max_mem: 11521M
[02/27 19:46:07 d2.utils.events]:  eta: 1 day, 2:06:19  iter: 2699  total_loss: 1.343  loss_cls: 0.3597  loss_box_reg: 0.2557  loss_mask: 0.4251  loss_rpn_cls: 0.08664  loss_rpn_loc: 0.1267  time: 0.5205  last_time: 0.5425  data_time: 0.0168  last_data_time: 0.0278   lr: 0.0001  max_mem: 11521M
[02/27 19:46:17 d2.utils.events]:  eta: 1 day, 2:05:44  iter: 2719  total_loss: 1.393  loss_cls: 0.4119  loss_box_reg: 0.2442  loss_mask: 0.4412  loss_rpn_cls: 0.08449  loss_rpn_loc: 0.1498  time: 0.5205  last_time: 0.4907  data_time: 0.0146  last_data_time: 0.0234   lr: 0.0001  max_mem: 11521M
[02/27 19:46:27 d2.utils.events]:  eta: 1 day, 2:05:10  iter: 2739  total_loss: 1.171  loss_cls: 0.3482  loss_box_reg: 0.2128  loss_mask: 0.3865  loss_rpn_cls: 0.08784  loss_rpn_loc: 0.09419  time: 0.5204  last_time: 0.5032  data_time: 0.0125  last_data_time: 0.0124   lr: 0.0001  max_mem: 11521M
[02/27 19:46:38 d2.utils.events]:  eta: 1 day, 2:05:00  iter: 2759  total_loss: 1.16  loss_cls: 0.3202  loss_box_reg: 0.1854  loss_mask: 0.4358  loss_rpn_cls: 0.09054  loss_rpn_loc: 0.1376  time: 0.5203  last_time: 0.4827  data_time: 0.0249  last_data_time: 0.0069   lr: 0.0001  max_mem: 11521M
[02/27 19:46:48 d2.utils.events]:  eta: 1 day, 2:04:49  iter: 2779  total_loss: 1.456  loss_cls: 0.3925  loss_box_reg: 0.2752  loss_mask: 0.4795  loss_rpn_cls: 0.09101  loss_rpn_loc: 0.149  time: 0.5203  last_time: 0.5366  data_time: 0.0120  last_data_time: 0.0402   lr: 0.0001  max_mem: 11521M
[02/27 19:46:58 d2.utils.events]:  eta: 1 day, 2:04:58  iter: 2799  total_loss: 1.296  loss_cls: 0.3656  loss_box_reg: 0.2418  loss_mask: 0.4509  loss_rpn_cls: 0.09495  loss_rpn_loc: 0.1262  time: 0.5203  last_time: 0.5177  data_time: 0.0215  last_data_time: 0.0097   lr: 0.0001  max_mem: 11521M
[02/27 19:47:09 d2.utils.events]:  eta: 1 day, 2:04:29  iter: 2819  total_loss: 1.476  loss_cls: 0.4209  loss_box_reg: 0.2841  loss_mask: 0.4476  loss_rpn_cls: 0.1064  loss_rpn_loc: 0.1749  time: 0.5202  last_time: 0.5290  data_time: 0.0124  last_data_time: 0.0395   lr: 0.0001  max_mem: 11521M
[02/27 19:47:19 d2.utils.events]:  eta: 1 day, 2:05:31  iter: 2839  total_loss: 1.439  loss_cls: 0.4011  loss_box_reg: 0.3  loss_mask: 0.4359  loss_rpn_cls: 0.0913  loss_rpn_loc: 0.1547  time: 0.5203  last_time: 0.4962  data_time: 0.0185  last_data_time: 0.0060   lr: 0.0001  max_mem: 11521M
[02/27 19:47:30 d2.utils.events]:  eta: 1 day, 2:05:31  iter: 2859  total_loss: 1.348  loss_cls: 0.4057  loss_box_reg: 0.2699  loss_mask: 0.4559  loss_rpn_cls: 0.0667  loss_rpn_loc: 0.1043  time: 0.5204  last_time: 0.5353  data_time: 0.0192  last_data_time: 0.0420   lr: 0.0001  max_mem: 11521M
[02/27 19:47:40 d2.utils.events]:  eta: 1 day, 2:05:02  iter: 2879  total_loss: 1.355  loss_cls: 0.4016  loss_box_reg: 0.2379  loss_mask: 0.4263  loss_rpn_cls: 0.09867  loss_rpn_loc: 0.1336  time: 0.5204  last_time: 0.5372  data_time: 0.0270  last_data_time: 0.0434   lr: 0.0001  max_mem: 11521M
[02/27 19:47:51 d2.utils.events]:  eta: 1 day, 2:04:52  iter: 2899  total_loss: 1.248  loss_cls: 0.3942  loss_box_reg: 0.2889  loss_mask: 0.4217  loss_rpn_cls: 0.06876  loss_rpn_loc: 0.07287  time: 0.5204  last_time: 0.5095  data_time: 0.0212  last_data_time: 0.0135   lr: 0.0001  max_mem: 11521M
[02/27 19:48:02 d2.utils.events]:  eta: 1 day, 2:05:04  iter: 2919  total_loss: 1.151  loss_cls: 0.3912  loss_box_reg: 0.2405  loss_mask: 0.4222  loss_rpn_cls: 0.07949  loss_rpn_loc: 0.1229  time: 0.5205  last_time: 0.5133  data_time: 0.0161  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 19:48:12 d2.utils.events]:  eta: 1 day, 2:04:49  iter: 2939  total_loss: 1.459  loss_cls: 0.4193  loss_box_reg: 0.259  loss_mask: 0.4778  loss_rpn_cls: 0.1019  loss_rpn_loc: 0.1848  time: 0.5204  last_time: 0.5092  data_time: 0.0135  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 19:48:22 d2.utils.events]:  eta: 1 day, 2:04:21  iter: 2959  total_loss: 1.284  loss_cls: 0.3345  loss_box_reg: 0.2472  loss_mask: 0.4429  loss_rpn_cls: 0.07213  loss_rpn_loc: 0.1923  time: 0.5204  last_time: 0.4986  data_time: 0.0192  last_data_time: 0.0053   lr: 0.0001  max_mem: 11521M
[02/27 19:48:33 d2.utils.events]:  eta: 1 day, 2:04:19  iter: 2979  total_loss: 1.238  loss_cls: 0.3393  loss_box_reg: 0.2146  loss_mask: 0.4118  loss_rpn_cls: 0.1017  loss_rpn_loc: 0.1789  time: 0.5204  last_time: 0.5125  data_time: 0.0164  last_data_time: 0.0287   lr: 0.0001  max_mem: 11521M
[02/27 19:48:43 d2.utils.events]:  eta: 1 day, 2:03:15  iter: 2999  total_loss: 1.302  loss_cls: 0.3664  loss_box_reg: 0.211  loss_mask: 0.4135  loss_rpn_cls: 0.07848  loss_rpn_loc: 0.1379  time: 0.5203  last_time: 0.5429  data_time: 0.0214  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 19:48:54 d2.utils.events]:  eta: 1 day, 2:04:20  iter: 3019  total_loss: 1.326  loss_cls: 0.4166  loss_box_reg: 0.2407  loss_mask: 0.4089  loss_rpn_cls: 0.09241  loss_rpn_loc: 0.1137  time: 0.5204  last_time: 0.5679  data_time: 0.0217  last_data_time: 0.0028   lr: 0.0001  max_mem: 11521M
[02/27 19:49:05 d2.utils.events]:  eta: 1 day, 2:05:09  iter: 3039  total_loss: 1.394  loss_cls: 0.4465  loss_box_reg: 0.2703  loss_mask: 0.4319  loss_rpn_cls: 0.09554  loss_rpn_loc: 0.1161  time: 0.5205  last_time: 0.5454  data_time: 0.0259  last_data_time: 0.0739   lr: 0.0001  max_mem: 11521M
[02/27 19:49:15 d2.utils.events]:  eta: 1 day, 2:04:59  iter: 3059  total_loss: 1.183  loss_cls: 0.3243  loss_box_reg: 0.2374  loss_mask: 0.4325  loss_rpn_cls: 0.08053  loss_rpn_loc: 0.1622  time: 0.5205  last_time: 0.4940  data_time: 0.0225  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 19:49:25 d2.utils.events]:  eta: 1 day, 2:04:16  iter: 3079  total_loss: 1.432  loss_cls: 0.44  loss_box_reg: 0.2602  loss_mask: 0.4397  loss_rpn_cls: 0.08354  loss_rpn_loc: 0.1502  time: 0.5205  last_time: 0.5093  data_time: 0.0103  last_data_time: 0.0053   lr: 0.0001  max_mem: 11521M
[02/27 19:49:36 d2.utils.events]:  eta: 1 day, 2:04:31  iter: 3099  total_loss: 1.331  loss_cls: 0.3928  loss_box_reg: 0.2205  loss_mask: 0.4353  loss_rpn_cls: 0.07662  loss_rpn_loc: 0.1591  time: 0.5205  last_time: 0.5457  data_time: 0.0141  last_data_time: 0.0275   lr: 0.0001  max_mem: 11521M
[02/27 19:49:47 d2.utils.events]:  eta: 1 day, 2:05:35  iter: 3119  total_loss: 1.363  loss_cls: 0.3911  loss_box_reg: 0.2547  loss_mask: 0.4125  loss_rpn_cls: 0.08431  loss_rpn_loc: 0.1362  time: 0.5206  last_time: 0.5240  data_time: 0.0093  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 19:49:57 d2.utils.events]:  eta: 1 day, 2:06:25  iter: 3139  total_loss: 1.377  loss_cls: 0.397  loss_box_reg: 0.254  loss_mask: 0.42  loss_rpn_cls: 0.08077  loss_rpn_loc: 0.1776  time: 0.5207  last_time: 0.5313  data_time: 0.0144  last_data_time: 0.0050   lr: 0.0001  max_mem: 11521M
[02/27 19:50:08 d2.utils.events]:  eta: 1 day, 2:08:09  iter: 3159  total_loss: 1.364  loss_cls: 0.4116  loss_box_reg: 0.2514  loss_mask: 0.4548  loss_rpn_cls: 0.08943  loss_rpn_loc: 0.1398  time: 0.5207  last_time: 0.5251  data_time: 0.0130  last_data_time: 0.0092   lr: 0.0001  max_mem: 11521M
[02/27 19:50:19 d2.utils.events]:  eta: 1 day, 2:07:25  iter: 3179  total_loss: 1.432  loss_cls: 0.4023  loss_box_reg: 0.2688  loss_mask: 0.4797  loss_rpn_cls: 0.08726  loss_rpn_loc: 0.1809  time: 0.5207  last_time: 0.5356  data_time: 0.0134  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 19:50:29 d2.utils.events]:  eta: 1 day, 2:09:47  iter: 3199  total_loss: 1.188  loss_cls: 0.3105  loss_box_reg: 0.2072  loss_mask: 0.407  loss_rpn_cls: 0.07145  loss_rpn_loc: 0.1182  time: 0.5208  last_time: 0.5048  data_time: 0.0179  last_data_time: 0.0135   lr: 0.0001  max_mem: 11521M
[02/27 19:50:40 d2.utils.events]:  eta: 1 day, 2:09:37  iter: 3219  total_loss: 1.366  loss_cls: 0.393  loss_box_reg: 0.2281  loss_mask: 0.4179  loss_rpn_cls: 0.09042  loss_rpn_loc: 0.1534  time: 0.5208  last_time: 0.5190  data_time: 0.0190  last_data_time: 0.0120   lr: 0.0001  max_mem: 11521M
[02/27 19:50:50 d2.utils.events]:  eta: 1 day, 2:09:19  iter: 3239  total_loss: 1.186  loss_cls: 0.3131  loss_box_reg: 0.1908  loss_mask: 0.44  loss_rpn_cls: 0.0667  loss_rpn_loc: 0.1731  time: 0.5208  last_time: 0.5091  data_time: 0.0169  last_data_time: 0.0383   lr: 0.0001  max_mem: 11521M
[02/27 19:51:01 d2.utils.events]:  eta: 1 day, 2:10:03  iter: 3259  total_loss: 1.314  loss_cls: 0.3739  loss_box_reg: 0.2359  loss_mask: 0.4083  loss_rpn_cls: 0.09073  loss_rpn_loc: 0.1895  time: 0.5209  last_time: 0.5462  data_time: 0.0160  last_data_time: 0.0116   lr: 0.0001  max_mem: 11521M
[02/27 19:51:12 d2.utils.events]:  eta: 1 day, 2:10:27  iter: 3279  total_loss: 1.491  loss_cls: 0.4378  loss_box_reg: 0.2718  loss_mask: 0.4801  loss_rpn_cls: 0.0979  loss_rpn_loc: 0.1418  time: 0.5209  last_time: 0.5240  data_time: 0.0207  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 19:51:22 d2.utils.events]:  eta: 1 day, 2:10:44  iter: 3299  total_loss: 1.427  loss_cls: 0.4216  loss_box_reg: 0.2593  loss_mask: 0.4562  loss_rpn_cls: 0.09236  loss_rpn_loc: 0.2009  time: 0.5209  last_time: 0.5339  data_time: 0.0186  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 19:51:32 d2.utils.events]:  eta: 1 day, 2:10:41  iter: 3319  total_loss: 1.32  loss_cls: 0.3924  loss_box_reg: 0.2518  loss_mask: 0.4222  loss_rpn_cls: 0.09362  loss_rpn_loc: 0.1442  time: 0.5209  last_time: 0.5031  data_time: 0.0120  last_data_time: 0.0051   lr: 0.0001  max_mem: 11521M
[02/27 19:51:43 d2.utils.events]:  eta: 1 day, 2:11:28  iter: 3339  total_loss: 1.454  loss_cls: 0.3909  loss_box_reg: 0.2423  loss_mask: 0.4534  loss_rpn_cls: 0.09014  loss_rpn_loc: 0.159  time: 0.5209  last_time: 0.5163  data_time: 0.0193  last_data_time: 0.0118   lr: 0.0001  max_mem: 11521M
[02/27 19:51:53 d2.utils.events]:  eta: 1 day, 2:11:35  iter: 3359  total_loss: 1.164  loss_cls: 0.3273  loss_box_reg: 0.1965  loss_mask: 0.4101  loss_rpn_cls: 0.08465  loss_rpn_loc: 0.152  time: 0.5209  last_time: 0.5250  data_time: 0.0221  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 19:52:04 d2.utils.events]:  eta: 1 day, 2:11:39  iter: 3379  total_loss: 1.309  loss_cls: 0.3555  loss_box_reg: 0.2453  loss_mask: 0.4141  loss_rpn_cls: 0.07863  loss_rpn_loc: 0.1324  time: 0.5209  last_time: 0.5106  data_time: 0.0258  last_data_time: 0.0247   lr: 0.0001  max_mem: 11521M
[02/27 19:52:14 d2.utils.events]:  eta: 1 day, 2:10:45  iter: 3399  total_loss: 1.317  loss_cls: 0.4331  loss_box_reg: 0.2551  loss_mask: 0.4211  loss_rpn_cls: 0.07368  loss_rpn_loc: 0.1285  time: 0.5209  last_time: 0.5037  data_time: 0.0176  last_data_time: 0.0139   lr: 0.0001  max_mem: 11521M
[02/27 19:52:25 d2.utils.events]:  eta: 1 day, 2:10:26  iter: 3419  total_loss: 1.24  loss_cls: 0.3623  loss_box_reg: 0.2202  loss_mask: 0.4125  loss_rpn_cls: 0.09783  loss_rpn_loc: 0.1574  time: 0.5209  last_time: 0.5172  data_time: 0.0164  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 19:52:35 d2.utils.events]:  eta: 1 day, 2:10:02  iter: 3439  total_loss: 1.425  loss_cls: 0.4069  loss_box_reg: 0.249  loss_mask: 0.447  loss_rpn_cls: 0.0842  loss_rpn_loc: 0.1379  time: 0.5208  last_time: 0.5220  data_time: 0.0108  last_data_time: 0.0107   lr: 0.0001  max_mem: 11521M
[02/27 19:52:46 d2.utils.events]:  eta: 1 day, 2:11:59  iter: 3459  total_loss: 1.311  loss_cls: 0.3946  loss_box_reg: 0.2544  loss_mask: 0.4236  loss_rpn_cls: 0.07143  loss_rpn_loc: 0.1273  time: 0.5210  last_time: 0.5403  data_time: 0.0214  last_data_time: 0.0399   lr: 0.0001  max_mem: 11521M
[02/27 19:52:57 d2.utils.events]:  eta: 1 day, 2:11:59  iter: 3479  total_loss: 1.346  loss_cls: 0.3765  loss_box_reg: 0.2521  loss_mask: 0.43  loss_rpn_cls: 0.07255  loss_rpn_loc: 0.1491  time: 0.5210  last_time: 0.4721  data_time: 0.0186  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 19:53:07 d2.utils.events]:  eta: 1 day, 2:11:38  iter: 3499  total_loss: 1.361  loss_cls: 0.3667  loss_box_reg: 0.2748  loss_mask: 0.4  loss_rpn_cls: 0.08691  loss_rpn_loc: 0.1233  time: 0.5210  last_time: 0.5315  data_time: 0.0164  last_data_time: 0.0251   lr: 0.0001  max_mem: 11521M
[02/27 19:53:18 d2.utils.events]:  eta: 1 day, 2:12:01  iter: 3519  total_loss: 1.314  loss_cls: 0.4064  loss_box_reg: 0.2442  loss_mask: 0.4432  loss_rpn_cls: 0.08544  loss_rpn_loc: 0.09095  time: 0.5210  last_time: 0.4926  data_time: 0.0155  last_data_time: 0.0111   lr: 0.0001  max_mem: 11521M
[02/27 19:53:28 d2.utils.events]:  eta: 1 day, 2:11:07  iter: 3539  total_loss: 1.28  loss_cls: 0.3693  loss_box_reg: 0.2575  loss_mask: 0.4113  loss_rpn_cls: 0.0855  loss_rpn_loc: 0.09953  time: 0.5209  last_time: 0.5106  data_time: 0.0107  last_data_time: 0.0102   lr: 0.0001  max_mem: 11521M
[02/27 19:53:39 d2.utils.events]:  eta: 1 day, 2:11:29  iter: 3559  total_loss: 1.348  loss_cls: 0.3769  loss_box_reg: 0.2422  loss_mask: 0.4234  loss_rpn_cls: 0.08309  loss_rpn_loc: 0.1763  time: 0.5209  last_time: 0.5225  data_time: 0.0246  last_data_time: 0.0677   lr: 0.0001  max_mem: 11521M
[02/27 19:53:49 d2.utils.events]:  eta: 1 day, 2:10:28  iter: 3579  total_loss: 1.095  loss_cls: 0.3469  loss_box_reg: 0.2283  loss_mask: 0.3766  loss_rpn_cls: 0.06126  loss_rpn_loc: 0.1228  time: 0.5209  last_time: 0.5046  data_time: 0.0218  last_data_time: 0.0287   lr: 0.0001  max_mem: 11521M
[02/27 19:53:59 d2.utils.events]:  eta: 1 day, 2:09:47  iter: 3599  total_loss: 1.31  loss_cls: 0.3738  loss_box_reg: 0.2229  loss_mask: 0.429  loss_rpn_cls: 0.1115  loss_rpn_loc: 0.1566  time: 0.5208  last_time: 0.5305  data_time: 0.0139  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 19:54:10 d2.utils.events]:  eta: 1 day, 2:09:51  iter: 3619  total_loss: 1.445  loss_cls: 0.3922  loss_box_reg: 0.2815  loss_mask: 0.45  loss_rpn_cls: 0.09249  loss_rpn_loc: 0.1739  time: 0.5208  last_time: 0.5048  data_time: 0.0198  last_data_time: 0.0286   lr: 0.0001  max_mem: 11521M
[02/27 19:54:20 d2.utils.events]:  eta: 1 day, 2:09:23  iter: 3639  total_loss: 1.355  loss_cls: 0.4004  loss_box_reg: 0.2576  loss_mask: 0.4328  loss_rpn_cls: 0.09043  loss_rpn_loc: 0.1567  time: 0.5208  last_time: 0.4893  data_time: 0.0215  last_data_time: 0.0120   lr: 0.0001  max_mem: 11521M
[02/27 19:54:30 d2.utils.events]:  eta: 1 day, 2:09:16  iter: 3659  total_loss: 1.13  loss_cls: 0.3331  loss_box_reg: 0.2141  loss_mask: 0.4052  loss_rpn_cls: 0.07263  loss_rpn_loc: 0.1049  time: 0.5208  last_time: 0.5104  data_time: 0.0182  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 19:54:41 d2.utils.events]:  eta: 1 day, 2:08:19  iter: 3679  total_loss: 1.217  loss_cls: 0.3257  loss_box_reg: 0.1901  loss_mask: 0.3883  loss_rpn_cls: 0.06856  loss_rpn_loc: 0.1233  time: 0.5208  last_time: 0.5005  data_time: 0.0141  last_data_time: 0.0172   lr: 0.0001  max_mem: 11521M
[02/27 19:54:51 d2.utils.events]:  eta: 1 day, 2:08:34  iter: 3699  total_loss: 1.198  loss_cls: 0.3627  loss_box_reg: 0.2132  loss_mask: 0.3969  loss_rpn_cls: 0.06319  loss_rpn_loc: 0.1008  time: 0.5207  last_time: 0.5462  data_time: 0.0242  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 19:55:01 d2.utils.events]:  eta: 1 day, 2:08:05  iter: 3719  total_loss: 1.317  loss_cls: 0.3885  loss_box_reg: 0.25  loss_mask: 0.4287  loss_rpn_cls: 0.09144  loss_rpn_loc: 0.1092  time: 0.5207  last_time: 0.5017  data_time: 0.0146  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 19:55:12 d2.utils.events]:  eta: 1 day, 2:09:05  iter: 3739  total_loss: 1.335  loss_cls: 0.417  loss_box_reg: 0.2274  loss_mask: 0.4064  loss_rpn_cls: 0.08486  loss_rpn_loc: 0.1161  time: 0.5207  last_time: 0.5014  data_time: 0.0172  last_data_time: 0.0537   lr: 0.0001  max_mem: 11521M
[02/27 19:55:23 d2.utils.events]:  eta: 1 day, 2:09:22  iter: 3759  total_loss: 1.246  loss_cls: 0.3602  loss_box_reg: 0.2136  loss_mask: 0.4075  loss_rpn_cls: 0.07713  loss_rpn_loc: 0.1413  time: 0.5207  last_time: 0.5281  data_time: 0.0173  last_data_time: 0.0131   lr: 0.0001  max_mem: 11521M
[02/27 19:55:33 d2.utils.events]:  eta: 1 day, 2:10:00  iter: 3779  total_loss: 1.257  loss_cls: 0.3506  loss_box_reg: 0.2344  loss_mask: 0.4082  loss_rpn_cls: 0.08478  loss_rpn_loc: 0.13  time: 0.5207  last_time: 0.5013  data_time: 0.0329  last_data_time: 0.0060   lr: 0.0001  max_mem: 11521M
[02/27 19:55:43 d2.utils.events]:  eta: 1 day, 2:09:42  iter: 3799  total_loss: 1.221  loss_cls: 0.3518  loss_box_reg: 0.2273  loss_mask: 0.3838  loss_rpn_cls: 0.1101  loss_rpn_loc: 0.1188  time: 0.5207  last_time: 0.5231  data_time: 0.0179  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 19:55:54 d2.utils.events]:  eta: 1 day, 2:10:27  iter: 3819  total_loss: 1.267  loss_cls: 0.3915  loss_box_reg: 0.2466  loss_mask: 0.4097  loss_rpn_cls: 0.07531  loss_rpn_loc: 0.1589  time: 0.5207  last_time: 0.4984  data_time: 0.0088  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 19:56:05 d2.utils.events]:  eta: 1 day, 2:10:25  iter: 3839  total_loss: 1.409  loss_cls: 0.4249  loss_box_reg: 0.2514  loss_mask: 0.4301  loss_rpn_cls: 0.07199  loss_rpn_loc: 0.1362  time: 0.5208  last_time: 0.5537  data_time: 0.0133  last_data_time: 0.0522   lr: 0.0001  max_mem: 11521M
[02/27 19:56:15 d2.utils.events]:  eta: 1 day, 2:11:06  iter: 3859  total_loss: 1.275  loss_cls: 0.3615  loss_box_reg: 0.2334  loss_mask: 0.4241  loss_rpn_cls: 0.07982  loss_rpn_loc: 0.1233  time: 0.5208  last_time: 0.5287  data_time: 0.0217  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 19:56:26 d2.utils.events]:  eta: 1 day, 2:12:20  iter: 3879  total_loss: 1.366  loss_cls: 0.3939  loss_box_reg: 0.2465  loss_mask: 0.4372  loss_rpn_cls: 0.08234  loss_rpn_loc: 0.1879  time: 0.5209  last_time: 0.4924  data_time: 0.0136  last_data_time: 0.0032   lr: 0.0001  max_mem: 11521M
[02/27 19:56:37 d2.utils.events]:  eta: 1 day, 2:12:29  iter: 3899  total_loss: 1.237  loss_cls: 0.337  loss_box_reg: 0.2455  loss_mask: 0.4342  loss_rpn_cls: 0.07168  loss_rpn_loc: 0.1205  time: 0.5210  last_time: 0.5177  data_time: 0.0104  last_data_time: 0.0123   lr: 0.0001  max_mem: 11521M
[02/27 19:56:47 d2.utils.events]:  eta: 1 day, 2:12:24  iter: 3919  total_loss: 1.307  loss_cls: 0.3433  loss_box_reg: 0.2342  loss_mask: 0.3803  loss_rpn_cls: 0.1023  loss_rpn_loc: 0.1627  time: 0.5210  last_time: 0.4951  data_time: 0.0151  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 19:56:58 d2.utils.events]:  eta: 1 day, 2:12:51  iter: 3939  total_loss: 1.186  loss_cls: 0.304  loss_box_reg: 0.2216  loss_mask: 0.4402  loss_rpn_cls: 0.07451  loss_rpn_loc: 0.1269  time: 0.5210  last_time: 0.5187  data_time: 0.0115  last_data_time: 0.0056   lr: 0.0001  max_mem: 11521M
[02/27 19:57:08 d2.utils.events]:  eta: 1 day, 2:12:52  iter: 3959  total_loss: 1.295  loss_cls: 0.3708  loss_box_reg: 0.2487  loss_mask: 0.4247  loss_rpn_cls: 0.08425  loss_rpn_loc: 0.123  time: 0.5210  last_time: 0.5246  data_time: 0.0306  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 19:57:19 d2.utils.events]:  eta: 1 day, 2:12:54  iter: 3979  total_loss: 1.177  loss_cls: 0.2719  loss_box_reg: 0.1771  loss_mask: 0.4154  loss_rpn_cls: 0.06599  loss_rpn_loc: 0.1519  time: 0.5210  last_time: 0.5163  data_time: 0.0159  last_data_time: 0.0312   lr: 0.0001  max_mem: 11521M
[02/27 19:57:30 d2.utils.events]:  eta: 1 day, 2:13:09  iter: 3999  total_loss: 1.319  loss_cls: 0.3619  loss_box_reg: 0.2495  loss_mask: 0.4157  loss_rpn_cls: 0.0886  loss_rpn_loc: 0.1499  time: 0.5211  last_time: 0.5816  data_time: 0.0204  last_data_time: 0.0343   lr: 0.0001  max_mem: 11521M
[02/27 19:57:40 d2.utils.events]:  eta: 1 day, 2:11:26  iter: 4019  total_loss: 1.286  loss_cls: 0.3824  loss_box_reg: 0.2505  loss_mask: 0.431  loss_rpn_cls: 0.09006  loss_rpn_loc: 0.1717  time: 0.5210  last_time: 0.5498  data_time: 0.0221  last_data_time: 0.0665   lr: 0.0001  max_mem: 11521M
[02/27 19:57:50 d2.utils.events]:  eta: 1 day, 2:10:10  iter: 4039  total_loss: 1.151  loss_cls: 0.3217  loss_box_reg: 0.2007  loss_mask: 0.3795  loss_rpn_cls: 0.07087  loss_rpn_loc: 0.1142  time: 0.5210  last_time: 0.5178  data_time: 0.0192  last_data_time: 0.0061   lr: 0.0001  max_mem: 11521M
[02/27 19:58:01 d2.utils.events]:  eta: 1 day, 2:10:43  iter: 4059  total_loss: 1.271  loss_cls: 0.3991  loss_box_reg: 0.2385  loss_mask: 0.4172  loss_rpn_cls: 0.07154  loss_rpn_loc: 0.09951  time: 0.5210  last_time: 0.5114  data_time: 0.0281  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 19:58:11 d2.utils.events]:  eta: 1 day, 2:10:18  iter: 4079  total_loss: 1.394  loss_cls: 0.4081  loss_box_reg: 0.2732  loss_mask: 0.4145  loss_rpn_cls: 0.1063  loss_rpn_loc: 0.1825  time: 0.5210  last_time: 0.5046  data_time: 0.0149  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 19:58:22 d2.utils.events]:  eta: 1 day, 2:09:01  iter: 4099  total_loss: 1.379  loss_cls: 0.3621  loss_box_reg: 0.2642  loss_mask: 0.428  loss_rpn_cls: 0.07735  loss_rpn_loc: 0.1475  time: 0.5210  last_time: 0.5761  data_time: 0.0124  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 19:58:32 d2.utils.events]:  eta: 1 day, 2:08:50  iter: 4119  total_loss: 1.417  loss_cls: 0.4257  loss_box_reg: 0.2849  loss_mask: 0.4079  loss_rpn_cls: 0.09987  loss_rpn_loc: 0.1797  time: 0.5210  last_time: 0.5488  data_time: 0.0261  last_data_time: 0.0226   lr: 0.0001  max_mem: 11521M
[02/27 19:58:43 d2.utils.events]:  eta: 1 day, 2:09:25  iter: 4139  total_loss: 1.389  loss_cls: 0.4315  loss_box_reg: 0.2852  loss_mask: 0.4201  loss_rpn_cls: 0.09057  loss_rpn_loc: 0.1458  time: 0.5210  last_time: 0.4974  data_time: 0.0193  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 19:58:54 d2.utils.events]:  eta: 1 day, 2:09:50  iter: 4159  total_loss: 1.294  loss_cls: 0.3918  loss_box_reg: 0.2525  loss_mask: 0.3906  loss_rpn_cls: 0.09688  loss_rpn_loc: 0.1609  time: 0.5211  last_time: 0.5377  data_time: 0.0213  last_data_time: 0.0051   lr: 0.0001  max_mem: 11521M
[02/27 19:59:04 d2.utils.events]:  eta: 1 day, 2:09:16  iter: 4179  total_loss: 1.256  loss_cls: 0.3243  loss_box_reg: 0.1996  loss_mask: 0.3886  loss_rpn_cls: 0.09071  loss_rpn_loc: 0.1841  time: 0.5211  last_time: 0.4772  data_time: 0.0161  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 19:59:15 d2.utils.events]:  eta: 1 day, 2:09:16  iter: 4199  total_loss: 1.496  loss_cls: 0.4557  loss_box_reg: 0.2916  loss_mask: 0.4331  loss_rpn_cls: 0.07984  loss_rpn_loc: 0.1231  time: 0.5212  last_time: 0.5466  data_time: 0.0251  last_data_time: 0.0112   lr: 0.0001  max_mem: 11521M
[02/27 19:59:26 d2.utils.events]:  eta: 1 day, 2:09:05  iter: 4219  total_loss: 1.355  loss_cls: 0.3776  loss_box_reg: 0.2616  loss_mask: 0.4389  loss_rpn_cls: 0.09216  loss_rpn_loc: 0.2319  time: 0.5212  last_time: 0.5388  data_time: 0.0124  last_data_time: 0.0794   lr: 0.0001  max_mem: 11521M
[02/27 19:59:36 d2.utils.events]:  eta: 1 day, 2:09:45  iter: 4239  total_loss: 1.325  loss_cls: 0.3499  loss_box_reg: 0.2382  loss_mask: 0.4276  loss_rpn_cls: 0.07194  loss_rpn_loc: 0.1607  time: 0.5213  last_time: 0.5179  data_time: 0.0183  last_data_time: 0.0130   lr: 0.0001  max_mem: 11521M
[02/27 19:59:47 d2.utils.events]:  eta: 1 day, 2:09:35  iter: 4259  total_loss: 1.333  loss_cls: 0.3991  loss_box_reg: 0.2579  loss_mask: 0.3866  loss_rpn_cls: 0.07104  loss_rpn_loc: 0.1592  time: 0.5213  last_time: 0.5402  data_time: 0.0156  last_data_time: 0.0386   lr: 0.0001  max_mem: 11521M
[02/27 19:59:57 d2.utils.events]:  eta: 1 day, 2:09:51  iter: 4279  total_loss: 1.302  loss_cls: 0.326  loss_box_reg: 0.2057  loss_mask: 0.412  loss_rpn_cls: 0.1031  loss_rpn_loc: 0.09887  time: 0.5213  last_time: 0.5082  data_time: 0.0169  last_data_time: 0.0208   lr: 0.0001  max_mem: 11521M
[02/27 20:00:08 d2.utils.events]:  eta: 1 day, 2:09:54  iter: 4299  total_loss: 1.429  loss_cls: 0.4178  loss_box_reg: 0.2866  loss_mask: 0.3882  loss_rpn_cls: 0.07543  loss_rpn_loc: 0.1672  time: 0.5214  last_time: 0.5090  data_time: 0.0297  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 20:00:19 d2.utils.events]:  eta: 1 day, 2:09:30  iter: 4319  total_loss: 1.149  loss_cls: 0.3418  loss_box_reg: 0.2395  loss_mask: 0.408  loss_rpn_cls: 0.05612  loss_rpn_loc: 0.1213  time: 0.5214  last_time: 0.5141  data_time: 0.0125  last_data_time: 0.0104   lr: 0.0001  max_mem: 11521M
[02/27 20:00:29 d2.utils.events]:  eta: 1 day, 2:09:31  iter: 4339  total_loss: 1.161  loss_cls: 0.3651  loss_box_reg: 0.2262  loss_mask: 0.4  loss_rpn_cls: 0.07305  loss_rpn_loc: 0.09732  time: 0.5214  last_time: 0.4891  data_time: 0.0115  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 20:00:40 d2.utils.events]:  eta: 1 day, 2:09:21  iter: 4359  total_loss: 1.166  loss_cls: 0.315  loss_box_reg: 0.22  loss_mask: 0.3649  loss_rpn_cls: 0.06335  loss_rpn_loc: 0.1229  time: 0.5214  last_time: 0.5126  data_time: 0.0157  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:00:50 d2.utils.events]:  eta: 1 day, 2:09:29  iter: 4379  total_loss: 1.265  loss_cls: 0.3695  loss_box_reg: 0.2221  loss_mask: 0.4164  loss_rpn_cls: 0.07694  loss_rpn_loc: 0.1502  time: 0.5214  last_time: 0.5072  data_time: 0.0210  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 20:01:01 d2.utils.events]:  eta: 1 day, 2:09:38  iter: 4399  total_loss: 1.154  loss_cls: 0.321  loss_box_reg: 0.2103  loss_mask: 0.3996  loss_rpn_cls: 0.06895  loss_rpn_loc: 0.1006  time: 0.5214  last_time: 0.5089  data_time: 0.0116  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 20:01:11 d2.utils.events]:  eta: 1 day, 2:10:56  iter: 4419  total_loss: 1.332  loss_cls: 0.3704  loss_box_reg: 0.2282  loss_mask: 0.3944  loss_rpn_cls: 0.08757  loss_rpn_loc: 0.174  time: 0.5214  last_time: 0.5607  data_time: 0.0324  last_data_time: 0.0599   lr: 0.0001  max_mem: 11521M
[02/27 20:01:22 d2.utils.events]:  eta: 1 day, 2:12:01  iter: 4439  total_loss: 1.183  loss_cls: 0.3328  loss_box_reg: 0.1976  loss_mask: 0.3934  loss_rpn_cls: 0.0786  loss_rpn_loc: 0.1282  time: 0.5214  last_time: 0.5122  data_time: 0.0125  last_data_time: 0.0049   lr: 0.0001  max_mem: 11521M
[02/27 20:01:32 d2.utils.events]:  eta: 1 day, 2:10:15  iter: 4459  total_loss: 1.095  loss_cls: 0.3066  loss_box_reg: 0.1907  loss_mask: 0.4018  loss_rpn_cls: 0.06988  loss_rpn_loc: 0.1141  time: 0.5215  last_time: 0.5307  data_time: 0.0095  last_data_time: 0.0124   lr: 0.0001  max_mem: 11521M
[02/27 20:01:43 d2.utils.events]:  eta: 1 day, 2:11:45  iter: 4479  total_loss: 1.434  loss_cls: 0.4201  loss_box_reg: 0.2827  loss_mask: 0.43  loss_rpn_cls: 0.0888  loss_rpn_loc: 0.1033  time: 0.5215  last_time: 0.5118  data_time: 0.0303  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 20:01:54 d2.utils.events]:  eta: 1 day, 2:11:51  iter: 4499  total_loss: 1.357  loss_cls: 0.439  loss_box_reg: 0.2731  loss_mask: 0.4109  loss_rpn_cls: 0.08259  loss_rpn_loc: 0.1213  time: 0.5215  last_time: 0.5041  data_time: 0.0187  last_data_time: 0.0027   lr: 0.0001  max_mem: 11521M
[02/27 20:02:04 d2.utils.events]:  eta: 1 day, 2:12:04  iter: 4519  total_loss: 1.265  loss_cls: 0.4118  loss_box_reg: 0.2535  loss_mask: 0.3968  loss_rpn_cls: 0.07999  loss_rpn_loc: 0.1075  time: 0.5215  last_time: 0.5493  data_time: 0.0259  last_data_time: 0.0266   lr: 0.0001  max_mem: 11521M
[02/27 20:02:15 d2.utils.events]:  eta: 1 day, 2:12:57  iter: 4539  total_loss: 1.193  loss_cls: 0.3381  loss_box_reg: 0.2149  loss_mask: 0.3969  loss_rpn_cls: 0.07346  loss_rpn_loc: 0.1386  time: 0.5215  last_time: 0.5236  data_time: 0.0175  last_data_time: 0.0106   lr: 0.0001  max_mem: 11521M
[02/27 20:02:25 d2.utils.events]:  eta: 1 day, 2:12:22  iter: 4559  total_loss: 1.299  loss_cls: 0.337  loss_box_reg: 0.2432  loss_mask: 0.3881  loss_rpn_cls: 0.07759  loss_rpn_loc: 0.1414  time: 0.5215  last_time: 0.5326  data_time: 0.0144  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 20:02:36 d2.utils.events]:  eta: 1 day, 2:13:22  iter: 4579  total_loss: 1.26  loss_cls: 0.3631  loss_box_reg: 0.2278  loss_mask: 0.4305  loss_rpn_cls: 0.06927  loss_rpn_loc: 0.1862  time: 0.5216  last_time: 0.4870  data_time: 0.0253  last_data_time: 0.0281   lr: 0.0001  max_mem: 11521M
[02/27 20:02:46 d2.utils.events]:  eta: 1 day, 2:13:12  iter: 4599  total_loss: 1.253  loss_cls: 0.3427  loss_box_reg: 0.2259  loss_mask: 0.4091  loss_rpn_cls: 0.1017  loss_rpn_loc: 0.1701  time: 0.5215  last_time: 0.5013  data_time: 0.0203  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 20:02:57 d2.utils.events]:  eta: 1 day, 2:12:53  iter: 4619  total_loss: 1.276  loss_cls: 0.3821  loss_box_reg: 0.2392  loss_mask: 0.4036  loss_rpn_cls: 0.06946  loss_rpn_loc: 0.1148  time: 0.5215  last_time: 0.5211  data_time: 0.0220  last_data_time: 0.0404   lr: 0.0001  max_mem: 11521M
[02/27 20:03:07 d2.utils.events]:  eta: 1 day, 2:12:59  iter: 4639  total_loss: 1.325  loss_cls: 0.3912  loss_box_reg: 0.2483  loss_mask: 0.4114  loss_rpn_cls: 0.07935  loss_rpn_loc: 0.16  time: 0.5215  last_time: 0.5257  data_time: 0.0208  last_data_time: 0.0249   lr: 0.0001  max_mem: 11521M
[02/27 20:03:18 d2.utils.events]:  eta: 1 day, 2:13:15  iter: 4659  total_loss: 1.197  loss_cls: 0.3575  loss_box_reg: 0.2115  loss_mask: 0.3956  loss_rpn_cls: 0.06608  loss_rpn_loc: 0.09535  time: 0.5215  last_time: 0.5093  data_time: 0.0269  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 20:03:28 d2.utils.events]:  eta: 1 day, 2:13:36  iter: 4679  total_loss: 1.255  loss_cls: 0.3086  loss_box_reg: 0.1806  loss_mask: 0.4195  loss_rpn_cls: 0.1014  loss_rpn_loc: 0.1862  time: 0.5216  last_time: 0.5242  data_time: 0.0205  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 20:03:39 d2.utils.events]:  eta: 1 day, 2:13:10  iter: 4699  total_loss: 1.469  loss_cls: 0.3719  loss_box_reg: 0.2452  loss_mask: 0.4537  loss_rpn_cls: 0.07572  loss_rpn_loc: 0.2251  time: 0.5216  last_time: 0.5156  data_time: 0.0119  last_data_time: 0.0053   lr: 0.0001  max_mem: 11521M
[02/27 20:03:49 d2.utils.events]:  eta: 1 day, 2:13:48  iter: 4719  total_loss: 1.332  loss_cls: 0.3309  loss_box_reg: 0.2533  loss_mask: 0.4235  loss_rpn_cls: 0.08754  loss_rpn_loc: 0.1225  time: 0.5216  last_time: 0.5292  data_time: 0.0133  last_data_time: 0.0504   lr: 0.0001  max_mem: 11521M
[02/27 20:04:00 d2.utils.events]:  eta: 1 day, 2:13:29  iter: 4739  total_loss: 1.582  loss_cls: 0.4653  loss_box_reg: 0.2848  loss_mask: 0.4681  loss_rpn_cls: 0.09228  loss_rpn_loc: 0.1681  time: 0.5216  last_time: 0.5511  data_time: 0.0198  last_data_time: 0.0685   lr: 0.0001  max_mem: 11521M
[02/27 20:04:11 d2.utils.events]:  eta: 1 day, 2:13:30  iter: 4759  total_loss: 1.375  loss_cls: 0.3406  loss_box_reg: 0.2026  loss_mask: 0.4433  loss_rpn_cls: 0.1221  loss_rpn_loc: 0.177  time: 0.5216  last_time: 0.5413  data_time: 0.0263  last_data_time: 0.0217   lr: 0.0001  max_mem: 11521M
[02/27 20:04:21 d2.utils.events]:  eta: 1 day, 2:13:20  iter: 4779  total_loss: 1.223  loss_cls: 0.3179  loss_box_reg: 0.2186  loss_mask: 0.4093  loss_rpn_cls: 0.08262  loss_rpn_loc: 0.1465  time: 0.5216  last_time: 0.5340  data_time: 0.0161  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 20:04:31 d2.utils.events]:  eta: 1 day, 2:13:06  iter: 4799  total_loss: 1.117  loss_cls: 0.2796  loss_box_reg: 0.1898  loss_mask: 0.3927  loss_rpn_cls: 0.09356  loss_rpn_loc: 0.09065  time: 0.5216  last_time: 0.5055  data_time: 0.0171  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 20:04:42 d2.utils.events]:  eta: 1 day, 2:12:52  iter: 4819  total_loss: 1.271  loss_cls: 0.3762  loss_box_reg: 0.2351  loss_mask: 0.3977  loss_rpn_cls: 0.07806  loss_rpn_loc: 0.1413  time: 0.5216  last_time: 0.5207  data_time: 0.0237  last_data_time: 0.0412   lr: 0.0001  max_mem: 11521M
[02/27 20:04:52 d2.utils.events]:  eta: 1 day, 2:11:40  iter: 4839  total_loss: 1.201  loss_cls: 0.3571  loss_box_reg: 0.2465  loss_mask: 0.4059  loss_rpn_cls: 0.06553  loss_rpn_loc: 0.1139  time: 0.5215  last_time: 0.5006  data_time: 0.0129  last_data_time: 0.0050   lr: 0.0001  max_mem: 11521M
[02/27 20:05:03 d2.utils.events]:  eta: 1 day, 2:10:44  iter: 4859  total_loss: 1.252  loss_cls: 0.3531  loss_box_reg: 0.255  loss_mask: 0.4321  loss_rpn_cls: 0.06846  loss_rpn_loc: 0.1384  time: 0.5215  last_time: 0.5422  data_time: 0.0197  last_data_time: 0.0259   lr: 0.0001  max_mem: 11521M
[02/27 20:05:13 d2.utils.events]:  eta: 1 day, 2:09:24  iter: 4879  total_loss: 1.229  loss_cls: 0.3489  loss_box_reg: 0.2218  loss_mask: 0.4213  loss_rpn_cls: 0.07971  loss_rpn_loc: 0.1265  time: 0.5215  last_time: 0.5306  data_time: 0.0127  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 20:05:24 d2.utils.events]:  eta: 1 day, 2:08:39  iter: 4899  total_loss: 1.131  loss_cls: 0.3066  loss_box_reg: 0.1933  loss_mask: 0.3973  loss_rpn_cls: 0.06713  loss_rpn_loc: 0.119  time: 0.5215  last_time: 0.5405  data_time: 0.0112  last_data_time: 0.0364   lr: 0.0001  max_mem: 11521M
[02/27 20:05:34 d2.utils.events]:  eta: 1 day, 2:08:20  iter: 4919  total_loss: 1.158  loss_cls: 0.2973  loss_box_reg: 0.2117  loss_mask: 0.3805  loss_rpn_cls: 0.06198  loss_rpn_loc: 0.172  time: 0.5215  last_time: 0.5164  data_time: 0.0150  last_data_time: 0.0093   lr: 0.0001  max_mem: 11521M
[02/27 20:05:44 d2.utils.events]:  eta: 1 day, 2:07:20  iter: 4939  total_loss: 1.215  loss_cls: 0.3309  loss_box_reg: 0.2235  loss_mask: 0.401  loss_rpn_cls: 0.08935  loss_rpn_loc: 0.1408  time: 0.5215  last_time: 0.5135  data_time: 0.0244  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 20:05:55 d2.utils.events]:  eta: 1 day, 2:06:25  iter: 4959  total_loss: 1.149  loss_cls: 0.3753  loss_box_reg: 0.2727  loss_mask: 0.389  loss_rpn_cls: 0.09075  loss_rpn_loc: 0.1132  time: 0.5215  last_time: 0.5100  data_time: 0.0230  last_data_time: 0.0220   lr: 0.0001  max_mem: 11521M
[02/27 20:06:05 d2.utils.events]:  eta: 1 day, 2:06:15  iter: 4979  total_loss: 1.224  loss_cls: 0.3619  loss_box_reg: 0.238  loss_mask: 0.4009  loss_rpn_cls: 0.08132  loss_rpn_loc: 0.1388  time: 0.5215  last_time: 0.5230  data_time: 0.0211  last_data_time: 0.0529   lr: 0.0001  max_mem: 11521M
[02/27 20:06:16 fvcore.common.checkpoint]: Saving checkpoint to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/model_0004999.pth
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541035/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541035/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541035/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[02/27 20:06:18 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_val2017.json
[02/27 20:06:18 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |
[02/27 20:06:18 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]
[02/27 20:06:18 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/27 20:06:18 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...
[02/27 20:06:18 d2.data.common]: Serialized dataset takes 19.19 MiB
[02/27 20:06:19 d2.evaluation.evaluator]: Start inference on 1250 batches
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541035/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[02/27 20:06:25 d2.evaluation.evaluator]: Inference done 11/1250. Dataloading: 0.0015 s/iter. Inference: 0.3232 s/iter. Eval: 0.0520 s/iter. Total: 0.3767 s/iter. ETA=0:07:46
[02/27 20:06:30 d2.evaluation.evaluator]: Inference done 26/1250. Dataloading: 0.0026 s/iter. Inference: 0.3120 s/iter. Eval: 0.0397 s/iter. Total: 0.3544 s/iter. ETA=0:07:13
[02/27 20:06:36 d2.evaluation.evaluator]: Inference done 42/1250. Dataloading: 0.0029 s/iter. Inference: 0.3014 s/iter. Eval: 0.0357 s/iter. Total: 0.3400 s/iter. ETA=0:06:50
[02/27 20:06:41 d2.evaluation.evaluator]: Inference done 57/1250. Dataloading: 0.0026 s/iter. Inference: 0.3030 s/iter. Eval: 0.0348 s/iter. Total: 0.3406 s/iter. ETA=0:06:46
[02/27 20:06:46 d2.evaluation.evaluator]: Inference done 71/1250. Dataloading: 0.0027 s/iter. Inference: 0.3091 s/iter. Eval: 0.0343 s/iter. Total: 0.3464 s/iter. ETA=0:06:48
[02/27 20:06:51 d2.evaluation.evaluator]: Inference done 86/1250. Dataloading: 0.0027 s/iter. Inference: 0.3092 s/iter. Eval: 0.0334 s/iter. Total: 0.3454 s/iter. ETA=0:06:42
[02/27 20:06:56 d2.evaluation.evaluator]: Inference done 101/1250. Dataloading: 0.0026 s/iter. Inference: 0.3110 s/iter. Eval: 0.0335 s/iter. Total: 0.3472 s/iter. ETA=0:06:38
[02/27 20:07:01 d2.evaluation.evaluator]: Inference done 116/1250. Dataloading: 0.0025 s/iter. Inference: 0.3094 s/iter. Eval: 0.0338 s/iter. Total: 0.3459 s/iter. ETA=0:06:32
[02/27 20:07:06 d2.evaluation.evaluator]: Inference done 131/1250. Dataloading: 0.0025 s/iter. Inference: 0.3065 s/iter. Eval: 0.0357 s/iter. Total: 0.3448 s/iter. ETA=0:06:25
[02/27 20:07:11 d2.evaluation.evaluator]: Inference done 147/1250. Dataloading: 0.0025 s/iter. Inference: 0.3033 s/iter. Eval: 0.0354 s/iter. Total: 0.3413 s/iter. ETA=0:06:16
[02/27 20:07:17 d2.evaluation.evaluator]: Inference done 162/1250. Dataloading: 0.0025 s/iter. Inference: 0.3027 s/iter. Eval: 0.0357 s/iter. Total: 0.3410 s/iter. ETA=0:06:11
[02/27 20:07:22 d2.evaluation.evaluator]: Inference done 176/1250. Dataloading: 0.0027 s/iter. Inference: 0.3039 s/iter. Eval: 0.0357 s/iter. Total: 0.3424 s/iter. ETA=0:06:07
[02/27 20:07:27 d2.evaluation.evaluator]: Inference done 190/1250. Dataloading: 0.0026 s/iter. Inference: 0.3060 s/iter. Eval: 0.0366 s/iter. Total: 0.3453 s/iter. ETA=0:06:06
[02/27 20:07:32 d2.evaluation.evaluator]: Inference done 205/1250. Dataloading: 0.0027 s/iter. Inference: 0.3056 s/iter. Eval: 0.0364 s/iter. Total: 0.3448 s/iter. ETA=0:06:00
[02/27 20:07:37 d2.evaluation.evaluator]: Inference done 220/1250. Dataloading: 0.0027 s/iter. Inference: 0.3065 s/iter. Eval: 0.0353 s/iter. Total: 0.3445 s/iter. ETA=0:05:54
[02/27 20:07:42 d2.evaluation.evaluator]: Inference done 233/1250. Dataloading: 0.0026 s/iter. Inference: 0.3095 s/iter. Eval: 0.0347 s/iter. Total: 0.3469 s/iter. ETA=0:05:52
[02/27 20:07:47 d2.evaluation.evaluator]: Inference done 248/1250. Dataloading: 0.0028 s/iter. Inference: 0.3096 s/iter. Eval: 0.0343 s/iter. Total: 0.3468 s/iter. ETA=0:05:47
[02/27 20:07:52 d2.evaluation.evaluator]: Inference done 265/1250. Dataloading: 0.0028 s/iter. Inference: 0.3075 s/iter. Eval: 0.0336 s/iter. Total: 0.3440 s/iter. ETA=0:05:38
[02/27 20:07:58 d2.evaluation.evaluator]: Inference done 282/1250. Dataloading: 0.0027 s/iter. Inference: 0.3059 s/iter. Eval: 0.0329 s/iter. Total: 0.3416 s/iter. ETA=0:05:30
[02/27 20:08:03 d2.evaluation.evaluator]: Inference done 297/1250. Dataloading: 0.0026 s/iter. Inference: 0.3046 s/iter. Eval: 0.0341 s/iter. Total: 0.3415 s/iter. ETA=0:05:25
[02/27 20:08:08 d2.evaluation.evaluator]: Inference done 313/1250. Dataloading: 0.0026 s/iter. Inference: 0.3046 s/iter. Eval: 0.0341 s/iter. Total: 0.3414 s/iter. ETA=0:05:19
[02/27 20:08:13 d2.evaluation.evaluator]: Inference done 329/1250. Dataloading: 0.0026 s/iter. Inference: 0.3039 s/iter. Eval: 0.0338 s/iter. Total: 0.3405 s/iter. ETA=0:05:13
[02/27 20:08:18 d2.evaluation.evaluator]: Inference done 346/1250. Dataloading: 0.0026 s/iter. Inference: 0.3019 s/iter. Eval: 0.0338 s/iter. Total: 0.3385 s/iter. ETA=0:05:05
[02/27 20:08:24 d2.evaluation.evaluator]: Inference done 362/1250. Dataloading: 0.0026 s/iter. Inference: 0.3012 s/iter. Eval: 0.0337 s/iter. Total: 0.3375 s/iter. ETA=0:04:59
[02/27 20:08:29 d2.evaluation.evaluator]: Inference done 377/1250. Dataloading: 0.0026 s/iter. Inference: 0.3013 s/iter. Eval: 0.0335 s/iter. Total: 0.3375 s/iter. ETA=0:04:54
[02/27 20:08:34 d2.evaluation.evaluator]: Inference done 393/1250. Dataloading: 0.0026 s/iter. Inference: 0.3008 s/iter. Eval: 0.0330 s/iter. Total: 0.3365 s/iter. ETA=0:04:48
[02/27 20:08:39 d2.evaluation.evaluator]: Inference done 409/1250. Dataloading: 0.0026 s/iter. Inference: 0.3002 s/iter. Eval: 0.0331 s/iter. Total: 0.3359 s/iter. ETA=0:04:42
[02/27 20:08:44 d2.evaluation.evaluator]: Inference done 424/1250. Dataloading: 0.0026 s/iter. Inference: 0.2999 s/iter. Eval: 0.0335 s/iter. Total: 0.3362 s/iter. ETA=0:04:37
[02/27 20:08:49 d2.evaluation.evaluator]: Inference done 441/1250. Dataloading: 0.0027 s/iter. Inference: 0.2988 s/iter. Eval: 0.0335 s/iter. Total: 0.3350 s/iter. ETA=0:04:31
[02/27 20:08:54 d2.evaluation.evaluator]: Inference done 456/1250. Dataloading: 0.0027 s/iter. Inference: 0.2995 s/iter. Eval: 0.0332 s/iter. Total: 0.3355 s/iter. ETA=0:04:26
[02/27 20:08:59 d2.evaluation.evaluator]: Inference done 471/1250. Dataloading: 0.0027 s/iter. Inference: 0.2997 s/iter. Eval: 0.0332 s/iter. Total: 0.3356 s/iter. ETA=0:04:21
[02/27 20:09:05 d2.evaluation.evaluator]: Inference done 487/1250. Dataloading: 0.0027 s/iter. Inference: 0.2995 s/iter. Eval: 0.0333 s/iter. Total: 0.3356 s/iter. ETA=0:04:16
[02/27 20:09:10 d2.evaluation.evaluator]: Inference done 503/1250. Dataloading: 0.0027 s/iter. Inference: 0.2989 s/iter. Eval: 0.0333 s/iter. Total: 0.3349 s/iter. ETA=0:04:10
[02/27 20:09:15 d2.evaluation.evaluator]: Inference done 519/1250. Dataloading: 0.0026 s/iter. Inference: 0.2986 s/iter. Eval: 0.0332 s/iter. Total: 0.3346 s/iter. ETA=0:04:04
[02/27 20:09:20 d2.evaluation.evaluator]: Inference done 533/1250. Dataloading: 0.0026 s/iter. Inference: 0.2991 s/iter. Eval: 0.0334 s/iter. Total: 0.3353 s/iter. ETA=0:04:00
[02/27 20:09:25 d2.evaluation.evaluator]: Inference done 548/1250. Dataloading: 0.0026 s/iter. Inference: 0.2994 s/iter. Eval: 0.0334 s/iter. Total: 0.3356 s/iter. ETA=0:03:55
[02/27 20:09:30 d2.evaluation.evaluator]: Inference done 563/1250. Dataloading: 0.0026 s/iter. Inference: 0.2995 s/iter. Eval: 0.0337 s/iter. Total: 0.3359 s/iter. ETA=0:03:50
[02/27 20:09:36 d2.evaluation.evaluator]: Inference done 578/1250. Dataloading: 0.0026 s/iter. Inference: 0.2998 s/iter. Eval: 0.0337 s/iter. Total: 0.3362 s/iter. ETA=0:03:45
[02/27 20:09:41 d2.evaluation.evaluator]: Inference done 593/1250. Dataloading: 0.0026 s/iter. Inference: 0.3004 s/iter. Eval: 0.0335 s/iter. Total: 0.3367 s/iter. ETA=0:03:41
[02/27 20:09:46 d2.evaluation.evaluator]: Inference done 609/1250. Dataloading: 0.0026 s/iter. Inference: 0.3005 s/iter. Eval: 0.0333 s/iter. Total: 0.3365 s/iter. ETA=0:03:35
[02/27 20:09:51 d2.evaluation.evaluator]: Inference done 625/1250. Dataloading: 0.0026 s/iter. Inference: 0.3000 s/iter. Eval: 0.0331 s/iter. Total: 0.3359 s/iter. ETA=0:03:29
[02/27 20:09:57 d2.evaluation.evaluator]: Inference done 641/1250. Dataloading: 0.0026 s/iter. Inference: 0.2999 s/iter. Eval: 0.0333 s/iter. Total: 0.3359 s/iter. ETA=0:03:24
[02/27 20:10:02 d2.evaluation.evaluator]: Inference done 656/1250. Dataloading: 0.0026 s/iter. Inference: 0.2999 s/iter. Eval: 0.0334 s/iter. Total: 0.3361 s/iter. ETA=0:03:19
[02/27 20:10:07 d2.evaluation.evaluator]: Inference done 670/1250. Dataloading: 0.0026 s/iter. Inference: 0.3010 s/iter. Eval: 0.0332 s/iter. Total: 0.3370 s/iter. ETA=0:03:15
[02/27 20:10:12 d2.evaluation.evaluator]: Inference done 685/1250. Dataloading: 0.0025 s/iter. Inference: 0.3009 s/iter. Eval: 0.0333 s/iter. Total: 0.3369 s/iter. ETA=0:03:10
[02/27 20:10:17 d2.evaluation.evaluator]: Inference done 701/1250. Dataloading: 0.0026 s/iter. Inference: 0.3007 s/iter. Eval: 0.0333 s/iter. Total: 0.3366 s/iter. ETA=0:03:04
[02/27 20:10:22 d2.evaluation.evaluator]: Inference done 716/1250. Dataloading: 0.0026 s/iter. Inference: 0.3006 s/iter. Eval: 0.0333 s/iter. Total: 0.3367 s/iter. ETA=0:02:59
[02/27 20:10:27 d2.evaluation.evaluator]: Inference done 730/1250. Dataloading: 0.0026 s/iter. Inference: 0.3007 s/iter. Eval: 0.0337 s/iter. Total: 0.3371 s/iter. ETA=0:02:55
[02/27 20:10:32 d2.evaluation.evaluator]: Inference done 745/1250. Dataloading: 0.0026 s/iter. Inference: 0.3007 s/iter. Eval: 0.0337 s/iter. Total: 0.3370 s/iter. ETA=0:02:50
[02/27 20:10:38 d2.evaluation.evaluator]: Inference done 762/1250. Dataloading: 0.0025 s/iter. Inference: 0.3001 s/iter. Eval: 0.0338 s/iter. Total: 0.3365 s/iter. ETA=0:02:44
[02/27 20:10:43 d2.evaluation.evaluator]: Inference done 779/1250. Dataloading: 0.0025 s/iter. Inference: 0.2997 s/iter. Eval: 0.0335 s/iter. Total: 0.3359 s/iter. ETA=0:02:38
[02/27 20:10:48 d2.evaluation.evaluator]: Inference done 795/1250. Dataloading: 0.0025 s/iter. Inference: 0.2994 s/iter. Eval: 0.0335 s/iter. Total: 0.3356 s/iter. ETA=0:02:32
[02/27 20:10:53 d2.evaluation.evaluator]: Inference done 810/1250. Dataloading: 0.0026 s/iter. Inference: 0.2994 s/iter. Eval: 0.0335 s/iter. Total: 0.3355 s/iter. ETA=0:02:27
[02/27 20:10:58 d2.evaluation.evaluator]: Inference done 826/1250. Dataloading: 0.0026 s/iter. Inference: 0.2994 s/iter. Eval: 0.0332 s/iter. Total: 0.3354 s/iter. ETA=0:02:22
[02/27 20:11:04 d2.evaluation.evaluator]: Inference done 842/1250. Dataloading: 0.0026 s/iter. Inference: 0.2993 s/iter. Eval: 0.0332 s/iter. Total: 0.3352 s/iter. ETA=0:02:16
[02/27 20:11:09 d2.evaluation.evaluator]: Inference done 857/1250. Dataloading: 0.0026 s/iter. Inference: 0.2993 s/iter. Eval: 0.0332 s/iter. Total: 0.3353 s/iter. ETA=0:02:11
[02/27 20:11:14 d2.evaluation.evaluator]: Inference done 872/1250. Dataloading: 0.0026 s/iter. Inference: 0.2994 s/iter. Eval: 0.0333 s/iter. Total: 0.3354 s/iter. ETA=0:02:06
[02/27 20:11:19 d2.evaluation.evaluator]: Inference done 888/1250. Dataloading: 0.0026 s/iter. Inference: 0.2994 s/iter. Eval: 0.0331 s/iter. Total: 0.3351 s/iter. ETA=0:02:01
[02/27 20:11:24 d2.evaluation.evaluator]: Inference done 905/1250. Dataloading: 0.0026 s/iter. Inference: 0.2990 s/iter. Eval: 0.0329 s/iter. Total: 0.3346 s/iter. ETA=0:01:55
[02/27 20:11:29 d2.evaluation.evaluator]: Inference done 920/1250. Dataloading: 0.0026 s/iter. Inference: 0.2991 s/iter. Eval: 0.0328 s/iter. Total: 0.3347 s/iter. ETA=0:01:50
[02/27 20:11:35 d2.evaluation.evaluator]: Inference done 935/1250. Dataloading: 0.0026 s/iter. Inference: 0.2994 s/iter. Eval: 0.0329 s/iter. Total: 0.3350 s/iter. ETA=0:01:45
[02/27 20:11:40 d2.evaluation.evaluator]: Inference done 950/1250. Dataloading: 0.0026 s/iter. Inference: 0.2995 s/iter. Eval: 0.0328 s/iter. Total: 0.3350 s/iter. ETA=0:01:40
[02/27 20:11:45 d2.evaluation.evaluator]: Inference done 965/1250. Dataloading: 0.0026 s/iter. Inference: 0.2998 s/iter. Eval: 0.0327 s/iter. Total: 0.3352 s/iter. ETA=0:01:35
[02/27 20:11:50 d2.evaluation.evaluator]: Inference done 981/1250. Dataloading: 0.0026 s/iter. Inference: 0.2995 s/iter. Eval: 0.0327 s/iter. Total: 0.3350 s/iter. ETA=0:01:30
[02/27 20:11:55 d2.evaluation.evaluator]: Inference done 997/1250. Dataloading: 0.0025 s/iter. Inference: 0.2997 s/iter. Eval: 0.0326 s/iter. Total: 0.3349 s/iter. ETA=0:01:24
[02/27 20:12:00 d2.evaluation.evaluator]: Inference done 1012/1250. Dataloading: 0.0025 s/iter. Inference: 0.2998 s/iter. Eval: 0.0326 s/iter. Total: 0.3350 s/iter. ETA=0:01:19
[02/27 20:12:06 d2.evaluation.evaluator]: Inference done 1027/1250. Dataloading: 0.0025 s/iter. Inference: 0.3000 s/iter. Eval: 0.0325 s/iter. Total: 0.3352 s/iter. ETA=0:01:14
[02/27 20:12:11 d2.evaluation.evaluator]: Inference done 1043/1250. Dataloading: 0.0025 s/iter. Inference: 0.2997 s/iter. Eval: 0.0326 s/iter. Total: 0.3351 s/iter. ETA=0:01:09
[02/27 20:12:16 d2.evaluation.evaluator]: Inference done 1060/1250. Dataloading: 0.0025 s/iter. Inference: 0.2994 s/iter. Eval: 0.0325 s/iter. Total: 0.3347 s/iter. ETA=0:01:03
[02/27 20:12:21 d2.evaluation.evaluator]: Inference done 1076/1250. Dataloading: 0.0025 s/iter. Inference: 0.2992 s/iter. Eval: 0.0327 s/iter. Total: 0.3346 s/iter. ETA=0:00:58
[02/27 20:12:27 d2.evaluation.evaluator]: Inference done 1092/1250. Dataloading: 0.0025 s/iter. Inference: 0.2990 s/iter. Eval: 0.0327 s/iter. Total: 0.3344 s/iter. ETA=0:00:52
[02/27 20:12:32 d2.evaluation.evaluator]: Inference done 1107/1250. Dataloading: 0.0025 s/iter. Inference: 0.2993 s/iter. Eval: 0.0326 s/iter. Total: 0.3346 s/iter. ETA=0:00:47
[02/27 20:12:37 d2.evaluation.evaluator]: Inference done 1124/1250. Dataloading: 0.0025 s/iter. Inference: 0.2988 s/iter. Eval: 0.0325 s/iter. Total: 0.3340 s/iter. ETA=0:00:42
[02/27 20:12:42 d2.evaluation.evaluator]: Inference done 1139/1250. Dataloading: 0.0025 s/iter. Inference: 0.2988 s/iter. Eval: 0.0325 s/iter. Total: 0.3340 s/iter. ETA=0:00:37
[02/27 20:12:47 d2.evaluation.evaluator]: Inference done 1155/1250. Dataloading: 0.0025 s/iter. Inference: 0.2987 s/iter. Eval: 0.0326 s/iter. Total: 0.3340 s/iter. ETA=0:00:31
[02/27 20:12:52 d2.evaluation.evaluator]: Inference done 1169/1250. Dataloading: 0.0025 s/iter. Inference: 0.2992 s/iter. Eval: 0.0326 s/iter. Total: 0.3344 s/iter. ETA=0:00:27
[02/27 20:12:58 d2.evaluation.evaluator]: Inference done 1186/1250. Dataloading: 0.0025 s/iter. Inference: 0.2990 s/iter. Eval: 0.0326 s/iter. Total: 0.3342 s/iter. ETA=0:00:21
[02/27 20:13:03 d2.evaluation.evaluator]: Inference done 1202/1250. Dataloading: 0.0025 s/iter. Inference: 0.2986 s/iter. Eval: 0.0326 s/iter. Total: 0.3339 s/iter. ETA=0:00:16
[02/27 20:13:08 d2.evaluation.evaluator]: Inference done 1217/1250. Dataloading: 0.0025 s/iter. Inference: 0.2987 s/iter. Eval: 0.0327 s/iter. Total: 0.3340 s/iter. ETA=0:00:11
[02/27 20:13:13 d2.evaluation.evaluator]: Inference done 1232/1250. Dataloading: 0.0025 s/iter. Inference: 0.2988 s/iter. Eval: 0.0328 s/iter. Total: 0.3343 s/iter. ETA=0:00:06
[02/27 20:13:18 d2.evaluation.evaluator]: Total inference time: 0:06:55.361884 (0.333624 s / iter per device, on 4 devices)
[02/27 20:13:18 d2.evaluation.evaluator]: Total inference pure compute time: 0:06:11 (0.298018 s / iter per device, on 4 devices)
[02/27 20:13:38 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[02/27 20:13:38 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
[02/27 20:13:38 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[02/27 20:13:45 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 6.52 seconds.
[02/27 20:13:45 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/27 20:13:45 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.69 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.069
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.020
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.026
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.106
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.110
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.084
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.170
[02/27 20:13:45 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 2.905 | 6.894  | 1.965  | 1.242 | 2.576 | 4.365 |
[02/27 20:13:45 d2.evaluation.coco_evaluation]: Per-category bbox AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 18.712 | bicycle      | 0.306  | car            | 7.587  |
| motorcycle    | 3.795  | airplane     | 6.667  | bus            | 9.430  |
| train         | 8.739  | truck        | 2.364  | boat           | 0.714  |
| traffic light | 0.989  | fire hydrant | 3.176  | stop sign      | 14.494 |
| parking meter | 0.000  | bench        | 0.243  | bird           | 0.315  |
| cat           | 7.991  | dog          | 3.878  | horse          | 7.977  |
| sheep         | 7.694  | cow          | 3.221  | elephant       | 8.690  |
| bear          | 9.931  | zebra        | 14.356 | giraffe        | 10.053 |
| backpack      | 0.025  | umbrella     | 0.533  | handbag        | 0.000  |
| tie           | 0.446  | suitcase     | 0.187  | frisbee        | 0.668  |
| skis          | 0.308  | snowboard    | 0.000  | sports ball    | 9.024  |
| kite          | 3.489  | baseball bat | 0.000  | baseball glove | 0.483  |
| skateboard    | 0.202  | surfboard    | 0.117  | tennis racket  | 0.000  |
| bottle        | 1.577  | wine glass   | 0.000  | cup            | 2.362  |
| fork          | 0.000  | knife        | 0.000  | spoon          | 0.000  |
| bowl          | 4.282  | banana       | 0.000  | apple          | 0.000  |
| sandwich      | 2.001  | orange       | 2.239  | broccoli       | 0.367  |
| carrot        | 0.000  | hot dog      | 0.206  | pizza          | 9.975  |
| donut         | 1.298  | cake         | 0.590  | chair          | 0.812  |
| couch         | 0.805  | potted plant | 0.132  | bed            | 5.278  |
| dining table  | 7.457  | toilet       | 7.034  | tv             | 4.525  |
| laptop        | 1.494  | mouse        | 1.068  | remote         | 0.000  |
| keyboard      | 0.154  | cell phone   | 0.000  | microwave      | 0.428  |
| oven          | 0.691  | toaster      | 0.000  | sink           | 1.215  |
| refrigerator  | 2.118  | book         | 0.355  | clock          | 5.628  |
| vase          | 0.165  | scissors     | 0.000  | teddy bear     | 1.345  |
| hair drier    | 0.000  | toothbrush   | 0.000  |                |        |
Loading and preparing results...
DONE (t=1.56s)
creating index...
index created!
[02/27 20:13:51 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*
[02/27 20:13:58 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 7.74 seconds.
[02/27 20:13:59 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/27 20:13:59 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.72 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.061
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.024
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.007
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.023
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.050
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.105
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.109
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.084
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.175
[02/27 20:14:00 d2.evaluation.coco_evaluation]: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 2.929 | 6.131  | 2.435  | 0.729 | 2.284 | 4.977 |
[02/27 20:14:00 d2.evaluation.coco_evaluation]: Per-category segm AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 15.764 | bicycle      | 0.094  | car            | 7.170  |
| motorcycle    | 2.034  | airplane     | 7.920  | bus            | 11.361 |
| train         | 11.712 | truck        | 2.518  | boat           | 1.246  |
| traffic light | 0.839  | fire hydrant | 4.218  | stop sign      | 17.825 |
| parking meter | 0.000  | bench        | 0.136  | bird           | 0.438  |
| cat           | 8.903  | dog          | 4.025  | horse          | 4.245  |
| sheep         | 6.828  | cow          | 3.309  | elephant       | 8.005  |
| bear          | 11.390 | zebra        | 11.146 | giraffe        | 9.393  |
| backpack      | 0.025  | umbrella     | 0.814  | handbag        | 0.000  |
| tie           | 0.594  | suitcase     | 0.204  | frisbee        | 0.875  |
| skis          | 0.000  | snowboard    | 0.000  | sports ball    | 11.138 |
| kite          | 3.931  | baseball bat | 0.000  | baseball glove | 0.679  |
| skateboard    | 0.103  | surfboard    | 0.138  | tennis racket  | 0.000  |
| bottle        | 1.722  | wine glass   | 0.000  | cup            | 2.787  |
| fork          | 0.000  | knife        | 0.000  | spoon          | 0.000  |
| bowl          | 3.979  | banana       | 0.000  | apple          | 0.000  |
| sandwich      | 2.095  | orange       | 2.643  | broccoli       | 0.480  |
| carrot        | 0.000  | hot dog      | 0.173  | pizza          | 10.129 |
| donut         | 1.562  | cake         | 0.627  | chair          | 0.689  |
| couch         | 0.151  | potted plant | 0.165  | bed            | 3.003  |
| dining table  | 3.217  | toilet       | 9.520  | tv             | 5.450  |
| laptop        | 2.034  | mouse        | 1.840  | remote         | 0.000  |
| keyboard      | 0.113  | cell phone   | 0.000  | microwave      | 0.403  |
| oven          | 0.617  | toaster      | 0.000  | sink           | 1.688  |
| refrigerator  | 2.408  | book         | 0.087  | clock          | 6.263  |
| vase          | 0.271  | scissors     | 0.000  | teddy bear     | 1.149  |
| hair drier    | 0.000  | toothbrush   | 0.000  |                |        |
[02/27 20:14:00 d2.evaluation.testing]: copypaste: Task: bbox
[02/27 20:14:00 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[02/27 20:14:00 d2.evaluation.testing]: copypaste: 2.9051,6.8944,1.9647,1.2416,2.5761,4.3646
[02/27 20:14:00 d2.evaluation.testing]: copypaste: Task: segm
[02/27 20:14:00 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[02/27 20:14:00 d2.evaluation.testing]: copypaste: 2.9286,6.1309,2.4353,0.7289,2.2839,4.9766
[02/27 20:14:00 d2.utils.events]:  eta: 1 day, 2:06:13  iter: 4999  total_loss: 1.255  loss_cls: 0.306  loss_box_reg: 0.2283  loss_mask: 0.4006  loss_rpn_cls: 0.09196  loss_rpn_loc: 0.1414  time: 0.5215  last_time: 0.5019  data_time: 0.0230  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 20:14:11 d2.utils.events]:  eta: 1 day, 2:06:49  iter: 5019  total_loss: 1.368  loss_cls: 0.3408  loss_box_reg: 0.2387  loss_mask: 0.4184  loss_rpn_cls: 0.08429  loss_rpn_loc: 0.1608  time: 0.5215  last_time: 0.5049  data_time: 0.0141  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 20:14:21 d2.utils.events]:  eta: 1 day, 2:06:44  iter: 5039  total_loss: 1.275  loss_cls: 0.3565  loss_box_reg: 0.2486  loss_mask: 0.4289  loss_rpn_cls: 0.06666  loss_rpn_loc: 0.1828  time: 0.5215  last_time: 0.5106  data_time: 0.0192  last_data_time: 0.0057   lr: 0.0001  max_mem: 11521M
[02/27 20:14:32 d2.utils.events]:  eta: 1 day, 2:06:28  iter: 5059  total_loss: 1.291  loss_cls: 0.3533  loss_box_reg: 0.237  loss_mask: 0.4107  loss_rpn_cls: 0.08388  loss_rpn_loc: 0.1316  time: 0.5215  last_time: 0.5453  data_time: 0.0111  last_data_time: 0.0063   lr: 0.0001  max_mem: 11521M
[02/27 20:14:42 d2.utils.events]:  eta: 1 day, 2:06:45  iter: 5079  total_loss: 1.172  loss_cls: 0.3561  loss_box_reg: 0.2307  loss_mask: 0.4016  loss_rpn_cls: 0.08382  loss_rpn_loc: 0.1485  time: 0.5215  last_time: 0.5205  data_time: 0.0135  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 20:14:53 d2.utils.events]:  eta: 1 day, 2:06:08  iter: 5099  total_loss: 1.323  loss_cls: 0.4011  loss_box_reg: 0.2224  loss_mask: 0.4165  loss_rpn_cls: 0.06168  loss_rpn_loc: 0.172  time: 0.5214  last_time: 0.5086  data_time: 0.0137  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 20:15:03 d2.utils.events]:  eta: 1 day, 2:05:50  iter: 5119  total_loss: 1.385  loss_cls: 0.4017  loss_box_reg: 0.246  loss_mask: 0.4281  loss_rpn_cls: 0.1035  loss_rpn_loc: 0.2447  time: 0.5215  last_time: 0.5691  data_time: 0.0230  last_data_time: 0.0140   lr: 0.0001  max_mem: 11521M
[02/27 20:15:13 d2.utils.events]:  eta: 1 day, 2:04:55  iter: 5139  total_loss: 1.244  loss_cls: 0.37  loss_box_reg: 0.2505  loss_mask: 0.3923  loss_rpn_cls: 0.07061  loss_rpn_loc: 0.1101  time: 0.5214  last_time: 0.4954  data_time: 0.0203  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 20:15:24 d2.utils.events]:  eta: 1 day, 2:02:20  iter: 5159  total_loss: 1.259  loss_cls: 0.3334  loss_box_reg: 0.2148  loss_mask: 0.3943  loss_rpn_cls: 0.07557  loss_rpn_loc: 0.1382  time: 0.5214  last_time: 0.5139  data_time: 0.0151  last_data_time: 0.0514   lr: 0.0001  max_mem: 11521M
[02/27 20:15:34 d2.utils.events]:  eta: 1 day, 2:02:38  iter: 5179  total_loss: 1.349  loss_cls: 0.3292  loss_box_reg: 0.233  loss_mask: 0.4271  loss_rpn_cls: 0.06819  loss_rpn_loc: 0.1597  time: 0.5214  last_time: 0.5512  data_time: 0.0139  last_data_time: 0.0462   lr: 0.0001  max_mem: 11521M
[02/27 20:15:45 d2.utils.events]:  eta: 1 day, 2:00:55  iter: 5199  total_loss: 1.271  loss_cls: 0.3571  loss_box_reg: 0.2262  loss_mask: 0.418  loss_rpn_cls: 0.07154  loss_rpn_loc: 0.1982  time: 0.5214  last_time: 0.5059  data_time: 0.0182  last_data_time: 0.0196   lr: 0.0001  max_mem: 11521M
[02/27 20:15:55 d2.utils.events]:  eta: 1 day, 2:01:49  iter: 5219  total_loss: 1.378  loss_cls: 0.4157  loss_box_reg: 0.2655  loss_mask: 0.4049  loss_rpn_cls: 0.08755  loss_rpn_loc: 0.1506  time: 0.5214  last_time: 0.5315  data_time: 0.0134  last_data_time: 0.0651   lr: 0.0001  max_mem: 11521M
[02/27 20:16:06 d2.utils.events]:  eta: 1 day, 2:01:37  iter: 5239  total_loss: 1.271  loss_cls: 0.3942  loss_box_reg: 0.2635  loss_mask: 0.388  loss_rpn_cls: 0.09122  loss_rpn_loc: 0.1723  time: 0.5214  last_time: 0.4582  data_time: 0.0367  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 20:16:16 d2.utils.events]:  eta: 1 day, 2:01:21  iter: 5259  total_loss: 1.179  loss_cls: 0.3374  loss_box_reg: 0.2406  loss_mask: 0.3744  loss_rpn_cls: 0.08067  loss_rpn_loc: 0.1194  time: 0.5214  last_time: 0.5592  data_time: 0.0284  last_data_time: 0.0526   lr: 0.0001  max_mem: 11521M
[02/27 20:16:27 d2.utils.events]:  eta: 1 day, 1:59:53  iter: 5279  total_loss: 1.074  loss_cls: 0.3341  loss_box_reg: 0.204  loss_mask: 0.4464  loss_rpn_cls: 0.0871  loss_rpn_loc: 0.1026  time: 0.5214  last_time: 0.5461  data_time: 0.0155  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 20:16:37 d2.utils.events]:  eta: 1 day, 1:59:14  iter: 5299  total_loss: 1.294  loss_cls: 0.4045  loss_box_reg: 0.2838  loss_mask: 0.3907  loss_rpn_cls: 0.0913  loss_rpn_loc: 0.1196  time: 0.5214  last_time: 0.5241  data_time: 0.0262  last_data_time: 0.0057   lr: 0.0001  max_mem: 11521M
[02/27 20:16:48 d2.utils.events]:  eta: 1 day, 2:00:34  iter: 5319  total_loss: 1.286  loss_cls: 0.3906  loss_box_reg: 0.234  loss_mask: 0.4329  loss_rpn_cls: 0.07041  loss_rpn_loc: 0.1227  time: 0.5215  last_time: 0.5045  data_time: 0.0227  last_data_time: 0.0748   lr: 0.0001  max_mem: 11521M
[02/27 20:16:59 d2.utils.events]:  eta: 1 day, 1:59:13  iter: 5339  total_loss: 1.166  loss_cls: 0.3204  loss_box_reg: 0.2285  loss_mask: 0.3866  loss_rpn_cls: 0.07561  loss_rpn_loc: 0.1286  time: 0.5215  last_time: 0.4922  data_time: 0.0116  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 20:17:09 d2.utils.events]:  eta: 1 day, 1:59:21  iter: 5359  total_loss: 1.318  loss_cls: 0.3725  loss_box_reg: 0.2757  loss_mask: 0.3724  loss_rpn_cls: 0.09273  loss_rpn_loc: 0.1409  time: 0.5215  last_time: 0.5231  data_time: 0.0086  last_data_time: 0.0129   lr: 0.0001  max_mem: 11521M
[02/27 20:17:20 d2.utils.events]:  eta: 1 day, 1:59:01  iter: 5379  total_loss: 1.085  loss_cls: 0.2764  loss_box_reg: 0.1874  loss_mask: 0.4016  loss_rpn_cls: 0.0602  loss_rpn_loc: 0.1041  time: 0.5215  last_time: 0.4866  data_time: 0.0111  last_data_time: 0.0260   lr: 0.0001  max_mem: 11521M
[02/27 20:17:30 d2.utils.events]:  eta: 1 day, 1:59:23  iter: 5399  total_loss: 1.342  loss_cls: 0.3456  loss_box_reg: 0.2304  loss_mask: 0.4134  loss_rpn_cls: 0.07864  loss_rpn_loc: 0.14  time: 0.5215  last_time: 0.5573  data_time: 0.0172  last_data_time: 0.0118   lr: 0.0001  max_mem: 11521M
[02/27 20:17:41 d2.utils.events]:  eta: 1 day, 1:58:22  iter: 5419  total_loss: 1.276  loss_cls: 0.3908  loss_box_reg: 0.2624  loss_mask: 0.4017  loss_rpn_cls: 0.07534  loss_rpn_loc: 0.135  time: 0.5215  last_time: 0.5186  data_time: 0.0172  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 20:17:51 d2.utils.events]:  eta: 1 day, 1:57:18  iter: 5439  total_loss: 1.205  loss_cls: 0.3844  loss_box_reg: 0.2435  loss_mask: 0.3795  loss_rpn_cls: 0.07646  loss_rpn_loc: 0.1489  time: 0.5215  last_time: 0.5541  data_time: 0.0154  last_data_time: 0.0294   lr: 0.0001  max_mem: 11521M
[02/27 20:18:01 d2.utils.events]:  eta: 1 day, 1:56:44  iter: 5459  total_loss: 1.226  loss_cls: 0.3299  loss_box_reg: 0.2397  loss_mask: 0.3944  loss_rpn_cls: 0.0811  loss_rpn_loc: 0.1733  time: 0.5214  last_time: 0.5340  data_time: 0.0212  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 20:18:12 d2.utils.events]:  eta: 1 day, 1:54:57  iter: 5479  total_loss: 1.18  loss_cls: 0.3196  loss_box_reg: 0.2161  loss_mask: 0.4076  loss_rpn_cls: 0.07616  loss_rpn_loc: 0.1206  time: 0.5214  last_time: 0.5201  data_time: 0.0158  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 20:18:22 d2.utils.events]:  eta: 1 day, 1:53:58  iter: 5499  total_loss: 1.186  loss_cls: 0.3403  loss_box_reg: 0.2078  loss_mask: 0.4075  loss_rpn_cls: 0.06938  loss_rpn_loc: 0.1451  time: 0.5214  last_time: 0.5401  data_time: 0.0321  last_data_time: 0.0415   lr: 0.0001  max_mem: 11521M
[02/27 20:18:32 d2.utils.events]:  eta: 1 day, 1:52:31  iter: 5519  total_loss: 1.185  loss_cls: 0.3143  loss_box_reg: 0.2022  loss_mask: 0.3842  loss_rpn_cls: 0.07271  loss_rpn_loc: 0.1543  time: 0.5214  last_time: 0.5523  data_time: 0.0192  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:18:43 d2.utils.events]:  eta: 1 day, 1:51:38  iter: 5539  total_loss: 1.278  loss_cls: 0.4089  loss_box_reg: 0.2259  loss_mask: 0.3995  loss_rpn_cls: 0.06578  loss_rpn_loc: 0.139  time: 0.5214  last_time: 0.5029  data_time: 0.0138  last_data_time: 0.0456   lr: 0.0001  max_mem: 11521M
[02/27 20:18:53 d2.utils.events]:  eta: 1 day, 1:50:55  iter: 5559  total_loss: 1.318  loss_cls: 0.346  loss_box_reg: 0.237  loss_mask: 0.4162  loss_rpn_cls: 0.07315  loss_rpn_loc: 0.1614  time: 0.5213  last_time: 0.4979  data_time: 0.0139  last_data_time: 0.0235   lr: 0.0001  max_mem: 11521M
[02/27 20:19:04 d2.utils.events]:  eta: 1 day, 1:50:59  iter: 5579  total_loss: 1.404  loss_cls: 0.4078  loss_box_reg: 0.2745  loss_mask: 0.4072  loss_rpn_cls: 0.06732  loss_rpn_loc: 0.1018  time: 0.5213  last_time: 0.5149  data_time: 0.0118  last_data_time: 0.0144   lr: 0.0001  max_mem: 11521M
[02/27 20:19:14 d2.utils.events]:  eta: 1 day, 1:52:19  iter: 5599  total_loss: 1.309  loss_cls: 0.3596  loss_box_reg: 0.2518  loss_mask: 0.389  loss_rpn_cls: 0.07512  loss_rpn_loc: 0.12  time: 0.5213  last_time: 0.5287  data_time: 0.0161  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 20:19:25 d2.utils.events]:  eta: 1 day, 1:52:27  iter: 5619  total_loss: 1.216  loss_cls: 0.3917  loss_box_reg: 0.2343  loss_mask: 0.399  loss_rpn_cls: 0.1082  loss_rpn_loc: 0.1663  time: 0.5214  last_time: 0.5446  data_time: 0.0208  last_data_time: 0.0749   lr: 0.0001  max_mem: 11521M
[02/27 20:19:35 d2.utils.events]:  eta: 1 day, 1:51:39  iter: 5639  total_loss: 1.248  loss_cls: 0.372  loss_box_reg: 0.2338  loss_mask: 0.4212  loss_rpn_cls: 0.07341  loss_rpn_loc: 0.1216  time: 0.5214  last_time: 0.5185  data_time: 0.0248  last_data_time: 0.0111   lr: 0.0001  max_mem: 11521M
[02/27 20:19:46 d2.utils.events]:  eta: 1 day, 1:51:10  iter: 5659  total_loss: 1.221  loss_cls: 0.3179  loss_box_reg: 0.2176  loss_mask: 0.4109  loss_rpn_cls: 0.07331  loss_rpn_loc: 0.1427  time: 0.5213  last_time: 0.4895  data_time: 0.0113  last_data_time: 0.0022   lr: 0.0001  max_mem: 11521M
[02/27 20:19:56 d2.utils.events]:  eta: 1 day, 1:50:47  iter: 5679  total_loss: 1.446  loss_cls: 0.4346  loss_box_reg: 0.2915  loss_mask: 0.408  loss_rpn_cls: 0.0761  loss_rpn_loc: 0.1339  time: 0.5214  last_time: 0.5023  data_time: 0.0198  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 20:20:07 d2.utils.events]:  eta: 1 day, 1:50:28  iter: 5699  total_loss: 1.282  loss_cls: 0.3963  loss_box_reg: 0.2513  loss_mask: 0.4034  loss_rpn_cls: 0.07225  loss_rpn_loc: 0.1235  time: 0.5213  last_time: 0.5376  data_time: 0.0106  last_data_time: 0.0287   lr: 0.0001  max_mem: 11521M
[02/27 20:20:17 d2.utils.events]:  eta: 1 day, 1:50:18  iter: 5719  total_loss: 1.197  loss_cls: 0.3365  loss_box_reg: 0.2089  loss_mask: 0.3961  loss_rpn_cls: 0.07351  loss_rpn_loc: 0.1852  time: 0.5213  last_time: 0.5165  data_time: 0.0145  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 20:20:27 d2.utils.events]:  eta: 1 day, 1:49:54  iter: 5739  total_loss: 1.245  loss_cls: 0.328  loss_box_reg: 0.2102  loss_mask: 0.4415  loss_rpn_cls: 0.08031  loss_rpn_loc: 0.1594  time: 0.5213  last_time: 0.5634  data_time: 0.0163  last_data_time: 0.0743   lr: 0.0001  max_mem: 11521M
[02/27 20:20:38 d2.utils.events]:  eta: 1 day, 1:49:22  iter: 5759  total_loss: 1.239  loss_cls: 0.3399  loss_box_reg: 0.2513  loss_mask: 0.4248  loss_rpn_cls: 0.08772  loss_rpn_loc: 0.132  time: 0.5214  last_time: 0.5465  data_time: 0.0258  last_data_time: 0.0629   lr: 0.0001  max_mem: 11521M
[02/27 20:20:49 d2.utils.events]:  eta: 1 day, 1:47:55  iter: 5779  total_loss: 1.244  loss_cls: 0.335  loss_box_reg: 0.2285  loss_mask: 0.3999  loss_rpn_cls: 0.084  loss_rpn_loc: 0.1698  time: 0.5213  last_time: 0.5013  data_time: 0.0177  last_data_time: 0.0141   lr: 0.0001  max_mem: 11521M
[02/27 20:20:59 d2.utils.events]:  eta: 1 day, 1:48:36  iter: 5799  total_loss: 1.369  loss_cls: 0.3571  loss_box_reg: 0.2823  loss_mask: 0.4322  loss_rpn_cls: 0.11  loss_rpn_loc: 0.1554  time: 0.5213  last_time: 0.4930  data_time: 0.0152  last_data_time: 0.0293   lr: 0.0001  max_mem: 11521M
[02/27 20:21:10 d2.utils.events]:  eta: 1 day, 1:49:39  iter: 5819  total_loss: 1.476  loss_cls: 0.4234  loss_box_reg: 0.2892  loss_mask: 0.4048  loss_rpn_cls: 0.08382  loss_rpn_loc: 0.1836  time: 0.5214  last_time: 0.5154  data_time: 0.0328  last_data_time: 0.0077   lr: 0.0001  max_mem: 11521M
[02/27 20:21:21 d2.utils.events]:  eta: 1 day, 1:50:08  iter: 5839  total_loss: 1.273  loss_cls: 0.3815  loss_box_reg: 0.252  loss_mask: 0.3971  loss_rpn_cls: 0.0771  loss_rpn_loc: 0.153  time: 0.5214  last_time: 0.5074  data_time: 0.0275  last_data_time: 0.0411   lr: 0.0001  max_mem: 11521M
[02/27 20:21:31 d2.utils.events]:  eta: 1 day, 1:49:26  iter: 5859  total_loss: 1.26  loss_cls: 0.3824  loss_box_reg: 0.2496  loss_mask: 0.4101  loss_rpn_cls: 0.07757  loss_rpn_loc: 0.1472  time: 0.5214  last_time: 0.5959  data_time: 0.0229  last_data_time: 0.1126   lr: 0.0001  max_mem: 11521M
[02/27 20:21:42 d2.utils.events]:  eta: 1 day, 1:50:24  iter: 5879  total_loss: 1.183  loss_cls: 0.3242  loss_box_reg: 0.2033  loss_mask: 0.4082  loss_rpn_cls: 0.07702  loss_rpn_loc: 0.1607  time: 0.5215  last_time: 0.5246  data_time: 0.0094  last_data_time: 0.0032   lr: 0.0001  max_mem: 11521M
[02/27 20:21:52 d2.utils.events]:  eta: 1 day, 1:50:30  iter: 5899  total_loss: 1.168  loss_cls: 0.3176  loss_box_reg: 0.2329  loss_mask: 0.3876  loss_rpn_cls: 0.07076  loss_rpn_loc: 0.1223  time: 0.5215  last_time: 0.5265  data_time: 0.0219  last_data_time: 0.0055   lr: 0.0001  max_mem: 11521M
[02/27 20:22:03 d2.utils.events]:  eta: 1 day, 1:49:43  iter: 5919  total_loss: 1.246  loss_cls: 0.3473  loss_box_reg: 0.2124  loss_mask: 0.3934  loss_rpn_cls: 0.06743  loss_rpn_loc: 0.1321  time: 0.5215  last_time: 0.5924  data_time: 0.0105  last_data_time: 0.0065   lr: 0.0001  max_mem: 11521M
[02/27 20:22:14 d2.utils.events]:  eta: 1 day, 1:50:57  iter: 5939  total_loss: 1.462  loss_cls: 0.405  loss_box_reg: 0.26  loss_mask: 0.4266  loss_rpn_cls: 0.1086  loss_rpn_loc: 0.1938  time: 0.5215  last_time: 0.4967  data_time: 0.0284  last_data_time: 0.0216   lr: 0.0001  max_mem: 11521M
[02/27 20:22:24 d2.utils.events]:  eta: 1 day, 1:51:38  iter: 5959  total_loss: 1.265  loss_cls: 0.3801  loss_box_reg: 0.243  loss_mask: 0.3847  loss_rpn_cls: 0.07589  loss_rpn_loc: 0.1093  time: 0.5215  last_time: 0.5068  data_time: 0.0180  last_data_time: 0.0055   lr: 0.0001  max_mem: 11521M
[02/27 20:22:34 d2.utils.events]:  eta: 1 day, 1:50:32  iter: 5979  total_loss: 1.2  loss_cls: 0.3029  loss_box_reg: 0.2384  loss_mask: 0.3914  loss_rpn_cls: 0.08946  loss_rpn_loc: 0.1189  time: 0.5215  last_time: 0.5153  data_time: 0.0122  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 20:22:45 d2.utils.events]:  eta: 1 day, 1:49:22  iter: 5999  total_loss: 1.229  loss_cls: 0.365  loss_box_reg: 0.2512  loss_mask: 0.3688  loss_rpn_cls: 0.07908  loss_rpn_loc: 0.1189  time: 0.5215  last_time: 0.5036  data_time: 0.0241  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 20:22:56 d2.utils.events]:  eta: 1 day, 1:50:11  iter: 6019  total_loss: 1.186  loss_cls: 0.3339  loss_box_reg: 0.2189  loss_mask: 0.3812  loss_rpn_cls: 0.07171  loss_rpn_loc: 0.1449  time: 0.5215  last_time: 0.5521  data_time: 0.0198  last_data_time: 0.0251   lr: 0.0001  max_mem: 11521M
[02/27 20:23:06 d2.utils.events]:  eta: 1 day, 1:50:43  iter: 6039  total_loss: 1.237  loss_cls: 0.4257  loss_box_reg: 0.2612  loss_mask: 0.3914  loss_rpn_cls: 0.08886  loss_rpn_loc: 0.1034  time: 0.5215  last_time: 0.5005  data_time: 0.0174  last_data_time: 0.0276   lr: 0.0001  max_mem: 11521M
[02/27 20:23:17 d2.utils.events]:  eta: 1 day, 1:50:33  iter: 6059  total_loss: 1.135  loss_cls: 0.348  loss_box_reg: 0.2229  loss_mask: 0.39  loss_rpn_cls: 0.07069  loss_rpn_loc: 0.1272  time: 0.5215  last_time: 0.5189  data_time: 0.0168  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 20:23:27 d2.utils.events]:  eta: 1 day, 1:50:36  iter: 6079  total_loss: 1.156  loss_cls: 0.327  loss_box_reg: 0.2035  loss_mask: 0.4153  loss_rpn_cls: 0.06151  loss_rpn_loc: 0.09807  time: 0.5215  last_time: 0.5183  data_time: 0.0166  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 20:23:38 d2.utils.events]:  eta: 1 day, 1:52:07  iter: 6099  total_loss: 1.28  loss_cls: 0.3891  loss_box_reg: 0.2562  loss_mask: 0.4149  loss_rpn_cls: 0.0589  loss_rpn_loc: 0.1285  time: 0.5216  last_time: 0.5163  data_time: 0.0163  last_data_time: 0.0079   lr: 0.0001  max_mem: 11521M
[02/27 20:23:48 d2.utils.events]:  eta: 1 day, 1:50:28  iter: 6119  total_loss: 1.362  loss_cls: 0.4027  loss_box_reg: 0.2645  loss_mask: 0.4162  loss_rpn_cls: 0.1039  loss_rpn_loc: 0.1424  time: 0.5215  last_time: 0.4957  data_time: 0.0135  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 20:23:59 d2.utils.events]:  eta: 1 day, 1:50:57  iter: 6139  total_loss: 1.154  loss_cls: 0.314  loss_box_reg: 0.2094  loss_mask: 0.377  loss_rpn_cls: 0.0998  loss_rpn_loc: 0.1693  time: 0.5215  last_time: 0.5025  data_time: 0.0194  last_data_time: 0.0141   lr: 0.0001  max_mem: 11521M
[02/27 20:24:09 d2.utils.events]:  eta: 1 day, 1:51:12  iter: 6159  total_loss: 1.346  loss_cls: 0.3527  loss_box_reg: 0.2715  loss_mask: 0.4227  loss_rpn_cls: 0.0816  loss_rpn_loc: 0.1341  time: 0.5215  last_time: 0.5177  data_time: 0.0178  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 20:24:20 d2.utils.events]:  eta: 1 day, 1:51:25  iter: 6179  total_loss: 1.257  loss_cls: 0.3632  loss_box_reg: 0.2098  loss_mask: 0.3904  loss_rpn_cls: 0.06267  loss_rpn_loc: 0.1507  time: 0.5216  last_time: 0.5213  data_time: 0.0333  last_data_time: 0.0376   lr: 0.0001  max_mem: 11521M
[02/27 20:24:30 d2.utils.events]:  eta: 1 day, 1:50:25  iter: 6199  total_loss: 1.275  loss_cls: 0.3782  loss_box_reg: 0.2432  loss_mask: 0.4245  loss_rpn_cls: 0.07112  loss_rpn_loc: 0.1409  time: 0.5215  last_time: 0.4997  data_time: 0.0096  last_data_time: 0.0098   lr: 0.0001  max_mem: 11521M
[02/27 20:24:41 d2.utils.events]:  eta: 1 day, 1:49:57  iter: 6219  total_loss: 1.349  loss_cls: 0.3349  loss_box_reg: 0.2388  loss_mask: 0.4051  loss_rpn_cls: 0.09747  loss_rpn_loc: 0.129  time: 0.5215  last_time: 0.5612  data_time: 0.0207  last_data_time: 0.0881   lr: 0.0001  max_mem: 11521M
[02/27 20:24:51 d2.utils.events]:  eta: 1 day, 1:47:30  iter: 6239  total_loss: 1.248  loss_cls: 0.3731  loss_box_reg: 0.2428  loss_mask: 0.3974  loss_rpn_cls: 0.08146  loss_rpn_loc: 0.1294  time: 0.5215  last_time: 0.4818  data_time: 0.0120  last_data_time: 0.0123   lr: 0.0001  max_mem: 11521M
[02/27 20:25:01 d2.utils.events]:  eta: 1 day, 1:47:19  iter: 6259  total_loss: 1.328  loss_cls: 0.375  loss_box_reg: 0.2397  loss_mask: 0.4046  loss_rpn_cls: 0.07433  loss_rpn_loc: 0.1585  time: 0.5215  last_time: 0.5302  data_time: 0.0375  last_data_time: 0.1423   lr: 0.0001  max_mem: 11521M
[02/27 20:25:12 d2.utils.events]:  eta: 1 day, 1:47:03  iter: 6279  total_loss: 1.188  loss_cls: 0.363  loss_box_reg: 0.245  loss_mask: 0.376  loss_rpn_cls: 0.06525  loss_rpn_loc: 0.1223  time: 0.5215  last_time: 0.5137  data_time: 0.0127  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:25:22 d2.utils.events]:  eta: 1 day, 1:46:44  iter: 6299  total_loss: 1.163  loss_cls: 0.3351  loss_box_reg: 0.2418  loss_mask: 0.3839  loss_rpn_cls: 0.07987  loss_rpn_loc: 0.1025  time: 0.5215  last_time: 0.5316  data_time: 0.0103  last_data_time: 0.0129   lr: 0.0001  max_mem: 11521M
[02/27 20:25:33 d2.utils.events]:  eta: 1 day, 1:45:51  iter: 6319  total_loss: 1.313  loss_cls: 0.3678  loss_box_reg: 0.2308  loss_mask: 0.3892  loss_rpn_cls: 0.09514  loss_rpn_loc: 0.146  time: 0.5215  last_time: 0.5543  data_time: 0.0106  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 20:25:43 d2.utils.events]:  eta: 1 day, 1:46:10  iter: 6339  total_loss: 1.173  loss_cls: 0.2858  loss_box_reg: 0.214  loss_mask: 0.3991  loss_rpn_cls: 0.06586  loss_rpn_loc: 0.1142  time: 0.5215  last_time: 0.4809  data_time: 0.0179  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 20:25:54 d2.utils.events]:  eta: 1 day, 1:46:27  iter: 6359  total_loss: 1.217  loss_cls: 0.3486  loss_box_reg: 0.2238  loss_mask: 0.3935  loss_rpn_cls: 0.06929  loss_rpn_loc: 0.07121  time: 0.5216  last_time: 0.5296  data_time: 0.0201  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:26:05 d2.utils.events]:  eta: 1 day, 1:46:11  iter: 6379  total_loss: 1.199  loss_cls: 0.3301  loss_box_reg: 0.2149  loss_mask: 0.3908  loss_rpn_cls: 0.06178  loss_rpn_loc: 0.106  time: 0.5216  last_time: 0.5115  data_time: 0.0191  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 20:26:15 d2.utils.events]:  eta: 1 day, 1:45:36  iter: 6399  total_loss: 1.265  loss_cls: 0.3753  loss_box_reg: 0.2522  loss_mask: 0.3782  loss_rpn_cls: 0.06036  loss_rpn_loc: 0.1372  time: 0.5216  last_time: 0.4989  data_time: 0.0203  last_data_time: 0.0120   lr: 0.0001  max_mem: 11521M
[02/27 20:26:26 d2.utils.events]:  eta: 1 day, 1:45:28  iter: 6419  total_loss: 1.178  loss_cls: 0.3357  loss_box_reg: 0.2268  loss_mask: 0.391  loss_rpn_cls: 0.06587  loss_rpn_loc: 0.1076  time: 0.5216  last_time: 0.4928  data_time: 0.0148  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 20:26:36 d2.utils.events]:  eta: 1 day, 1:45:56  iter: 6439  total_loss: 1.27  loss_cls: 0.3208  loss_box_reg: 0.2192  loss_mask: 0.4145  loss_rpn_cls: 0.09289  loss_rpn_loc: 0.1291  time: 0.5216  last_time: 0.5804  data_time: 0.0214  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 20:26:47 d2.utils.events]:  eta: 1 day, 1:45:50  iter: 6459  total_loss: 1.159  loss_cls: 0.3574  loss_box_reg: 0.221  loss_mask: 0.3786  loss_rpn_cls: 0.082  loss_rpn_loc: 0.1096  time: 0.5216  last_time: 0.5273  data_time: 0.0153  last_data_time: 0.0113   lr: 0.0001  max_mem: 11521M
[02/27 20:26:57 d2.utils.events]:  eta: 1 day, 1:46:03  iter: 6479  total_loss: 1.295  loss_cls: 0.3436  loss_box_reg: 0.2718  loss_mask: 0.4095  loss_rpn_cls: 0.06594  loss_rpn_loc: 0.1583  time: 0.5217  last_time: 0.5863  data_time: 0.0145  last_data_time: 0.0116   lr: 0.0001  max_mem: 11521M
[02/27 20:27:08 d2.utils.events]:  eta: 1 day, 1:46:43  iter: 6499  total_loss: 1.282  loss_cls: 0.3497  loss_box_reg: 0.2405  loss_mask: 0.3749  loss_rpn_cls: 0.06592  loss_rpn_loc: 0.1504  time: 0.5216  last_time: 0.5169  data_time: 0.0189  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:27:19 d2.utils.events]:  eta: 1 day, 1:47:28  iter: 6519  total_loss: 1.226  loss_cls: 0.3494  loss_box_reg: 0.2454  loss_mask: 0.3758  loss_rpn_cls: 0.06358  loss_rpn_loc: 0.1135  time: 0.5217  last_time: 0.5222  data_time: 0.0328  last_data_time: 0.0628   lr: 0.0001  max_mem: 11521M
[02/27 20:27:29 d2.utils.events]:  eta: 1 day, 1:47:35  iter: 6539  total_loss: 1.341  loss_cls: 0.3966  loss_box_reg: 0.3138  loss_mask: 0.4137  loss_rpn_cls: 0.08415  loss_rpn_loc: 0.1107  time: 0.5216  last_time: 0.5222  data_time: 0.0190  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 20:27:40 d2.utils.events]:  eta: 1 day, 1:48:04  iter: 6559  total_loss: 1.241  loss_cls: 0.3528  loss_box_reg: 0.2303  loss_mask: 0.4331  loss_rpn_cls: 0.06905  loss_rpn_loc: 0.1153  time: 0.5217  last_time: 0.5592  data_time: 0.0247  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 20:27:50 d2.utils.events]:  eta: 1 day, 1:47:39  iter: 6579  total_loss: 1.317  loss_cls: 0.396  loss_box_reg: 0.2775  loss_mask: 0.3486  loss_rpn_cls: 0.08206  loss_rpn_loc: 0.1004  time: 0.5217  last_time: 0.5331  data_time: 0.0260  last_data_time: 0.0960   lr: 0.0001  max_mem: 11521M
[02/27 20:28:01 d2.utils.events]:  eta: 1 day, 1:47:24  iter: 6599  total_loss: 1.291  loss_cls: 0.3793  loss_box_reg: 0.3346  loss_mask: 0.4022  loss_rpn_cls: 0.0685  loss_rpn_loc: 0.1157  time: 0.5217  last_time: 0.5236  data_time: 0.0212  last_data_time: 0.0259   lr: 0.0001  max_mem: 11521M
[02/27 20:28:11 d2.utils.events]:  eta: 1 day, 1:47:07  iter: 6619  total_loss: 1.183  loss_cls: 0.3435  loss_box_reg: 0.217  loss_mask: 0.3672  loss_rpn_cls: 0.06783  loss_rpn_loc: 0.1473  time: 0.5217  last_time: 0.5164  data_time: 0.0193  last_data_time: 0.0200   lr: 0.0001  max_mem: 11521M
[02/27 20:28:22 d2.utils.events]:  eta: 1 day, 1:46:56  iter: 6639  total_loss: 1.316  loss_cls: 0.4525  loss_box_reg: 0.2899  loss_mask: 0.3903  loss_rpn_cls: 0.06022  loss_rpn_loc: 0.1309  time: 0.5218  last_time: 0.5666  data_time: 0.0233  last_data_time: 0.0515   lr: 0.0001  max_mem: 11521M
[02/27 20:28:33 d2.utils.events]:  eta: 1 day, 1:46:53  iter: 6659  total_loss: 1.386  loss_cls: 0.3957  loss_box_reg: 0.2668  loss_mask: 0.3879  loss_rpn_cls: 0.06964  loss_rpn_loc: 0.1886  time: 0.5217  last_time: 0.5206  data_time: 0.0165  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 20:28:43 d2.utils.events]:  eta: 1 day, 1:46:05  iter: 6679  total_loss: 1.364  loss_cls: 0.3694  loss_box_reg: 0.2561  loss_mask: 0.4454  loss_rpn_cls: 0.07533  loss_rpn_loc: 0.161  time: 0.5217  last_time: 0.5000  data_time: 0.0194  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 20:28:53 d2.utils.events]:  eta: 1 day, 1:45:54  iter: 6699  total_loss: 1.209  loss_cls: 0.343  loss_box_reg: 0.2316  loss_mask: 0.3884  loss_rpn_cls: 0.06609  loss_rpn_loc: 0.1205  time: 0.5217  last_time: 0.5137  data_time: 0.0218  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 20:29:04 d2.utils.events]:  eta: 1 day, 1:45:34  iter: 6719  total_loss: 1.277  loss_cls: 0.3568  loss_box_reg: 0.2439  loss_mask: 0.3974  loss_rpn_cls: 0.09819  loss_rpn_loc: 0.1331  time: 0.5217  last_time: 0.5393  data_time: 0.0187  last_data_time: 0.0451   lr: 0.0001  max_mem: 11521M
[02/27 20:29:14 d2.utils.events]:  eta: 1 day, 1:45:23  iter: 6739  total_loss: 1.156  loss_cls: 0.3017  loss_box_reg: 0.1953  loss_mask: 0.4545  loss_rpn_cls: 0.06879  loss_rpn_loc: 0.1148  time: 0.5217  last_time: 0.5984  data_time: 0.0150  last_data_time: 0.0199   lr: 0.0001  max_mem: 11521M
[02/27 20:29:24 d2.utils.events]:  eta: 1 day, 1:43:50  iter: 6759  total_loss: 1.232  loss_cls: 0.3465  loss_box_reg: 0.2247  loss_mask: 0.3954  loss_rpn_cls: 0.05303  loss_rpn_loc: 0.1101  time: 0.5217  last_time: 0.4878  data_time: 0.0247  last_data_time: 0.0300   lr: 0.0001  max_mem: 11521M
[02/27 20:29:35 d2.utils.events]:  eta: 1 day, 1:45:02  iter: 6779  total_loss: 1.37  loss_cls: 0.4101  loss_box_reg: 0.2647  loss_mask: 0.3926  loss_rpn_cls: 0.09462  loss_rpn_loc: 0.1232  time: 0.5217  last_time: 0.5174  data_time: 0.0169  last_data_time: 0.0216   lr: 0.0001  max_mem: 11521M
[02/27 20:29:46 d2.utils.events]:  eta: 1 day, 1:45:05  iter: 6799  total_loss: 1.473  loss_cls: 0.4527  loss_box_reg: 0.294  loss_mask: 0.4182  loss_rpn_cls: 0.08224  loss_rpn_loc: 0.1431  time: 0.5217  last_time: 0.5343  data_time: 0.0153  last_data_time: 0.0273   lr: 0.0001  max_mem: 11521M
[02/27 20:29:56 d2.utils.events]:  eta: 1 day, 1:43:39  iter: 6819  total_loss: 1.37  loss_cls: 0.4394  loss_box_reg: 0.2778  loss_mask: 0.4013  loss_rpn_cls: 0.08264  loss_rpn_loc: 0.1926  time: 0.5217  last_time: 0.5180  data_time: 0.0177  last_data_time: 0.0140   lr: 0.0001  max_mem: 11521M
[02/27 20:30:06 d2.utils.events]:  eta: 1 day, 1:42:03  iter: 6839  total_loss: 1.225  loss_cls: 0.3596  loss_box_reg: 0.268  loss_mask: 0.367  loss_rpn_cls: 0.07074  loss_rpn_loc: 0.1207  time: 0.5216  last_time: 0.5157  data_time: 0.0135  last_data_time: 0.0301   lr: 0.0001  max_mem: 11521M
[02/27 20:30:17 d2.utils.events]:  eta: 1 day, 1:41:36  iter: 6859  total_loss: 1.074  loss_cls: 0.3151  loss_box_reg: 0.2031  loss_mask: 0.3945  loss_rpn_cls: 0.06084  loss_rpn_loc: 0.08959  time: 0.5216  last_time: 0.4992  data_time: 0.0299  last_data_time: 0.0522   lr: 0.0001  max_mem: 11521M
[02/27 20:30:27 d2.utils.events]:  eta: 1 day, 1:41:12  iter: 6879  total_loss: 1.051  loss_cls: 0.2899  loss_box_reg: 0.1868  loss_mask: 0.3458  loss_rpn_cls: 0.06528  loss_rpn_loc: 0.08308  time: 0.5216  last_time: 0.5119  data_time: 0.0190  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 20:30:38 d2.utils.events]:  eta: 1 day, 1:39:53  iter: 6899  total_loss: 1.271  loss_cls: 0.364  loss_box_reg: 0.2729  loss_mask: 0.3889  loss_rpn_cls: 0.06042  loss_rpn_loc: 0.1455  time: 0.5216  last_time: 0.4915  data_time: 0.0178  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:30:48 d2.utils.events]:  eta: 1 day, 1:39:49  iter: 6919  total_loss: 1.445  loss_cls: 0.4262  loss_box_reg: 0.313  loss_mask: 0.4022  loss_rpn_cls: 0.07696  loss_rpn_loc: 0.141  time: 0.5216  last_time: 0.5170  data_time: 0.0216  last_data_time: 0.0188   lr: 0.0001  max_mem: 11521M
[02/27 20:30:59 d2.utils.events]:  eta: 1 day, 1:40:13  iter: 6939  total_loss: 1.29  loss_cls: 0.3507  loss_box_reg: 0.2693  loss_mask: 0.4011  loss_rpn_cls: 0.08247  loss_rpn_loc: 0.1725  time: 0.5216  last_time: 0.6321  data_time: 0.0247  last_data_time: 0.0264   lr: 0.0001  max_mem: 11521M
[02/27 20:31:09 d2.utils.events]:  eta: 1 day, 1:39:39  iter: 6959  total_loss: 1.248  loss_cls: 0.3591  loss_box_reg: 0.2347  loss_mask: 0.3957  loss_rpn_cls: 0.06462  loss_rpn_loc: 0.1582  time: 0.5216  last_time: 0.4993  data_time: 0.0107  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 20:31:20 d2.utils.events]:  eta: 1 day, 1:40:15  iter: 6979  total_loss: 1.277  loss_cls: 0.3872  loss_box_reg: 0.2561  loss_mask: 0.4156  loss_rpn_cls: 0.0644  loss_rpn_loc: 0.1335  time: 0.5216  last_time: 0.5233  data_time: 0.0277  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:31:30 d2.utils.events]:  eta: 1 day, 1:40:16  iter: 6999  total_loss: 1.204  loss_cls: 0.3524  loss_box_reg: 0.2471  loss_mask: 0.3849  loss_rpn_cls: 0.0652  loss_rpn_loc: 0.1184  time: 0.5216  last_time: 0.5096  data_time: 0.0234  last_data_time: 0.0123   lr: 0.0001  max_mem: 11521M
[02/27 20:31:41 d2.utils.events]:  eta: 1 day, 1:38:52  iter: 7019  total_loss: 1.206  loss_cls: 0.3506  loss_box_reg: 0.2737  loss_mask: 0.397  loss_rpn_cls: 0.07095  loss_rpn_loc: 0.1194  time: 0.5216  last_time: 0.5080  data_time: 0.0140  last_data_time: 0.0113   lr: 0.0001  max_mem: 11521M
[02/27 20:31:51 d2.utils.events]:  eta: 1 day, 1:38:42  iter: 7039  total_loss: 1.235  loss_cls: 0.3559  loss_box_reg: 0.2227  loss_mask: 0.4202  loss_rpn_cls: 0.0781  loss_rpn_loc: 0.1168  time: 0.5216  last_time: 0.5322  data_time: 0.0174  last_data_time: 0.0073   lr: 0.0001  max_mem: 11521M
[02/27 20:32:01 d2.utils.events]:  eta: 1 day, 1:38:06  iter: 7059  total_loss: 1.295  loss_cls: 0.3567  loss_box_reg: 0.2388  loss_mask: 0.3896  loss_rpn_cls: 0.07779  loss_rpn_loc: 0.1041  time: 0.5216  last_time: 0.5461  data_time: 0.0166  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 20:32:12 d2.utils.events]:  eta: 1 day, 1:38:21  iter: 7079  total_loss: 1.211  loss_cls: 0.3615  loss_box_reg: 0.252  loss_mask: 0.4025  loss_rpn_cls: 0.06947  loss_rpn_loc: 0.1039  time: 0.5216  last_time: 0.5049  data_time: 0.0133  last_data_time: 0.0028   lr: 0.0001  max_mem: 11521M
[02/27 20:32:23 d2.utils.events]:  eta: 1 day, 1:38:20  iter: 7099  total_loss: 1.178  loss_cls: 0.3055  loss_box_reg: 0.2464  loss_mask: 0.3714  loss_rpn_cls: 0.06222  loss_rpn_loc: 0.1079  time: 0.5216  last_time: 0.5295  data_time: 0.0167  last_data_time: 0.0049   lr: 0.0001  max_mem: 11521M
[02/27 20:32:33 d2.utils.events]:  eta: 1 day, 1:38:05  iter: 7119  total_loss: 1.298  loss_cls: 0.3739  loss_box_reg: 0.2561  loss_mask: 0.3763  loss_rpn_cls: 0.08682  loss_rpn_loc: 0.1669  time: 0.5216  last_time: 0.5391  data_time: 0.0153  last_data_time: 0.0137   lr: 0.0001  max_mem: 11521M
[02/27 20:32:44 d2.utils.events]:  eta: 1 day, 1:36:15  iter: 7139  total_loss: 1.209  loss_cls: 0.3442  loss_box_reg: 0.244  loss_mask: 0.4229  loss_rpn_cls: 0.07195  loss_rpn_loc: 0.1613  time: 0.5216  last_time: 0.5075  data_time: 0.0176  last_data_time: 0.0109   lr: 0.0001  max_mem: 11521M
[02/27 20:32:54 d2.utils.events]:  eta: 1 day, 1:36:45  iter: 7159  total_loss: 1.187  loss_cls: 0.3757  loss_box_reg: 0.2438  loss_mask: 0.3808  loss_rpn_cls: 0.08583  loss_rpn_loc: 0.1407  time: 0.5216  last_time: 0.5219  data_time: 0.0252  last_data_time: 0.0098   lr: 0.0001  max_mem: 11521M
[02/27 20:33:05 d2.utils.events]:  eta: 1 day, 1:35:40  iter: 7179  total_loss: 1.216  loss_cls: 0.361  loss_box_reg: 0.2141  loss_mask: 0.3676  loss_rpn_cls: 0.08188  loss_rpn_loc: 0.1095  time: 0.5216  last_time: 0.5213  data_time: 0.0129  last_data_time: 0.0278   lr: 0.0001  max_mem: 11521M
[02/27 20:33:15 d2.utils.events]:  eta: 1 day, 1:36:49  iter: 7199  total_loss: 1.14  loss_cls: 0.3435  loss_box_reg: 0.2196  loss_mask: 0.3796  loss_rpn_cls: 0.05482  loss_rpn_loc: 0.1076  time: 0.5216  last_time: 0.5385  data_time: 0.0145  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 20:33:25 d2.utils.events]:  eta: 1 day, 1:35:57  iter: 7219  total_loss: 1.293  loss_cls: 0.345  loss_box_reg: 0.2501  loss_mask: 0.3974  loss_rpn_cls: 0.06622  loss_rpn_loc: 0.1246  time: 0.5216  last_time: 0.5201  data_time: 0.0168  last_data_time: 0.0299   lr: 0.0001  max_mem: 11521M
[02/27 20:33:36 d2.utils.events]:  eta: 1 day, 1:37:52  iter: 7239  total_loss: 1.229  loss_cls: 0.3802  loss_box_reg: 0.2416  loss_mask: 0.3824  loss_rpn_cls: 0.07228  loss_rpn_loc: 0.1341  time: 0.5216  last_time: 0.5199  data_time: 0.0105  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 20:33:47 d2.utils.events]:  eta: 1 day, 1:38:00  iter: 7259  total_loss: 1.34  loss_cls: 0.4308  loss_box_reg: 0.2601  loss_mask: 0.4146  loss_rpn_cls: 0.07443  loss_rpn_loc: 0.155  time: 0.5216  last_time: 0.5137  data_time: 0.0157  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:33:58 d2.utils.events]:  eta: 1 day, 1:39:16  iter: 7279  total_loss: 1.197  loss_cls: 0.3283  loss_box_reg: 0.2281  loss_mask: 0.3835  loss_rpn_cls: 0.06554  loss_rpn_loc: 0.157  time: 0.5217  last_time: 0.5321  data_time: 0.0184  last_data_time: 0.0252   lr: 0.0001  max_mem: 11521M
[02/27 20:34:08 d2.utils.events]:  eta: 1 day, 1:38:29  iter: 7299  total_loss: 1.328  loss_cls: 0.3413  loss_box_reg: 0.2284  loss_mask: 0.4246  loss_rpn_cls: 0.1053  loss_rpn_loc: 0.1429  time: 0.5217  last_time: 0.4934  data_time: 0.0257  last_data_time: 0.0203   lr: 0.0001  max_mem: 11521M
[02/27 20:34:19 d2.utils.events]:  eta: 1 day, 1:38:16  iter: 7319  total_loss: 1.224  loss_cls: 0.3434  loss_box_reg: 0.2142  loss_mask: 0.4069  loss_rpn_cls: 0.05971  loss_rpn_loc: 0.1147  time: 0.5217  last_time: 0.5390  data_time: 0.0248  last_data_time: 0.0653   lr: 0.0001  max_mem: 11521M
[02/27 20:34:29 d2.utils.events]:  eta: 1 day, 1:38:08  iter: 7339  total_loss: 1.141  loss_cls: 0.321  loss_box_reg: 0.2152  loss_mask: 0.4028  loss_rpn_cls: 0.07224  loss_rpn_loc: 0.1027  time: 0.5217  last_time: 0.5336  data_time: 0.0108  last_data_time: 0.0073   lr: 0.0001  max_mem: 11521M
[02/27 20:34:40 d2.utils.events]:  eta: 1 day, 1:37:18  iter: 7359  total_loss: 1.24  loss_cls: 0.3376  loss_box_reg: 0.223  loss_mask: 0.4044  loss_rpn_cls: 0.06158  loss_rpn_loc: 0.1292  time: 0.5217  last_time: 0.4944  data_time: 0.0106  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 20:34:50 d2.utils.events]:  eta: 1 day, 1:36:58  iter: 7379  total_loss: 1.44  loss_cls: 0.3736  loss_box_reg: 0.2568  loss_mask: 0.4307  loss_rpn_cls: 0.07067  loss_rpn_loc: 0.1432  time: 0.5217  last_time: 0.5126  data_time: 0.0188  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 20:35:01 d2.utils.events]:  eta: 1 day, 1:37:08  iter: 7399  total_loss: 1.284  loss_cls: 0.3983  loss_box_reg: 0.2559  loss_mask: 0.4007  loss_rpn_cls: 0.09218  loss_rpn_loc: 0.1421  time: 0.5217  last_time: 0.5569  data_time: 0.0205  last_data_time: 0.0282   lr: 0.0001  max_mem: 11521M
[02/27 20:35:11 d2.utils.events]:  eta: 1 day, 1:36:58  iter: 7419  total_loss: 1.275  loss_cls: 0.4222  loss_box_reg: 0.2784  loss_mask: 0.3955  loss_rpn_cls: 0.07863  loss_rpn_loc: 0.137  time: 0.5217  last_time: 0.5048  data_time: 0.0128  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 20:35:21 d2.utils.events]:  eta: 1 day, 1:36:15  iter: 7439  total_loss: 1.212  loss_cls: 0.3526  loss_box_reg: 0.2312  loss_mask: 0.3985  loss_rpn_cls: 0.05848  loss_rpn_loc: 0.1033  time: 0.5217  last_time: 0.5633  data_time: 0.0201  last_data_time: 0.0598   lr: 0.0001  max_mem: 11521M
[02/27 20:35:32 d2.utils.events]:  eta: 1 day, 1:35:12  iter: 7459  total_loss: 1.178  loss_cls: 0.3686  loss_box_reg: 0.2251  loss_mask: 0.3915  loss_rpn_cls: 0.07214  loss_rpn_loc: 0.128  time: 0.5216  last_time: 0.5113  data_time: 0.0225  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 20:35:42 d2.utils.events]:  eta: 1 day, 1:33:58  iter: 7479  total_loss: 1.241  loss_cls: 0.3576  loss_box_reg: 0.2271  loss_mask: 0.4167  loss_rpn_cls: 0.08379  loss_rpn_loc: 0.1423  time: 0.5216  last_time: 0.5569  data_time: 0.0197  last_data_time: 0.0642   lr: 0.0001  max_mem: 11521M
[02/27 20:35:52 d2.utils.events]:  eta: 1 day, 1:33:42  iter: 7499  total_loss: 1.222  loss_cls: 0.3689  loss_box_reg: 0.2567  loss_mask: 0.3809  loss_rpn_cls: 0.06617  loss_rpn_loc: 0.1384  time: 0.5216  last_time: 0.5376  data_time: 0.0258  last_data_time: 0.0471   lr: 0.0001  max_mem: 11521M
[02/27 20:36:03 d2.utils.events]:  eta: 1 day, 1:33:29  iter: 7519  total_loss: 1.079  loss_cls: 0.2799  loss_box_reg: 0.1939  loss_mask: 0.4027  loss_rpn_cls: 0.06453  loss_rpn_loc: 0.1059  time: 0.5216  last_time: 0.5103  data_time: 0.0152  last_data_time: 0.0223   lr: 0.0001  max_mem: 11521M
[02/27 20:36:13 d2.utils.events]:  eta: 1 day, 1:33:27  iter: 7539  total_loss: 1.109  loss_cls: 0.3141  loss_box_reg: 0.2214  loss_mask: 0.3921  loss_rpn_cls: 0.05831  loss_rpn_loc: 0.1164  time: 0.5216  last_time: 0.5148  data_time: 0.0216  last_data_time: 0.0062   lr: 0.0001  max_mem: 11521M
[02/27 20:36:24 d2.utils.events]:  eta: 1 day, 1:33:11  iter: 7559  total_loss: 1.366  loss_cls: 0.3785  loss_box_reg: 0.27  loss_mask: 0.4328  loss_rpn_cls: 0.07054  loss_rpn_loc: 0.166  time: 0.5216  last_time: 0.5591  data_time: 0.0142  last_data_time: 0.0409   lr: 0.0001  max_mem: 11521M
[02/27 20:36:35 d2.utils.events]:  eta: 1 day, 1:33:14  iter: 7579  total_loss: 1.347  loss_cls: 0.3905  loss_box_reg: 0.2831  loss_mask: 0.4125  loss_rpn_cls: 0.07991  loss_rpn_loc: 0.1493  time: 0.5216  last_time: 0.5570  data_time: 0.0097  last_data_time: 0.0105   lr: 0.0001  max_mem: 11521M
[02/27 20:36:45 d2.utils.events]:  eta: 1 day, 1:32:50  iter: 7599  total_loss: 1.223  loss_cls: 0.3535  loss_box_reg: 0.2466  loss_mask: 0.3739  loss_rpn_cls: 0.0836  loss_rpn_loc: 0.1416  time: 0.5216  last_time: 0.5425  data_time: 0.0311  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:36:55 d2.utils.events]:  eta: 1 day, 1:32:17  iter: 7619  total_loss: 1.257  loss_cls: 0.3769  loss_box_reg: 0.2288  loss_mask: 0.415  loss_rpn_cls: 0.08137  loss_rpn_loc: 0.09724  time: 0.5216  last_time: 0.5414  data_time: 0.0122  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 20:37:06 d2.utils.events]:  eta: 1 day, 1:32:35  iter: 7639  total_loss: 1.228  loss_cls: 0.3645  loss_box_reg: 0.2379  loss_mask: 0.3748  loss_rpn_cls: 0.06382  loss_rpn_loc: 0.09411  time: 0.5216  last_time: 0.5407  data_time: 0.0119  last_data_time: 0.0090   lr: 0.0001  max_mem: 11521M
[02/27 20:37:17 d2.utils.events]:  eta: 1 day, 1:33:42  iter: 7659  total_loss: 1.267  loss_cls: 0.3856  loss_box_reg: 0.2528  loss_mask: 0.3733  loss_rpn_cls: 0.08495  loss_rpn_loc: 0.1776  time: 0.5217  last_time: 0.5326  data_time: 0.0166  last_data_time: 0.0157   lr: 0.0001  max_mem: 11521M
[02/27 20:37:27 d2.utils.events]:  eta: 1 day, 1:33:56  iter: 7679  total_loss: 1.324  loss_cls: 0.3945  loss_box_reg: 0.253  loss_mask: 0.3978  loss_rpn_cls: 0.09306  loss_rpn_loc: 0.1544  time: 0.5216  last_time: 0.5323  data_time: 0.0123  last_data_time: 0.0102   lr: 0.0001  max_mem: 11521M
[02/27 20:37:38 d2.utils.events]:  eta: 1 day, 1:34:00  iter: 7699  total_loss: 1.257  loss_cls: 0.394  loss_box_reg: 0.2347  loss_mask: 0.3752  loss_rpn_cls: 0.0648  loss_rpn_loc: 0.1359  time: 0.5216  last_time: 0.5017  data_time: 0.0178  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 20:37:48 d2.utils.events]:  eta: 1 day, 1:34:03  iter: 7719  total_loss: 1.121  loss_cls: 0.3696  loss_box_reg: 0.2258  loss_mask: 0.3644  loss_rpn_cls: 0.06355  loss_rpn_loc: 0.1065  time: 0.5216  last_time: 0.5402  data_time: 0.0335  last_data_time: 0.0111   lr: 0.0001  max_mem: 11521M
[02/27 20:37:58 d2.utils.events]:  eta: 1 day, 1:33:26  iter: 7739  total_loss: 1.285  loss_cls: 0.3572  loss_box_reg: 0.2316  loss_mask: 0.3629  loss_rpn_cls: 0.0548  loss_rpn_loc: 0.1247  time: 0.5216  last_time: 0.5649  data_time: 0.0235  last_data_time: 0.0635   lr: 0.0001  max_mem: 11521M
[02/27 20:38:09 d2.utils.events]:  eta: 1 day, 1:33:06  iter: 7759  total_loss: 1.156  loss_cls: 0.3644  loss_box_reg: 0.2217  loss_mask: 0.3681  loss_rpn_cls: 0.0789  loss_rpn_loc: 0.08031  time: 0.5216  last_time: 0.5274  data_time: 0.0291  last_data_time: 0.0023   lr: 0.0001  max_mem: 11521M
[02/27 20:38:20 d2.utils.events]:  eta: 1 day, 1:32:39  iter: 7779  total_loss: 1.093  loss_cls: 0.2963  loss_box_reg: 0.1859  loss_mask: 0.3658  loss_rpn_cls: 0.07734  loss_rpn_loc: 0.1226  time: 0.5216  last_time: 0.5320  data_time: 0.0221  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 20:38:30 d2.utils.events]:  eta: 1 day, 1:32:38  iter: 7799  total_loss: 1.211  loss_cls: 0.3542  loss_box_reg: 0.2446  loss_mask: 0.3953  loss_rpn_cls: 0.08268  loss_rpn_loc: 0.1041  time: 0.5216  last_time: 0.5325  data_time: 0.0131  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 20:38:41 d2.utils.events]:  eta: 1 day, 1:32:35  iter: 7819  total_loss: 1.102  loss_cls: 0.2865  loss_box_reg: 0.1965  loss_mask: 0.3986  loss_rpn_cls: 0.07188  loss_rpn_loc: 0.1532  time: 0.5216  last_time: 0.4965  data_time: 0.0140  last_data_time: 0.0051   lr: 0.0001  max_mem: 11521M
[02/27 20:38:51 d2.utils.events]:  eta: 1 day, 1:32:57  iter: 7839  total_loss: 1.211  loss_cls: 0.3547  loss_box_reg: 0.2709  loss_mask: 0.3915  loss_rpn_cls: 0.04662  loss_rpn_loc: 0.1404  time: 0.5216  last_time: 0.5409  data_time: 0.0129  last_data_time: 0.0403   lr: 0.0001  max_mem: 11521M
[02/27 20:39:02 d2.utils.events]:  eta: 1 day, 1:33:41  iter: 7859  total_loss: 1.219  loss_cls: 0.3526  loss_box_reg: 0.2441  loss_mask: 0.3761  loss_rpn_cls: 0.08446  loss_rpn_loc: 0.1242  time: 0.5216  last_time: 0.5144  data_time: 0.0184  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 20:39:12 d2.utils.events]:  eta: 1 day, 1:32:00  iter: 7879  total_loss: 0.9905  loss_cls: 0.2886  loss_box_reg: 0.2059  loss_mask: 0.3748  loss_rpn_cls: 0.07059  loss_rpn_loc: 0.1056  time: 0.5216  last_time: 0.5374  data_time: 0.0140  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 20:39:22 d2.utils.events]:  eta: 1 day, 1:32:32  iter: 7899  total_loss: 1.078  loss_cls: 0.3427  loss_box_reg: 0.2224  loss_mask: 0.3794  loss_rpn_cls: 0.07561  loss_rpn_loc: 0.08523  time: 0.5216  last_time: 0.4816  data_time: 0.0157  last_data_time: 0.0411   lr: 0.0001  max_mem: 11521M
[02/27 20:39:33 d2.utils.events]:  eta: 1 day, 1:32:22  iter: 7919  total_loss: 1.156  loss_cls: 0.33  loss_box_reg: 0.2134  loss_mask: 0.38  loss_rpn_cls: 0.06869  loss_rpn_loc: 0.1525  time: 0.5216  last_time: 0.5143  data_time: 0.0128  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 20:39:43 d2.utils.events]:  eta: 1 day, 1:31:29  iter: 7939  total_loss: 1.249  loss_cls: 0.3769  loss_box_reg: 0.2765  loss_mask: 0.4044  loss_rpn_cls: 0.05908  loss_rpn_loc: 0.1243  time: 0.5216  last_time: 0.5378  data_time: 0.0174  last_data_time: 0.0212   lr: 0.0001  max_mem: 11521M
[02/27 20:39:54 d2.utils.events]:  eta: 1 day, 1:31:40  iter: 7959  total_loss: 1.272  loss_cls: 0.3313  loss_box_reg: 0.2333  loss_mask: 0.4218  loss_rpn_cls: 0.06096  loss_rpn_loc: 0.1258  time: 0.5216  last_time: 0.5176  data_time: 0.0213  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 20:40:04 d2.utils.events]:  eta: 1 day, 1:31:05  iter: 7979  total_loss: 1.483  loss_cls: 0.4955  loss_box_reg: 0.3102  loss_mask: 0.4123  loss_rpn_cls: 0.09431  loss_rpn_loc: 0.1852  time: 0.5216  last_time: 0.5053  data_time: 0.0148  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 20:40:15 d2.utils.events]:  eta: 1 day, 1:31:44  iter: 7999  total_loss: 1.326  loss_cls: 0.3246  loss_box_reg: 0.2596  loss_mask: 0.4375  loss_rpn_cls: 0.08329  loss_rpn_loc: 0.1339  time: 0.5216  last_time: 0.5279  data_time: 0.0208  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 20:40:25 d2.utils.events]:  eta: 1 day, 1:31:34  iter: 8019  total_loss: 1.037  loss_cls: 0.2785  loss_box_reg: 0.1957  loss_mask: 0.3763  loss_rpn_cls: 0.06474  loss_rpn_loc: 0.1171  time: 0.5216  last_time: 0.5085  data_time: 0.0250  last_data_time: 0.0081   lr: 0.0001  max_mem: 11521M
[02/27 20:40:36 d2.utils.events]:  eta: 1 day, 1:30:58  iter: 8039  total_loss: 1.212  loss_cls: 0.3244  loss_box_reg: 0.2058  loss_mask: 0.4202  loss_rpn_cls: 0.06784  loss_rpn_loc: 0.1792  time: 0.5216  last_time: 0.5007  data_time: 0.0184  last_data_time: 0.0489   lr: 0.0001  max_mem: 11521M
[02/27 20:40:46 d2.utils.events]:  eta: 1 day, 1:31:08  iter: 8059  total_loss: 1.117  loss_cls: 0.3141  loss_box_reg: 0.2036  loss_mask: 0.3543  loss_rpn_cls: 0.05322  loss_rpn_loc: 0.1023  time: 0.5216  last_time: 0.4722  data_time: 0.0169  last_data_time: 0.0090   lr: 0.0001  max_mem: 11521M
[02/27 20:40:57 d2.utils.events]:  eta: 1 day, 1:30:12  iter: 8079  total_loss: 1.132  loss_cls: 0.2837  loss_box_reg: 0.2252  loss_mask: 0.3758  loss_rpn_cls: 0.04927  loss_rpn_loc: 0.1271  time: 0.5216  last_time: 0.5215  data_time: 0.0176  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 20:41:07 d2.utils.events]:  eta: 1 day, 1:29:52  iter: 8099  total_loss: 1.346  loss_cls: 0.3918  loss_box_reg: 0.2706  loss_mask: 0.3973  loss_rpn_cls: 0.06535  loss_rpn_loc: 0.1874  time: 0.5216  last_time: 0.5056  data_time: 0.0163  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:41:18 d2.utils.events]:  eta: 1 day, 1:29:55  iter: 8119  total_loss: 1.103  loss_cls: 0.289  loss_box_reg: 0.1951  loss_mask: 0.3887  loss_rpn_cls: 0.0811  loss_rpn_loc: 0.1237  time: 0.5216  last_time: 0.5534  data_time: 0.0216  last_data_time: 0.0424   lr: 0.0001  max_mem: 11521M
[02/27 20:41:28 d2.utils.events]:  eta: 1 day, 1:30:26  iter: 8139  total_loss: 1.215  loss_cls: 0.3159  loss_box_reg: 0.2127  loss_mask: 0.3857  loss_rpn_cls: 0.07453  loss_rpn_loc: 0.09133  time: 0.5216  last_time: 0.5066  data_time: 0.0138  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:41:39 d2.utils.events]:  eta: 1 day, 1:30:31  iter: 8159  total_loss: 1.37  loss_cls: 0.4362  loss_box_reg: 0.3001  loss_mask: 0.4263  loss_rpn_cls: 0.06052  loss_rpn_loc: 0.1266  time: 0.5216  last_time: 0.5525  data_time: 0.0285  last_data_time: 0.0106   lr: 0.0001  max_mem: 11521M
[02/27 20:41:50 d2.utils.events]:  eta: 1 day, 1:31:15  iter: 8179  total_loss: 1.211  loss_cls: 0.3068  loss_box_reg: 0.2308  loss_mask: 0.388  loss_rpn_cls: 0.06612  loss_rpn_loc: 0.1393  time: 0.5216  last_time: 0.4892  data_time: 0.0213  last_data_time: 0.0130   lr: 0.0001  max_mem: 11521M
[02/27 20:42:00 d2.utils.events]:  eta: 1 day, 1:31:05  iter: 8199  total_loss: 1.226  loss_cls: 0.3266  loss_box_reg: 0.2293  loss_mask: 0.3895  loss_rpn_cls: 0.07965  loss_rpn_loc: 0.153  time: 0.5216  last_time: 0.5289  data_time: 0.0297  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:42:11 d2.utils.events]:  eta: 1 day, 1:31:33  iter: 8219  total_loss: 1.175  loss_cls: 0.3262  loss_box_reg: 0.2512  loss_mask: 0.4014  loss_rpn_cls: 0.06493  loss_rpn_loc: 0.1283  time: 0.5216  last_time: 0.5216  data_time: 0.0336  last_data_time: 0.0264   lr: 0.0001  max_mem: 11521M
[02/27 20:42:21 d2.utils.events]:  eta: 1 day, 1:31:12  iter: 8239  total_loss: 1.319  loss_cls: 0.4321  loss_box_reg: 0.2564  loss_mask: 0.4019  loss_rpn_cls: 0.0898  loss_rpn_loc: 0.1111  time: 0.5217  last_time: 0.5335  data_time: 0.0262  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 20:42:32 d2.utils.events]:  eta: 1 day, 1:30:42  iter: 8259  total_loss: 1.242  loss_cls: 0.3723  loss_box_reg: 0.2609  loss_mask: 0.3985  loss_rpn_cls: 0.06307  loss_rpn_loc: 0.108  time: 0.5217  last_time: 0.5249  data_time: 0.0163  last_data_time: 0.0249   lr: 0.0001  max_mem: 11521M
[02/27 20:42:42 d2.utils.events]:  eta: 1 day, 1:29:28  iter: 8279  total_loss: 1.125  loss_cls: 0.3131  loss_box_reg: 0.2425  loss_mask: 0.3649  loss_rpn_cls: 0.06107  loss_rpn_loc: 0.1031  time: 0.5217  last_time: 0.5169  data_time: 0.0136  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 20:42:53 d2.utils.events]:  eta: 1 day, 1:30:03  iter: 8299  total_loss: 1.215  loss_cls: 0.345  loss_box_reg: 0.2063  loss_mask: 0.4181  loss_rpn_cls: 0.06445  loss_rpn_loc: 0.1231  time: 0.5217  last_time: 0.5163  data_time: 0.0250  last_data_time: 0.0432   lr: 0.0001  max_mem: 11521M
[02/27 20:43:03 d2.utils.events]:  eta: 1 day, 1:30:07  iter: 8319  total_loss: 1.067  loss_cls: 0.3122  loss_box_reg: 0.2303  loss_mask: 0.3925  loss_rpn_cls: 0.05689  loss_rpn_loc: 0.08858  time: 0.5217  last_time: 0.5235  data_time: 0.0297  last_data_time: 0.0508   lr: 0.0001  max_mem: 11521M
[02/27 20:43:14 d2.utils.events]:  eta: 1 day, 1:29:42  iter: 8339  total_loss: 1.4  loss_cls: 0.4148  loss_box_reg: 0.2734  loss_mask: 0.408  loss_rpn_cls: 0.06547  loss_rpn_loc: 0.1847  time: 0.5216  last_time: 0.5230  data_time: 0.0235  last_data_time: 0.0055   lr: 0.0001  max_mem: 11521M
[02/27 20:43:24 d2.utils.events]:  eta: 1 day, 1:29:41  iter: 8359  total_loss: 1.125  loss_cls: 0.3188  loss_box_reg: 0.2005  loss_mask: 0.3738  loss_rpn_cls: 0.08636  loss_rpn_loc: 0.1219  time: 0.5217  last_time: 0.4898  data_time: 0.0252  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 20:43:35 d2.utils.events]:  eta: 1 day, 1:30:33  iter: 8379  total_loss: 1.25  loss_cls: 0.3308  loss_box_reg: 0.2289  loss_mask: 0.4198  loss_rpn_cls: 0.07334  loss_rpn_loc: 0.1034  time: 0.5217  last_time: 0.5243  data_time: 0.0081  last_data_time: 0.0137   lr: 0.0001  max_mem: 11521M
[02/27 20:43:45 d2.utils.events]:  eta: 1 day, 1:29:25  iter: 8399  total_loss: 1.269  loss_cls: 0.3577  loss_box_reg: 0.2588  loss_mask: 0.3901  loss_rpn_cls: 0.07072  loss_rpn_loc: 0.1758  time: 0.5216  last_time: 0.4927  data_time: 0.0105  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 20:43:56 d2.utils.events]:  eta: 1 day, 1:29:56  iter: 8419  total_loss: 1.149  loss_cls: 0.3194  loss_box_reg: 0.2296  loss_mask: 0.3606  loss_rpn_cls: 0.07029  loss_rpn_loc: 0.105  time: 0.5217  last_time: 0.5498  data_time: 0.0213  last_data_time: 0.0113   lr: 0.0001  max_mem: 11521M
[02/27 20:44:06 d2.utils.events]:  eta: 1 day, 1:30:07  iter: 8439  total_loss: 1.215  loss_cls: 0.3872  loss_box_reg: 0.2401  loss_mask: 0.4  loss_rpn_cls: 0.0658  loss_rpn_loc: 0.1384  time: 0.5216  last_time: 0.5456  data_time: 0.0268  last_data_time: 0.0763   lr: 0.0001  max_mem: 11521M
[02/27 20:44:16 d2.utils.events]:  eta: 1 day, 1:29:04  iter: 8459  total_loss: 1.291  loss_cls: 0.3767  loss_box_reg: 0.2514  loss_mask: 0.3611  loss_rpn_cls: 0.08945  loss_rpn_loc: 0.1386  time: 0.5216  last_time: 0.5035  data_time: 0.0105  last_data_time: 0.0294   lr: 0.0001  max_mem: 11521M
[02/27 20:44:27 d2.utils.events]:  eta: 1 day, 1:30:47  iter: 8479  total_loss: 1.257  loss_cls: 0.3395  loss_box_reg: 0.2538  loss_mask: 0.3638  loss_rpn_cls: 0.08477  loss_rpn_loc: 0.1589  time: 0.5216  last_time: 0.5111  data_time: 0.0237  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 20:44:38 d2.utils.events]:  eta: 1 day, 1:31:37  iter: 8499  total_loss: 1.187  loss_cls: 0.3401  loss_box_reg: 0.2491  loss_mask: 0.3536  loss_rpn_cls: 0.06767  loss_rpn_loc: 0.1556  time: 0.5216  last_time: 0.5174  data_time: 0.0173  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 20:44:48 d2.utils.events]:  eta: 1 day, 1:32:42  iter: 8519  total_loss: 1.241  loss_cls: 0.3798  loss_box_reg: 0.2681  loss_mask: 0.3823  loss_rpn_cls: 0.07249  loss_rpn_loc: 0.1332  time: 0.5216  last_time: 0.5233  data_time: 0.0173  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 20:44:59 d2.utils.events]:  eta: 1 day, 1:32:06  iter: 8539  total_loss: 1.165  loss_cls: 0.3208  loss_box_reg: 0.2165  loss_mask: 0.3643  loss_rpn_cls: 0.05911  loss_rpn_loc: 0.1473  time: 0.5216  last_time: 0.5369  data_time: 0.0131  last_data_time: 0.0519   lr: 0.0001  max_mem: 11521M
[02/27 20:45:09 d2.utils.events]:  eta: 1 day, 1:32:21  iter: 8559  total_loss: 1.266  loss_cls: 0.382  loss_box_reg: 0.2408  loss_mask: 0.4048  loss_rpn_cls: 0.05156  loss_rpn_loc: 0.1603  time: 0.5216  last_time: 0.5393  data_time: 0.0099  last_data_time: 0.0107   lr: 0.0001  max_mem: 11521M
[02/27 20:45:20 d2.utils.events]:  eta: 1 day, 1:30:44  iter: 8579  total_loss: 1.328  loss_cls: 0.3894  loss_box_reg: 0.2713  loss_mask: 0.3908  loss_rpn_cls: 0.06674  loss_rpn_loc: 0.09909  time: 0.5216  last_time: 0.5096  data_time: 0.0187  last_data_time: 0.0240   lr: 0.0001  max_mem: 11521M
[02/27 20:45:31 d2.utils.events]:  eta: 1 day, 1:32:11  iter: 8599  total_loss: 1.145  loss_cls: 0.3544  loss_box_reg: 0.2532  loss_mask: 0.3689  loss_rpn_cls: 0.05734  loss_rpn_loc: 0.08324  time: 0.5217  last_time: 0.5485  data_time: 0.0128  last_data_time: 0.0027   lr: 0.0001  max_mem: 11521M
[02/27 20:45:41 d2.utils.events]:  eta: 1 day, 1:32:00  iter: 8619  total_loss: 1.228  loss_cls: 0.3612  loss_box_reg: 0.2311  loss_mask: 0.3962  loss_rpn_cls: 0.07258  loss_rpn_loc: 0.1197  time: 0.5217  last_time: 0.4892  data_time: 0.0119  last_data_time: 0.0105   lr: 0.0001  max_mem: 11521M
[02/27 20:45:52 d2.utils.events]:  eta: 1 day, 1:30:30  iter: 8639  total_loss: 1.245  loss_cls: 0.3464  loss_box_reg: 0.2569  loss_mask: 0.3849  loss_rpn_cls: 0.07557  loss_rpn_loc: 0.1079  time: 0.5217  last_time: 0.5181  data_time: 0.0217  last_data_time: 0.0284   lr: 0.0001  max_mem: 11521M
[02/27 20:46:02 d2.utils.events]:  eta: 1 day, 1:27:49  iter: 8659  total_loss: 1.162  loss_cls: 0.3248  loss_box_reg: 0.2539  loss_mask: 0.3773  loss_rpn_cls: 0.06886  loss_rpn_loc: 0.1009  time: 0.5216  last_time: 0.5183  data_time: 0.0154  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 20:46:13 d2.utils.events]:  eta: 1 day, 1:28:31  iter: 8679  total_loss: 1.158  loss_cls: 0.3009  loss_box_reg: 0.2068  loss_mask: 0.3667  loss_rpn_cls: 0.06168  loss_rpn_loc: 0.1194  time: 0.5216  last_time: 0.5283  data_time: 0.0195  last_data_time: 0.0088   lr: 0.0001  max_mem: 11521M
[02/27 20:46:23 d2.utils.events]:  eta: 1 day, 1:28:02  iter: 8699  total_loss: 1.143  loss_cls: 0.315  loss_box_reg: 0.2074  loss_mask: 0.352  loss_rpn_cls: 0.05573  loss_rpn_loc: 0.1065  time: 0.5216  last_time: 0.4956  data_time: 0.0121  last_data_time: 0.0102   lr: 0.0001  max_mem: 11521M
[02/27 20:46:33 d2.utils.events]:  eta: 1 day, 1:27:36  iter: 8719  total_loss: 1.126  loss_cls: 0.304  loss_box_reg: 0.2139  loss_mask: 0.3727  loss_rpn_cls: 0.06879  loss_rpn_loc: 0.1084  time: 0.5216  last_time: 0.5074  data_time: 0.0154  last_data_time: 0.0072   lr: 0.0001  max_mem: 11521M
[02/27 20:46:44 d2.utils.events]:  eta: 1 day, 1:28:44  iter: 8739  total_loss: 1.352  loss_cls: 0.3923  loss_box_reg: 0.2907  loss_mask: 0.4218  loss_rpn_cls: 0.07814  loss_rpn_loc: 0.1605  time: 0.5217  last_time: 0.5314  data_time: 0.0216  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 20:46:55 d2.utils.events]:  eta: 1 day, 1:29:38  iter: 8759  total_loss: 1.317  loss_cls: 0.4109  loss_box_reg: 0.266  loss_mask: 0.3885  loss_rpn_cls: 0.08292  loss_rpn_loc: 0.1667  time: 0.5217  last_time: 0.5175  data_time: 0.0178  last_data_time: 0.0078   lr: 0.0001  max_mem: 11521M
[02/27 20:47:05 d2.utils.events]:  eta: 1 day, 1:28:05  iter: 8779  total_loss: 1.224  loss_cls: 0.3677  loss_box_reg: 0.2381  loss_mask: 0.3608  loss_rpn_cls: 0.07567  loss_rpn_loc: 0.1555  time: 0.5217  last_time: 0.5059  data_time: 0.0133  last_data_time: 0.0032   lr: 0.0001  max_mem: 11521M
[02/27 20:47:16 d2.utils.events]:  eta: 1 day, 1:26:06  iter: 8799  total_loss: 1.144  loss_cls: 0.3267  loss_box_reg: 0.2163  loss_mask: 0.3706  loss_rpn_cls: 0.06456  loss_rpn_loc: 0.1136  time: 0.5216  last_time: 0.5134  data_time: 0.0154  last_data_time: 0.0100   lr: 0.0001  max_mem: 11521M
[02/27 20:47:26 d2.utils.events]:  eta: 1 day, 1:26:26  iter: 8819  total_loss: 1.238  loss_cls: 0.3791  loss_box_reg: 0.2707  loss_mask: 0.4014  loss_rpn_cls: 0.07756  loss_rpn_loc: 0.1379  time: 0.5217  last_time: 0.5324  data_time: 0.0217  last_data_time: 0.0067   lr: 0.0001  max_mem: 11521M
[02/27 20:47:37 d2.utils.events]:  eta: 1 day, 1:25:58  iter: 8839  total_loss: 1.304  loss_cls: 0.396  loss_box_reg: 0.2763  loss_mask: 0.4277  loss_rpn_cls: 0.08516  loss_rpn_loc: 0.1639  time: 0.5216  last_time: 0.4913  data_time: 0.0208  last_data_time: 0.0128   lr: 0.0001  max_mem: 11521M
[02/27 20:47:47 d2.utils.events]:  eta: 1 day, 1:26:38  iter: 8859  total_loss: 1.256  loss_cls: 0.3242  loss_box_reg: 0.2482  loss_mask: 0.3819  loss_rpn_cls: 0.06059  loss_rpn_loc: 0.1521  time: 0.5217  last_time: 0.5258  data_time: 0.0271  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 20:47:58 d2.utils.events]:  eta: 1 day, 1:28:07  iter: 8879  total_loss: 1.297  loss_cls: 0.3572  loss_box_reg: 0.2592  loss_mask: 0.4153  loss_rpn_cls: 0.07108  loss_rpn_loc: 0.1581  time: 0.5217  last_time: 0.5592  data_time: 0.0255  last_data_time: 0.0059   lr: 0.0001  max_mem: 11521M
[02/27 20:48:08 d2.utils.events]:  eta: 1 day, 1:26:36  iter: 8899  total_loss: 1.246  loss_cls: 0.3654  loss_box_reg: 0.2703  loss_mask: 0.374  loss_rpn_cls: 0.05975  loss_rpn_loc: 0.1448  time: 0.5216  last_time: 0.5081  data_time: 0.0186  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 20:48:18 d2.utils.events]:  eta: 1 day, 1:24:50  iter: 8919  total_loss: 1.35  loss_cls: 0.4674  loss_box_reg: 0.3208  loss_mask: 0.4146  loss_rpn_cls: 0.0693  loss_rpn_loc: 0.1394  time: 0.5216  last_time: 0.4891  data_time: 0.0153  last_data_time: 0.0097   lr: 0.0001  max_mem: 11521M
[02/27 20:48:29 d2.utils.events]:  eta: 1 day, 1:24:28  iter: 8939  total_loss: 1.302  loss_cls: 0.3452  loss_box_reg: 0.2391  loss_mask: 0.3616  loss_rpn_cls: 0.06751  loss_rpn_loc: 0.1768  time: 0.5216  last_time: 0.5400  data_time: 0.0108  last_data_time: 0.0473   lr: 0.0001  max_mem: 11521M
[02/27 20:48:39 d2.utils.events]:  eta: 1 day, 1:24:55  iter: 8959  total_loss: 1.299  loss_cls: 0.389  loss_box_reg: 0.2794  loss_mask: 0.3786  loss_rpn_cls: 0.06914  loss_rpn_loc: 0.1457  time: 0.5216  last_time: 0.5282  data_time: 0.0121  last_data_time: 0.0131   lr: 0.0001  max_mem: 11521M
[02/27 20:48:50 d2.utils.events]:  eta: 1 day, 1:25:03  iter: 8979  total_loss: 1.115  loss_cls: 0.3654  loss_box_reg: 0.2353  loss_mask: 0.4114  loss_rpn_cls: 0.06704  loss_rpn_loc: 0.1205  time: 0.5216  last_time: 0.5021  data_time: 0.0074  last_data_time: 0.0052   lr: 0.0001  max_mem: 11521M
[02/27 20:49:00 d2.utils.events]:  eta: 1 day, 1:23:33  iter: 8999  total_loss: 1.157  loss_cls: 0.341  loss_box_reg: 0.2418  loss_mask: 0.346  loss_rpn_cls: 0.06419  loss_rpn_loc: 0.1018  time: 0.5216  last_time: 0.5211  data_time: 0.0115  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 20:49:10 d2.utils.events]:  eta: 1 day, 1:23:47  iter: 9019  total_loss: 1.315  loss_cls: 0.3687  loss_box_reg: 0.2689  loss_mask: 0.4284  loss_rpn_cls: 0.07391  loss_rpn_loc: 0.1464  time: 0.5216  last_time: 0.5417  data_time: 0.0185  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:49:21 d2.utils.events]:  eta: 1 day, 1:24:53  iter: 9039  total_loss: 1.259  loss_cls: 0.3596  loss_box_reg: 0.2387  loss_mask: 0.3878  loss_rpn_cls: 0.06121  loss_rpn_loc: 0.1399  time: 0.5216  last_time: 0.5336  data_time: 0.0181  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 20:49:31 d2.utils.events]:  eta: 1 day, 1:23:54  iter: 9059  total_loss: 1.088  loss_cls: 0.3443  loss_box_reg: 0.24  loss_mask: 0.3514  loss_rpn_cls: 0.08886  loss_rpn_loc: 0.09906  time: 0.5216  last_time: 0.5018  data_time: 0.0150  last_data_time: 0.0056   lr: 0.0001  max_mem: 11521M
[02/27 20:49:42 d2.utils.events]:  eta: 1 day, 1:23:52  iter: 9079  total_loss: 1.215  loss_cls: 0.3655  loss_box_reg: 0.2199  loss_mask: 0.4064  loss_rpn_cls: 0.06739  loss_rpn_loc: 0.1316  time: 0.5215  last_time: 0.4887  data_time: 0.0146  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 20:49:52 d2.utils.events]:  eta: 1 day, 1:23:42  iter: 9099  total_loss: 1.185  loss_cls: 0.3249  loss_box_reg: 0.2172  loss_mask: 0.398  loss_rpn_cls: 0.0627  loss_rpn_loc: 0.1369  time: 0.5216  last_time: 0.5029  data_time: 0.0114  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 20:50:03 d2.utils.events]:  eta: 1 day, 1:22:35  iter: 9119  total_loss: 1.033  loss_cls: 0.2595  loss_box_reg: 0.1993  loss_mask: 0.3929  loss_rpn_cls: 0.05815  loss_rpn_loc: 0.1135  time: 0.5215  last_time: 0.4720  data_time: 0.0147  last_data_time: 0.0277   lr: 0.0001  max_mem: 11521M
[02/27 20:50:13 d2.utils.events]:  eta: 1 day, 1:22:25  iter: 9139  total_loss: 1.209  loss_cls: 0.3307  loss_box_reg: 0.1815  loss_mask: 0.378  loss_rpn_cls: 0.07145  loss_rpn_loc: 0.1511  time: 0.5215  last_time: 0.5683  data_time: 0.0167  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:50:24 d2.utils.events]:  eta: 1 day, 1:22:08  iter: 9159  total_loss: 1.219  loss_cls: 0.3402  loss_box_reg: 0.2275  loss_mask: 0.3916  loss_rpn_cls: 0.06253  loss_rpn_loc: 0.1192  time: 0.5215  last_time: 0.5439  data_time: 0.0213  last_data_time: 0.0874   lr: 0.0001  max_mem: 11521M
[02/27 20:50:34 d2.utils.events]:  eta: 1 day, 1:21:39  iter: 9179  total_loss: 1.356  loss_cls: 0.3679  loss_box_reg: 0.2429  loss_mask: 0.3593  loss_rpn_cls: 0.07908  loss_rpn_loc: 0.2599  time: 0.5215  last_time: 0.5236  data_time: 0.0235  last_data_time: 0.0111   lr: 0.0001  max_mem: 11521M
[02/27 20:50:44 d2.utils.events]:  eta: 1 day, 1:21:29  iter: 9199  total_loss: 1.034  loss_cls: 0.2681  loss_box_reg: 0.1578  loss_mask: 0.3597  loss_rpn_cls: 0.06187  loss_rpn_loc: 0.1453  time: 0.5215  last_time: 0.5293  data_time: 0.0179  last_data_time: 0.0086   lr: 0.0001  max_mem: 11521M
[02/27 20:50:55 d2.utils.events]:  eta: 1 day, 1:21:05  iter: 9219  total_loss: 1.128  loss_cls: 0.3477  loss_box_reg: 0.2389  loss_mask: 0.3699  loss_rpn_cls: 0.05845  loss_rpn_loc: 0.112  time: 0.5215  last_time: 0.5262  data_time: 0.0198  last_data_time: 0.0237   lr: 0.0001  max_mem: 11521M
[02/27 20:51:05 d2.utils.events]:  eta: 1 day, 1:19:28  iter: 9239  total_loss: 1.177  loss_cls: 0.3517  loss_box_reg: 0.2216  loss_mask: 0.4042  loss_rpn_cls: 0.06079  loss_rpn_loc: 0.1116  time: 0.5215  last_time: 0.5189  data_time: 0.0129  last_data_time: 0.0286   lr: 0.0001  max_mem: 11521M
[02/27 20:51:16 d2.utils.events]:  eta: 1 day, 1:19:28  iter: 9259  total_loss: 1.165  loss_cls: 0.3356  loss_box_reg: 0.248  loss_mask: 0.3747  loss_rpn_cls: 0.0694  loss_rpn_loc: 0.1072  time: 0.5215  last_time: 0.5017  data_time: 0.0231  last_data_time: 0.0137   lr: 0.0001  max_mem: 11521M
[02/27 20:51:27 d2.utils.events]:  eta: 1 day, 1:20:43  iter: 9279  total_loss: 1.261  loss_cls: 0.3954  loss_box_reg: 0.2541  loss_mask: 0.3801  loss_rpn_cls: 0.06787  loss_rpn_loc: 0.1298  time: 0.5215  last_time: 0.5450  data_time: 0.0128  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 20:51:37 d2.utils.events]:  eta: 1 day, 1:19:40  iter: 9299  total_loss: 1.278  loss_cls: 0.3842  loss_box_reg: 0.2339  loss_mask: 0.3934  loss_rpn_cls: 0.1077  loss_rpn_loc: 0.1654  time: 0.5215  last_time: 0.5192  data_time: 0.0120  last_data_time: 0.0025   lr: 0.0001  max_mem: 11521M
[02/27 20:51:48 d2.utils.events]:  eta: 1 day, 1:19:30  iter: 9319  total_loss: 1.219  loss_cls: 0.4219  loss_box_reg: 0.3067  loss_mask: 0.3727  loss_rpn_cls: 0.05889  loss_rpn_loc: 0.1519  time: 0.5215  last_time: 0.5633  data_time: 0.0246  last_data_time: 0.0502   lr: 0.0001  max_mem: 11521M
[02/27 20:51:59 d2.utils.events]:  eta: 1 day, 1:20:03  iter: 9339  total_loss: 1.214  loss_cls: 0.3681  loss_box_reg: 0.2459  loss_mask: 0.3802  loss_rpn_cls: 0.08704  loss_rpn_loc: 0.1581  time: 0.5216  last_time: 0.5299  data_time: 0.0242  last_data_time: 0.0401   lr: 0.0001  max_mem: 11521M
[02/27 20:52:09 d2.utils.events]:  eta: 1 day, 1:18:36  iter: 9359  total_loss: 1.217  loss_cls: 0.3457  loss_box_reg: 0.2321  loss_mask: 0.3808  loss_rpn_cls: 0.07268  loss_rpn_loc: 0.1267  time: 0.5215  last_time: 0.5195  data_time: 0.0211  last_data_time: 0.0402   lr: 0.0001  max_mem: 11521M
[02/27 20:52:19 d2.utils.events]:  eta: 1 day, 1:16:34  iter: 9379  total_loss: 1.151  loss_cls: 0.3501  loss_box_reg: 0.2301  loss_mask: 0.3863  loss_rpn_cls: 0.06917  loss_rpn_loc: 0.1107  time: 0.5215  last_time: 0.5402  data_time: 0.0219  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 20:52:30 d2.utils.events]:  eta: 1 day, 1:18:24  iter: 9399  total_loss: 1.041  loss_cls: 0.3034  loss_box_reg: 0.2043  loss_mask: 0.366  loss_rpn_cls: 0.05315  loss_rpn_loc: 0.09268  time: 0.5215  last_time: 0.5088  data_time: 0.0194  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 20:52:40 d2.utils.events]:  eta: 1 day, 1:16:54  iter: 9419  total_loss: 1.257  loss_cls: 0.3472  loss_box_reg: 0.2391  loss_mask: 0.4175  loss_rpn_cls: 0.0594  loss_rpn_loc: 0.1238  time: 0.5215  last_time: 0.5110  data_time: 0.0202  last_data_time: 0.0340   lr: 0.0001  max_mem: 11521M
[02/27 20:52:51 d2.utils.events]:  eta: 1 day, 1:16:17  iter: 9439  total_loss: 1.231  loss_cls: 0.3283  loss_box_reg: 0.2376  loss_mask: 0.3855  loss_rpn_cls: 0.08352  loss_rpn_loc: 0.1557  time: 0.5215  last_time: 0.5069  data_time: 0.0248  last_data_time: 0.0267   lr: 0.0001  max_mem: 11521M
[02/27 20:53:01 d2.utils.events]:  eta: 1 day, 1:16:56  iter: 9459  total_loss: 1.164  loss_cls: 0.3521  loss_box_reg: 0.206  loss_mask: 0.3654  loss_rpn_cls: 0.07207  loss_rpn_loc: 0.1484  time: 0.5215  last_time: 0.5193  data_time: 0.0149  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 20:53:11 d2.utils.events]:  eta: 1 day, 1:15:57  iter: 9479  total_loss: 1.079  loss_cls: 0.3283  loss_box_reg: 0.2533  loss_mask: 0.3647  loss_rpn_cls: 0.06803  loss_rpn_loc: 0.1164  time: 0.5215  last_time: 0.5083  data_time: 0.0170  last_data_time: 0.0135   lr: 0.0001  max_mem: 11521M
[02/27 20:53:22 d2.utils.events]:  eta: 1 day, 1:15:19  iter: 9499  total_loss: 1.323  loss_cls: 0.4181  loss_box_reg: 0.3023  loss_mask: 0.3821  loss_rpn_cls: 0.07917  loss_rpn_loc: 0.1504  time: 0.5215  last_time: 0.5469  data_time: 0.0148  last_data_time: 0.0402   lr: 0.0001  max_mem: 11521M
[02/27 20:53:33 d2.utils.events]:  eta: 1 day, 1:14:48  iter: 9519  total_loss: 1.065  loss_cls: 0.3206  loss_box_reg: 0.2234  loss_mask: 0.415  loss_rpn_cls: 0.05593  loss_rpn_loc: 0.07946  time: 0.5215  last_time: 0.5187  data_time: 0.0189  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 20:53:43 d2.utils.events]:  eta: 1 day, 1:14:37  iter: 9539  total_loss: 1.063  loss_cls: 0.2765  loss_box_reg: 0.2235  loss_mask: 0.3688  loss_rpn_cls: 0.06279  loss_rpn_loc: 0.1167  time: 0.5215  last_time: 0.5002  data_time: 0.0193  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 20:53:53 d2.utils.events]:  eta: 1 day, 1:13:44  iter: 9559  total_loss: 1.163  loss_cls: 0.3636  loss_box_reg: 0.2539  loss_mask: 0.3818  loss_rpn_cls: 0.05717  loss_rpn_loc: 0.08959  time: 0.5215  last_time: 0.5400  data_time: 0.0188  last_data_time: 0.0102   lr: 0.0001  max_mem: 11521M
[02/27 20:54:04 d2.utils.events]:  eta: 1 day, 1:13:11  iter: 9579  total_loss: 1.233  loss_cls: 0.4003  loss_box_reg: 0.2487  loss_mask: 0.354  loss_rpn_cls: 0.06531  loss_rpn_loc: 0.1441  time: 0.5215  last_time: 0.5341  data_time: 0.0254  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 20:54:14 d2.utils.events]:  eta: 1 day, 1:11:03  iter: 9599  total_loss: 1.305  loss_cls: 0.3498  loss_box_reg: 0.2197  loss_mask: 0.38  loss_rpn_cls: 0.06553  loss_rpn_loc: 0.1194  time: 0.5214  last_time: 0.5313  data_time: 0.0078  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 20:54:25 d2.utils.events]:  eta: 1 day, 1:11:08  iter: 9619  total_loss: 1.362  loss_cls: 0.3534  loss_box_reg: 0.2522  loss_mask: 0.3886  loss_rpn_cls: 0.09168  loss_rpn_loc: 0.1842  time: 0.5214  last_time: 0.5143  data_time: 0.0290  last_data_time: 0.0126   lr: 0.0001  max_mem: 11521M
[02/27 20:54:35 d2.utils.events]:  eta: 1 day, 1:11:19  iter: 9639  total_loss: 1.225  loss_cls: 0.3485  loss_box_reg: 0.2567  loss_mask: 0.3845  loss_rpn_cls: 0.06926  loss_rpn_loc: 0.1232  time: 0.5214  last_time: 0.5324  data_time: 0.0170  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 20:54:46 d2.utils.events]:  eta: 1 day, 1:11:47  iter: 9659  total_loss: 1.168  loss_cls: 0.3637  loss_box_reg: 0.2235  loss_mask: 0.3649  loss_rpn_cls: 0.07062  loss_rpn_loc: 0.07145  time: 0.5214  last_time: 0.5150  data_time: 0.0170  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 20:54:56 d2.utils.events]:  eta: 1 day, 1:10:36  iter: 9679  total_loss: 1.125  loss_cls: 0.3487  loss_box_reg: 0.2379  loss_mask: 0.3896  loss_rpn_cls: 0.05975  loss_rpn_loc: 0.08603  time: 0.5214  last_time: 0.5061  data_time: 0.0156  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 20:55:07 d2.utils.events]:  eta: 1 day, 1:10:34  iter: 9699  total_loss: 1.369  loss_cls: 0.3941  loss_box_reg: 0.2692  loss_mask: 0.3641  loss_rpn_cls: 0.07188  loss_rpn_loc: 0.155  time: 0.5214  last_time: 0.5083  data_time: 0.0175  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 20:55:17 d2.utils.events]:  eta: 1 day, 1:11:35  iter: 9719  total_loss: 1.143  loss_cls: 0.362  loss_box_reg: 0.2288  loss_mask: 0.3895  loss_rpn_cls: 0.05419  loss_rpn_loc: 0.08376  time: 0.5214  last_time: 0.5178  data_time: 0.0138  last_data_time: 0.0074   lr: 0.0001  max_mem: 11521M
[02/27 20:55:28 d2.utils.events]:  eta: 1 day, 1:11:25  iter: 9739  total_loss: 1.255  loss_cls: 0.3536  loss_box_reg: 0.254  loss_mask: 0.3419  loss_rpn_cls: 0.06906  loss_rpn_loc: 0.1128  time: 0.5214  last_time: 0.4939  data_time: 0.0153  last_data_time: 0.0213   lr: 0.0001  max_mem: 11521M
[02/27 20:55:38 d2.utils.events]:  eta: 1 day, 1:10:36  iter: 9759  total_loss: 1.087  loss_cls: 0.3597  loss_box_reg: 0.2557  loss_mask: 0.3732  loss_rpn_cls: 0.05002  loss_rpn_loc: 0.07482  time: 0.5214  last_time: 0.5528  data_time: 0.0160  last_data_time: 0.0448   lr: 0.0001  max_mem: 11521M
[02/27 20:55:49 d2.utils.events]:  eta: 1 day, 1:11:08  iter: 9779  total_loss: 1.372  loss_cls: 0.4031  loss_box_reg: 0.2924  loss_mask: 0.3836  loss_rpn_cls: 0.08351  loss_rpn_loc: 0.1586  time: 0.5214  last_time: 0.5576  data_time: 0.0165  last_data_time: 0.0635   lr: 0.0001  max_mem: 11521M
[02/27 20:56:00 d2.utils.events]:  eta: 1 day, 1:12:55  iter: 9799  total_loss: 1.151  loss_cls: 0.3144  loss_box_reg: 0.2111  loss_mask: 0.4002  loss_rpn_cls: 0.0793  loss_rpn_loc: 0.1246  time: 0.5214  last_time: 0.5515  data_time: 0.0122  last_data_time: 0.0060   lr: 0.0001  max_mem: 11521M
[02/27 20:56:10 d2.utils.events]:  eta: 1 day, 1:13:29  iter: 9819  total_loss: 1.052  loss_cls: 0.3375  loss_box_reg: 0.2129  loss_mask: 0.3484  loss_rpn_cls: 0.05926  loss_rpn_loc: 0.07702  time: 0.5214  last_time: 0.5229  data_time: 0.0252  last_data_time: 0.0390   lr: 0.0001  max_mem: 11521M
[02/27 20:56:21 d2.utils.events]:  eta: 1 day, 1:13:38  iter: 9839  total_loss: 1.301  loss_cls: 0.3909  loss_box_reg: 0.2575  loss_mask: 0.3758  loss_rpn_cls: 0.06399  loss_rpn_loc: 0.08268  time: 0.5215  last_time: 0.5131  data_time: 0.0116  last_data_time: 0.0206   lr: 0.0001  max_mem: 11521M
[02/27 20:56:31 d2.utils.events]:  eta: 1 day, 1:13:08  iter: 9859  total_loss: 1.255  loss_cls: 0.3566  loss_box_reg: 0.2679  loss_mask: 0.4083  loss_rpn_cls: 0.07169  loss_rpn_loc: 0.1374  time: 0.5214  last_time: 0.5298  data_time: 0.0169  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 20:56:42 d2.utils.events]:  eta: 1 day, 1:13:05  iter: 9879  total_loss: 1.236  loss_cls: 0.3687  loss_box_reg: 0.2299  loss_mask: 0.3739  loss_rpn_cls: 0.05388  loss_rpn_loc: 0.109  time: 0.5215  last_time: 0.4868  data_time: 0.0084  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 20:56:53 d2.utils.events]:  eta: 1 day, 1:13:42  iter: 9899  total_loss: 1.221  loss_cls: 0.3544  loss_box_reg: 0.2613  loss_mask: 0.3916  loss_rpn_cls: 0.06884  loss_rpn_loc: 0.1385  time: 0.5215  last_time: 0.4664  data_time: 0.0242  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 20:57:03 d2.utils.events]:  eta: 1 day, 1:15:10  iter: 9919  total_loss: 1.212  loss_cls: 0.3689  loss_box_reg: 0.2536  loss_mask: 0.3721  loss_rpn_cls: 0.06284  loss_rpn_loc: 0.1264  time: 0.5215  last_time: 0.5540  data_time: 0.0165  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 20:57:14 d2.utils.events]:  eta: 1 day, 1:15:00  iter: 9939  total_loss: 1.252  loss_cls: 0.3674  loss_box_reg: 0.2532  loss_mask: 0.427  loss_rpn_cls: 0.08425  loss_rpn_loc: 0.1738  time: 0.5215  last_time: 0.5574  data_time: 0.0195  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 20:57:24 d2.utils.events]:  eta: 1 day, 1:13:37  iter: 9959  total_loss: 1.252  loss_cls: 0.3504  loss_box_reg: 0.2411  loss_mask: 0.3972  loss_rpn_cls: 0.05932  loss_rpn_loc: 0.1522  time: 0.5215  last_time: 0.5433  data_time: 0.0223  last_data_time: 0.0602   lr: 0.0001  max_mem: 11521M
[02/27 20:57:35 d2.utils.events]:  eta: 1 day, 1:13:51  iter: 9979  total_loss: 1.162  loss_cls: 0.3935  loss_box_reg: 0.276  loss_mask: 0.3467  loss_rpn_cls: 0.05277  loss_rpn_loc: 0.08848  time: 0.5215  last_time: 0.5415  data_time: 0.0141  last_data_time: 0.0353   lr: 0.0001  max_mem: 11521M
[02/27 20:57:45 fvcore.common.checkpoint]: Saving checkpoint to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/model_0009999.pth
[02/27 20:57:47 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_val2017.json
[02/27 20:57:48 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]
[02/27 20:57:48 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/27 20:57:48 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...
[02/27 20:57:48 d2.data.common]: Serialized dataset takes 19.19 MiB
[02/27 20:57:48 d2.evaluation.evaluator]: Start inference on 1250 batches
[02/27 20:57:55 d2.evaluation.evaluator]: Inference done 11/1250. Dataloading: 0.0010 s/iter. Inference: 0.3170 s/iter. Eval: 0.0377 s/iter. Total: 0.3557 s/iter. ETA=0:07:20
[02/27 20:58:00 d2.evaluation.evaluator]: Inference done 26/1250. Dataloading: 0.0013 s/iter. Inference: 0.3139 s/iter. Eval: 0.0338 s/iter. Total: 0.3491 s/iter. ETA=0:07:07
[02/27 20:58:05 d2.evaluation.evaluator]: Inference done 42/1250. Dataloading: 0.0016 s/iter. Inference: 0.3053 s/iter. Eval: 0.0281 s/iter. Total: 0.3351 s/iter. ETA=0:06:44
[02/27 20:58:10 d2.evaluation.evaluator]: Inference done 58/1250. Dataloading: 0.0018 s/iter. Inference: 0.3025 s/iter. Eval: 0.0306 s/iter. Total: 0.3350 s/iter. ETA=0:06:39
[02/27 20:58:16 d2.evaluation.evaluator]: Inference done 74/1250. Dataloading: 0.0017 s/iter. Inference: 0.3028 s/iter. Eval: 0.0285 s/iter. Total: 0.3330 s/iter. ETA=0:06:31
[02/27 20:58:21 d2.evaluation.evaluator]: Inference done 89/1250. Dataloading: 0.0016 s/iter. Inference: 0.3023 s/iter. Eval: 0.0298 s/iter. Total: 0.3338 s/iter. ETA=0:06:27
[02/27 20:58:26 d2.evaluation.evaluator]: Inference done 104/1250. Dataloading: 0.0016 s/iter. Inference: 0.3039 s/iter. Eval: 0.0300 s/iter. Total: 0.3355 s/iter. ETA=0:06:24
[02/27 20:58:31 d2.evaluation.evaluator]: Inference done 120/1250. Dataloading: 0.0017 s/iter. Inference: 0.3023 s/iter. Eval: 0.0307 s/iter. Total: 0.3348 s/iter. ETA=0:06:18
[02/27 20:58:36 d2.evaluation.evaluator]: Inference done 136/1250. Dataloading: 0.0017 s/iter. Inference: 0.3008 s/iter. Eval: 0.0317 s/iter. Total: 0.3343 s/iter. ETA=0:06:12
[02/27 20:58:41 d2.evaluation.evaluator]: Inference done 151/1250. Dataloading: 0.0018 s/iter. Inference: 0.3008 s/iter. Eval: 0.0320 s/iter. Total: 0.3347 s/iter. ETA=0:06:07
[02/27 20:58:47 d2.evaluation.evaluator]: Inference done 166/1250. Dataloading: 0.0019 s/iter. Inference: 0.3003 s/iter. Eval: 0.0330 s/iter. Total: 0.3352 s/iter. ETA=0:06:03
[02/27 20:58:52 d2.evaluation.evaluator]: Inference done 182/1250. Dataloading: 0.0019 s/iter. Inference: 0.2985 s/iter. Eval: 0.0332 s/iter. Total: 0.3336 s/iter. ETA=0:05:56
[02/27 20:58:57 d2.evaluation.evaluator]: Inference done 198/1250. Dataloading: 0.0019 s/iter. Inference: 0.2975 s/iter. Eval: 0.0332 s/iter. Total: 0.3327 s/iter. ETA=0:05:49
[02/27 20:59:02 d2.evaluation.evaluator]: Inference done 215/1250. Dataloading: 0.0018 s/iter. Inference: 0.2968 s/iter. Eval: 0.0326 s/iter. Total: 0.3313 s/iter. ETA=0:05:42
[02/27 20:59:07 d2.evaluation.evaluator]: Inference done 231/1250. Dataloading: 0.0018 s/iter. Inference: 0.2975 s/iter. Eval: 0.0315 s/iter. Total: 0.3309 s/iter. ETA=0:05:37
[02/27 20:59:12 d2.evaluation.evaluator]: Inference done 247/1250. Dataloading: 0.0018 s/iter. Inference: 0.2967 s/iter. Eval: 0.0313 s/iter. Total: 0.3298 s/iter. ETA=0:05:30
[02/27 20:59:18 d2.evaluation.evaluator]: Inference done 262/1250. Dataloading: 0.0019 s/iter. Inference: 0.2966 s/iter. Eval: 0.0319 s/iter. Total: 0.3305 s/iter. ETA=0:05:26
[02/27 20:59:23 d2.evaluation.evaluator]: Inference done 277/1250. Dataloading: 0.0019 s/iter. Inference: 0.2978 s/iter. Eval: 0.0320 s/iter. Total: 0.3317 s/iter. ETA=0:05:22
[02/27 20:59:28 d2.evaluation.evaluator]: Inference done 291/1250. Dataloading: 0.0019 s/iter. Inference: 0.2994 s/iter. Eval: 0.0320 s/iter. Total: 0.3334 s/iter. ETA=0:05:19
[02/27 20:59:33 d2.evaluation.evaluator]: Inference done 306/1250. Dataloading: 0.0019 s/iter. Inference: 0.3006 s/iter. Eval: 0.0317 s/iter. Total: 0.3342 s/iter. ETA=0:05:15
[02/27 20:59:38 d2.evaluation.evaluator]: Inference done 320/1250. Dataloading: 0.0019 s/iter. Inference: 0.3007 s/iter. Eval: 0.0326 s/iter. Total: 0.3353 s/iter. ETA=0:05:11
[02/27 20:59:43 d2.evaluation.evaluator]: Inference done 333/1250. Dataloading: 0.0019 s/iter. Inference: 0.3027 s/iter. Eval: 0.0329 s/iter. Total: 0.3375 s/iter. ETA=0:05:09
[02/27 20:59:49 d2.evaluation.evaluator]: Inference done 349/1250. Dataloading: 0.0019 s/iter. Inference: 0.3022 s/iter. Eval: 0.0330 s/iter. Total: 0.3372 s/iter. ETA=0:05:03
[02/27 20:59:54 d2.evaluation.evaluator]: Inference done 365/1250. Dataloading: 0.0019 s/iter. Inference: 0.3016 s/iter. Eval: 0.0332 s/iter. Total: 0.3368 s/iter. ETA=0:04:58
[02/27 20:59:59 d2.evaluation.evaluator]: Inference done 381/1250. Dataloading: 0.0019 s/iter. Inference: 0.3013 s/iter. Eval: 0.0334 s/iter. Total: 0.3367 s/iter. ETA=0:04:52
[02/27 21:00:04 d2.evaluation.evaluator]: Inference done 397/1250. Dataloading: 0.0020 s/iter. Inference: 0.3005 s/iter. Eval: 0.0334 s/iter. Total: 0.3359 s/iter. ETA=0:04:46
[02/27 21:00:09 d2.evaluation.evaluator]: Inference done 412/1250. Dataloading: 0.0020 s/iter. Inference: 0.3010 s/iter. Eval: 0.0330 s/iter. Total: 0.3360 s/iter. ETA=0:04:41
[02/27 21:00:15 d2.evaluation.evaluator]: Inference done 427/1250. Dataloading: 0.0020 s/iter. Inference: 0.3015 s/iter. Eval: 0.0330 s/iter. Total: 0.3365 s/iter. ETA=0:04:36
[02/27 21:00:20 d2.evaluation.evaluator]: Inference done 443/1250. Dataloading: 0.0020 s/iter. Inference: 0.3008 s/iter. Eval: 0.0332 s/iter. Total: 0.3360 s/iter. ETA=0:04:31
[02/27 21:00:25 d2.evaluation.evaluator]: Inference done 460/1250. Dataloading: 0.0020 s/iter. Inference: 0.3002 s/iter. Eval: 0.0327 s/iter. Total: 0.3350 s/iter. ETA=0:04:24
[02/27 21:00:30 d2.evaluation.evaluator]: Inference done 475/1250. Dataloading: 0.0020 s/iter. Inference: 0.3009 s/iter. Eval: 0.0328 s/iter. Total: 0.3358 s/iter. ETA=0:04:20
[02/27 21:00:36 d2.evaluation.evaluator]: Inference done 490/1250. Dataloading: 0.0021 s/iter. Inference: 0.3013 s/iter. Eval: 0.0330 s/iter. Total: 0.3364 s/iter. ETA=0:04:15
[02/27 21:00:41 d2.evaluation.evaluator]: Inference done 504/1250. Dataloading: 0.0021 s/iter. Inference: 0.3022 s/iter. Eval: 0.0329 s/iter. Total: 0.3372 s/iter. ETA=0:04:11
[02/27 21:00:46 d2.evaluation.evaluator]: Inference done 518/1250. Dataloading: 0.0021 s/iter. Inference: 0.3029 s/iter. Eval: 0.0328 s/iter. Total: 0.3378 s/iter. ETA=0:04:07
[02/27 21:00:51 d2.evaluation.evaluator]: Inference done 532/1250. Dataloading: 0.0021 s/iter. Inference: 0.3031 s/iter. Eval: 0.0331 s/iter. Total: 0.3384 s/iter. ETA=0:04:02
[02/27 21:00:56 d2.evaluation.evaluator]: Inference done 548/1250. Dataloading: 0.0021 s/iter. Inference: 0.3021 s/iter. Eval: 0.0333 s/iter. Total: 0.3377 s/iter. ETA=0:03:57
[02/27 21:01:01 d2.evaluation.evaluator]: Inference done 563/1250. Dataloading: 0.0021 s/iter. Inference: 0.3022 s/iter. Eval: 0.0336 s/iter. Total: 0.3380 s/iter. ETA=0:03:52
[02/27 21:01:06 d2.evaluation.evaluator]: Inference done 579/1250. Dataloading: 0.0022 s/iter. Inference: 0.3018 s/iter. Eval: 0.0336 s/iter. Total: 0.3377 s/iter. ETA=0:03:46
[02/27 21:01:11 d2.evaluation.evaluator]: Inference done 594/1250. Dataloading: 0.0022 s/iter. Inference: 0.3015 s/iter. Eval: 0.0338 s/iter. Total: 0.3375 s/iter. ETA=0:03:41
[02/27 21:01:17 d2.evaluation.evaluator]: Inference done 609/1250. Dataloading: 0.0022 s/iter. Inference: 0.3018 s/iter. Eval: 0.0336 s/iter. Total: 0.3377 s/iter. ETA=0:03:36
[02/27 21:01:22 d2.evaluation.evaluator]: Inference done 625/1250. Dataloading: 0.0022 s/iter. Inference: 0.3015 s/iter. Eval: 0.0335 s/iter. Total: 0.3373 s/iter. ETA=0:03:30
[02/27 21:01:27 d2.evaluation.evaluator]: Inference done 640/1250. Dataloading: 0.0022 s/iter. Inference: 0.3013 s/iter. Eval: 0.0338 s/iter. Total: 0.3374 s/iter. ETA=0:03:25
[02/27 21:01:32 d2.evaluation.evaluator]: Inference done 655/1250. Dataloading: 0.0023 s/iter. Inference: 0.3015 s/iter. Eval: 0.0339 s/iter. Total: 0.3377 s/iter. ETA=0:03:20
[02/27 21:01:37 d2.evaluation.evaluator]: Inference done 669/1250. Dataloading: 0.0023 s/iter. Inference: 0.3019 s/iter. Eval: 0.0339 s/iter. Total: 0.3382 s/iter. ETA=0:03:16
[02/27 21:01:42 d2.evaluation.evaluator]: Inference done 683/1250. Dataloading: 0.0023 s/iter. Inference: 0.3024 s/iter. Eval: 0.0338 s/iter. Total: 0.3386 s/iter. ETA=0:03:11
[02/27 21:01:47 d2.evaluation.evaluator]: Inference done 698/1250. Dataloading: 0.0023 s/iter. Inference: 0.3026 s/iter. Eval: 0.0337 s/iter. Total: 0.3386 s/iter. ETA=0:03:06
[02/27 21:01:53 d2.evaluation.evaluator]: Inference done 714/1250. Dataloading: 0.0022 s/iter. Inference: 0.3024 s/iter. Eval: 0.0337 s/iter. Total: 0.3384 s/iter. ETA=0:03:01
[02/27 21:01:58 d2.evaluation.evaluator]: Inference done 729/1250. Dataloading: 0.0023 s/iter. Inference: 0.3025 s/iter. Eval: 0.0337 s/iter. Total: 0.3385 s/iter. ETA=0:02:56
[02/27 21:02:03 d2.evaluation.evaluator]: Inference done 744/1250. Dataloading: 0.0022 s/iter. Inference: 0.3030 s/iter. Eval: 0.0336 s/iter. Total: 0.3389 s/iter. ETA=0:02:51
[02/27 21:02:08 d2.evaluation.evaluator]: Inference done 759/1250. Dataloading: 0.0023 s/iter. Inference: 0.3025 s/iter. Eval: 0.0341 s/iter. Total: 0.3390 s/iter. ETA=0:02:46
[02/27 21:02:13 d2.evaluation.evaluator]: Inference done 774/1250. Dataloading: 0.0023 s/iter. Inference: 0.3025 s/iter. Eval: 0.0342 s/iter. Total: 0.3391 s/iter. ETA=0:02:41
[02/27 21:02:19 d2.evaluation.evaluator]: Inference done 790/1250. Dataloading: 0.0023 s/iter. Inference: 0.3024 s/iter. Eval: 0.0342 s/iter. Total: 0.3390 s/iter. ETA=0:02:35
[02/27 21:02:24 d2.evaluation.evaluator]: Inference done 805/1250. Dataloading: 0.0024 s/iter. Inference: 0.3022 s/iter. Eval: 0.0345 s/iter. Total: 0.3391 s/iter. ETA=0:02:30
[02/27 21:02:29 d2.evaluation.evaluator]: Inference done 822/1250. Dataloading: 0.0024 s/iter. Inference: 0.3018 s/iter. Eval: 0.0342 s/iter. Total: 0.3384 s/iter. ETA=0:02:24
[02/27 21:02:34 d2.evaluation.evaluator]: Inference done 837/1250. Dataloading: 0.0024 s/iter. Inference: 0.3017 s/iter. Eval: 0.0342 s/iter. Total: 0.3384 s/iter. ETA=0:02:19
[02/27 21:02:39 d2.evaluation.evaluator]: Inference done 852/1250. Dataloading: 0.0024 s/iter. Inference: 0.3018 s/iter. Eval: 0.0341 s/iter. Total: 0.3383 s/iter. ETA=0:02:14
[02/27 21:02:44 d2.evaluation.evaluator]: Inference done 867/1250. Dataloading: 0.0024 s/iter. Inference: 0.3018 s/iter. Eval: 0.0341 s/iter. Total: 0.3385 s/iter. ETA=0:02:09
[02/27 21:02:50 d2.evaluation.evaluator]: Inference done 882/1250. Dataloading: 0.0024 s/iter. Inference: 0.3020 s/iter. Eval: 0.0341 s/iter. Total: 0.3386 s/iter. ETA=0:02:04
[02/27 21:02:55 d2.evaluation.evaluator]: Inference done 897/1250. Dataloading: 0.0024 s/iter. Inference: 0.3020 s/iter. Eval: 0.0341 s/iter. Total: 0.3387 s/iter. ETA=0:01:59
[02/27 21:03:00 d2.evaluation.evaluator]: Inference done 914/1250. Dataloading: 0.0024 s/iter. Inference: 0.3015 s/iter. Eval: 0.0340 s/iter. Total: 0.3380 s/iter. ETA=0:01:53
[02/27 21:03:05 d2.evaluation.evaluator]: Inference done 929/1250. Dataloading: 0.0024 s/iter. Inference: 0.3015 s/iter. Eval: 0.0339 s/iter. Total: 0.3379 s/iter. ETA=0:01:48
[02/27 21:03:10 d2.evaluation.evaluator]: Inference done 945/1250. Dataloading: 0.0024 s/iter. Inference: 0.3012 s/iter. Eval: 0.0338 s/iter. Total: 0.3376 s/iter. ETA=0:01:42
[02/27 21:03:15 d2.evaluation.evaluator]: Inference done 960/1250. Dataloading: 0.0024 s/iter. Inference: 0.3013 s/iter. Eval: 0.0339 s/iter. Total: 0.3378 s/iter. ETA=0:01:37
[02/27 21:03:20 d2.evaluation.evaluator]: Inference done 975/1250. Dataloading: 0.0024 s/iter. Inference: 0.3014 s/iter. Eval: 0.0340 s/iter. Total: 0.3380 s/iter. ETA=0:01:32
[02/27 21:03:26 d2.evaluation.evaluator]: Inference done 990/1250. Dataloading: 0.0024 s/iter. Inference: 0.3016 s/iter. Eval: 0.0339 s/iter. Total: 0.3381 s/iter. ETA=0:01:27
[02/27 21:03:31 d2.evaluation.evaluator]: Inference done 1006/1250. Dataloading: 0.0024 s/iter. Inference: 0.3014 s/iter. Eval: 0.0339 s/iter. Total: 0.3379 s/iter. ETA=0:01:22
[02/27 21:03:36 d2.evaluation.evaluator]: Inference done 1020/1250. Dataloading: 0.0024 s/iter. Inference: 0.3017 s/iter. Eval: 0.0341 s/iter. Total: 0.3384 s/iter. ETA=0:01:17
[02/27 21:03:41 d2.evaluation.evaluator]: Inference done 1035/1250. Dataloading: 0.0024 s/iter. Inference: 0.3016 s/iter. Eval: 0.0341 s/iter. Total: 0.3383 s/iter. ETA=0:01:12
[02/27 21:03:46 d2.evaluation.evaluator]: Inference done 1050/1250. Dataloading: 0.0024 s/iter. Inference: 0.3017 s/iter. Eval: 0.0341 s/iter. Total: 0.3384 s/iter. ETA=0:01:07
[02/27 21:03:51 d2.evaluation.evaluator]: Inference done 1066/1250. Dataloading: 0.0024 s/iter. Inference: 0.3014 s/iter. Eval: 0.0342 s/iter. Total: 0.3382 s/iter. ETA=0:01:02
[02/27 21:03:56 d2.evaluation.evaluator]: Inference done 1081/1250. Dataloading: 0.0024 s/iter. Inference: 0.3013 s/iter. Eval: 0.0342 s/iter. Total: 0.3382 s/iter. ETA=0:00:57
[02/27 21:04:02 d2.evaluation.evaluator]: Inference done 1096/1250. Dataloading: 0.0024 s/iter. Inference: 0.3012 s/iter. Eval: 0.0344 s/iter. Total: 0.3382 s/iter. ETA=0:00:52
[02/27 21:04:07 d2.evaluation.evaluator]: Inference done 1111/1250. Dataloading: 0.0024 s/iter. Inference: 0.3012 s/iter. Eval: 0.0345 s/iter. Total: 0.3383 s/iter. ETA=0:00:47
[02/27 21:04:12 d2.evaluation.evaluator]: Inference done 1126/1250. Dataloading: 0.0024 s/iter. Inference: 0.3013 s/iter. Eval: 0.0344 s/iter. Total: 0.3383 s/iter. ETA=0:00:41
[02/27 21:04:17 d2.evaluation.evaluator]: Inference done 1142/1250. Dataloading: 0.0024 s/iter. Inference: 0.3010 s/iter. Eval: 0.0346 s/iter. Total: 0.3382 s/iter. ETA=0:00:36
[02/27 21:04:22 d2.evaluation.evaluator]: Inference done 1158/1250. Dataloading: 0.0024 s/iter. Inference: 0.3007 s/iter. Eval: 0.0346 s/iter. Total: 0.3379 s/iter. ETA=0:00:31
[02/27 21:04:27 d2.evaluation.evaluator]: Inference done 1172/1250. Dataloading: 0.0024 s/iter. Inference: 0.3012 s/iter. Eval: 0.0345 s/iter. Total: 0.3383 s/iter. ETA=0:00:26
[02/27 21:04:33 d2.evaluation.evaluator]: Inference done 1186/1250. Dataloading: 0.0024 s/iter. Inference: 0.3015 s/iter. Eval: 0.0346 s/iter. Total: 0.3387 s/iter. ETA=0:00:21
[02/27 21:04:38 d2.evaluation.evaluator]: Inference done 1202/1250. Dataloading: 0.0024 s/iter. Inference: 0.3011 s/iter. Eval: 0.0349 s/iter. Total: 0.3386 s/iter. ETA=0:00:16
[02/27 21:04:43 d2.evaluation.evaluator]: Inference done 1217/1250. Dataloading: 0.0024 s/iter. Inference: 0.3009 s/iter. Eval: 0.0350 s/iter. Total: 0.3385 s/iter. ETA=0:00:11
[02/27 21:04:48 d2.evaluation.evaluator]: Inference done 1232/1250. Dataloading: 0.0024 s/iter. Inference: 0.3009 s/iter. Eval: 0.0350 s/iter. Total: 0.3385 s/iter. ETA=0:00:06
[02/27 21:04:53 d2.evaluation.evaluator]: Inference done 1250/1250. Dataloading: 0.0024 s/iter. Inference: 0.3004 s/iter. Eval: 0.0349 s/iter. Total: 0.3379 s/iter. ETA=0:00:00
[02/27 21:04:54 d2.evaluation.evaluator]: Total inference time: 0:07:01.095629 (0.338229 s / iter per device, on 4 devices)
[02/27 21:04:54 d2.evaluation.evaluator]: Total inference pure compute time: 0:06:13 (0.300386 s / iter per device, on 4 devices)
[02/27 21:05:11 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[02/27 21:05:11 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
[02/27 21:05:11 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[02/27 21:05:17 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 6.17 seconds.
[02/27 21:05:17 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/27 21:05:18 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.72 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.061
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.050
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.061
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.088
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.106
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.166
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.264
[02/27 21:05:18 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 6.113 | 12.744 | 4.981  | 3.120 | 6.067 | 8.825 |
[02/27 21:05:18 d2.evaluation.coco_evaluation]: Per-category bbox AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 24.700 | bicycle      | 1.816  | car            | 11.061 |
| motorcycle    | 9.954  | airplane     | 16.288 | bus            | 15.273 |
| train         | 12.905 | truck        | 3.787  | boat           | 1.863  |
| traffic light | 4.536  | fire hydrant | 16.873 | stop sign      | 32.140 |
| parking meter | 5.067  | bench        | 1.255  | bird           | 1.999  |
| cat           | 12.468 | dog          | 6.193  | horse          | 10.936 |
| sheep         | 7.833  | cow          | 11.496 | elephant       | 12.667 |
| bear          | 19.069 | zebra        | 20.804 | giraffe        | 22.446 |
| backpack      | 0.450  | umbrella     | 4.682  | handbag        | 0.047  |
| tie           | 2.840  | suitcase     | 0.896  | frisbee        | 8.188  |
| skis          | 0.871  | snowboard    | 0.350  | sports ball    | 14.201 |
| kite          | 9.008  | baseball bat | 0.101  | baseball glove | 1.687  |
| skateboard    | 2.545  | surfboard    | 0.818  | tennis racket  | 4.327  |
| bottle        | 6.226  | wine glass   | 0.493  | cup            | 6.207  |
| fork          | 0.132  | knife        | 0.000  | spoon          | 0.011  |
| bowl          | 7.754  | banana       | 0.399  | apple          | 0.510  |
| sandwich      | 2.944  | orange       | 6.341  | broccoli       | 1.807  |
| carrot        | 0.971  | hot dog      | 1.257  | pizza          | 13.409 |
| donut         | 1.895  | cake         | 2.472  | chair          | 1.421  |
| couch         | 4.820  | potted plant | 1.347  | bed            | 12.459 |
| dining table  | 7.383  | toilet       | 18.314 | tv             | 11.008 |
| laptop        | 4.724  | mouse        | 4.389  | remote         | 0.594  |
| keyboard      | 1.212  | cell phone   | 2.138  | microwave      | 1.840  |
| oven          | 3.178  | toaster      | 0.000  | sink           | 4.523  |
| refrigerator  | 2.546  | book         | 1.069  | clock          | 13.946 |
| vase          | 1.738  | scissors     | 0.000  | teddy bear     | 3.119  |
| hair drier    | 0.000  | toothbrush   | 0.000  |                |        |
Loading and preparing results...
DONE (t=1.58s)
creating index...
index created!
[02/27 21:05:24 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*
[02/27 21:05:33 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 8.31 seconds.
[02/27 21:05:33 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/27 21:05:34 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.72 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.119
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.061
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.105
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.113
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.178
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.058
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.171
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.280
[02/27 21:05:34 d2.evaluation.coco_evaluation]: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:-----:|:------:|:------:|:-----:|:-----:|:------:|
| 6.372 | 11.947 | 6.073  | 2.011 | 5.833 | 10.528 |
[02/27 21:05:34 d2.evaluation.coco_evaluation]: Per-category segm AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 22.097 | bicycle      | 0.966  | car            | 10.680 |
| motorcycle    | 6.703  | airplane     | 18.225 | bus            | 18.558 |
| train         | 17.326 | truck        | 4.491  | boat           | 1.853  |
| traffic light | 4.721  | fire hydrant | 22.001 | stop sign      | 36.647 |
| parking meter | 6.346  | bench        | 0.940  | bird           | 2.153  |
| cat           | 16.800 | dog          | 7.956  | horse          | 6.319  |
| sheep         | 6.482  | cow          | 11.509 | elephant       | 10.976 |
| bear          | 22.043 | zebra        | 17.336 | giraffe        | 16.933 |
| backpack      | 0.533  | umbrella     | 7.849  | handbag        | 0.114  |
| tie           | 3.057  | suitcase     | 0.955  | frisbee        | 8.891  |
| skis          | 0.000  | snowboard    | 0.123  | sports ball    | 16.417 |
| kite          | 7.293  | baseball bat | 0.179  | baseball glove | 2.187  |
| skateboard    | 0.689  | surfboard    | 0.844  | tennis racket  | 8.462  |
| bottle        | 7.106  | wine glass   | 0.572  | cup            | 6.844  |
| fork          | 0.000  | knife        | 0.000  | spoon          | 0.000  |
| bowl          | 7.486  | banana       | 0.269  | apple          | 0.588  |
| sandwich      | 3.433  | orange       | 7.139  | broccoli       | 2.043  |
| carrot        | 1.058  | hot dog      | 1.475  | pizza          | 13.059 |
| donut         | 2.250  | cake         | 2.670  | chair          | 0.862  |
| couch         | 3.129  | potted plant | 1.134  | bed            | 8.962  |
| dining table  | 3.547  | toilet       | 23.022 | tv             | 13.618 |
| laptop        | 6.595  | mouse        | 4.909  | remote         | 0.654  |
| keyboard      | 1.424  | cell phone   | 2.623  | microwave      | 2.767  |
| oven          | 3.383  | toaster      | 0.000  | sink           | 5.004  |
| refrigerator  | 3.157  | book         | 0.327  | clock          | 15.408 |
| vase          | 2.151  | scissors     | 0.000  | teddy bear     | 3.471  |
| hair drier    | 0.000  | toothbrush   | 0.000  |                |        |
[02/27 21:05:35 d2.evaluation.testing]: copypaste: Task: bbox
[02/27 21:05:35 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[02/27 21:05:35 d2.evaluation.testing]: copypaste: 6.1130,12.7443,4.9810,3.1202,6.0667,8.8251
[02/27 21:05:35 d2.evaluation.testing]: copypaste: Task: segm
[02/27 21:05:35 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[02/27 21:05:35 d2.evaluation.testing]: copypaste: 6.3724,11.9468,6.0733,2.0110,5.8329,10.5280
[02/27 21:05:35 d2.utils.events]:  eta: 1 day, 1:14:34  iter: 9999  total_loss: 1.221  loss_cls: 0.3365  loss_box_reg: 0.243  loss_mask: 0.3579  loss_rpn_cls: 0.0542  loss_rpn_loc: 0.1042  time: 0.5215  last_time: 0.5210  data_time: 0.0166  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 21:05:45 d2.utils.events]:  eta: 1 day, 1:14:24  iter: 10019  total_loss: 1.18  loss_cls: 0.3164  loss_box_reg: 0.2256  loss_mask: 0.3903  loss_rpn_cls: 0.06576  loss_rpn_loc: 0.1417  time: 0.5215  last_time: 0.5241  data_time: 0.0141  last_data_time: 0.0124   lr: 0.0001  max_mem: 11521M
[02/27 21:05:56 d2.utils.events]:  eta: 1 day, 1:13:10  iter: 10039  total_loss: 1.11  loss_cls: 0.2951  loss_box_reg: 0.2186  loss_mask: 0.391  loss_rpn_cls: 0.05975  loss_rpn_loc: 0.1047  time: 0.5215  last_time: 0.5027  data_time: 0.0170  last_data_time: 0.0052   lr: 0.0001  max_mem: 11521M
[02/27 21:06:06 d2.utils.events]:  eta: 1 day, 1:12:48  iter: 10059  total_loss: 1.371  loss_cls: 0.3364  loss_box_reg: 0.2475  loss_mask: 0.3811  loss_rpn_cls: 0.07599  loss_rpn_loc: 0.1728  time: 0.5214  last_time: 0.5467  data_time: 0.0171  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 21:06:16 d2.utils.events]:  eta: 1 day, 1:13:10  iter: 10079  total_loss: 1.231  loss_cls: 0.3457  loss_box_reg: 0.238  loss_mask: 0.4209  loss_rpn_cls: 0.0762  loss_rpn_loc: 0.2011  time: 0.5214  last_time: 0.5220  data_time: 0.0204  last_data_time: 0.0391   lr: 0.0001  max_mem: 11521M
[02/27 21:06:27 d2.utils.events]:  eta: 1 day, 1:12:21  iter: 10099  total_loss: 1.165  loss_cls: 0.3213  loss_box_reg: 0.2324  loss_mask: 0.3713  loss_rpn_cls: 0.06269  loss_rpn_loc: 0.1124  time: 0.5214  last_time: 0.4891  data_time: 0.0094  last_data_time: 0.0094   lr: 0.0001  max_mem: 11521M
[02/27 21:06:37 d2.utils.events]:  eta: 1 day, 1:13:49  iter: 10119  total_loss: 1.114  loss_cls: 0.2975  loss_box_reg: 0.2157  loss_mask: 0.3702  loss_rpn_cls: 0.05458  loss_rpn_loc: 0.178  time: 0.5214  last_time: 0.5153  data_time: 0.0337  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 21:06:48 d2.utils.events]:  eta: 1 day, 1:14:07  iter: 10139  total_loss: 1.201  loss_cls: 0.3974  loss_box_reg: 0.2611  loss_mask: 0.3618  loss_rpn_cls: 0.06448  loss_rpn_loc: 0.1197  time: 0.5214  last_time: 0.5250  data_time: 0.0229  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 21:06:58 d2.utils.events]:  eta: 1 day, 1:12:09  iter: 10159  total_loss: 1.164  loss_cls: 0.3274  loss_box_reg: 0.2352  loss_mask: 0.3723  loss_rpn_cls: 0.06818  loss_rpn_loc: 0.09092  time: 0.5214  last_time: 0.5324  data_time: 0.0208  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 21:07:09 d2.utils.events]:  eta: 1 day, 1:12:49  iter: 10179  total_loss: 1.15  loss_cls: 0.3311  loss_box_reg: 0.2293  loss_mask: 0.3988  loss_rpn_cls: 0.07219  loss_rpn_loc: 0.1279  time: 0.5214  last_time: 0.5737  data_time: 0.0229  last_data_time: 0.0805   lr: 0.0001  max_mem: 11521M
[02/27 21:07:19 d2.utils.events]:  eta: 1 day, 1:11:48  iter: 10199  total_loss: 1.482  loss_cls: 0.4236  loss_box_reg: 0.2958  loss_mask: 0.4139  loss_rpn_cls: 0.06303  loss_rpn_loc: 0.215  time: 0.5214  last_time: 0.5188  data_time: 0.0211  last_data_time: 0.0062   lr: 0.0001  max_mem: 11521M
[02/27 21:07:30 d2.utils.events]:  eta: 1 day, 1:11:17  iter: 10219  total_loss: 1.105  loss_cls: 0.3339  loss_box_reg: 0.2261  loss_mask: 0.3732  loss_rpn_cls: 0.06307  loss_rpn_loc: 0.08855  time: 0.5214  last_time: 0.5370  data_time: 0.0188  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 21:07:40 d2.utils.events]:  eta: 1 day, 1:11:47  iter: 10239  total_loss: 1.172  loss_cls: 0.345  loss_box_reg: 0.2438  loss_mask: 0.3952  loss_rpn_cls: 0.06813  loss_rpn_loc: 0.1252  time: 0.5214  last_time: 0.5292  data_time: 0.0161  last_data_time: 0.0024   lr: 0.0001  max_mem: 11521M
[02/27 21:07:51 d2.utils.events]:  eta: 1 day, 1:11:04  iter: 10259  total_loss: 1.07  loss_cls: 0.334  loss_box_reg: 0.2508  loss_mask: 0.3921  loss_rpn_cls: 0.05301  loss_rpn_loc: 0.08237  time: 0.5214  last_time: 0.5160  data_time: 0.0102  last_data_time: 0.0116   lr: 0.0001  max_mem: 11521M
[02/27 21:08:01 d2.utils.events]:  eta: 1 day, 1:09:49  iter: 10279  total_loss: 1.227  loss_cls: 0.3614  loss_box_reg: 0.2672  loss_mask: 0.4011  loss_rpn_cls: 0.08348  loss_rpn_loc: 0.1461  time: 0.5214  last_time: 0.5154  data_time: 0.0148  last_data_time: 0.0199   lr: 0.0001  max_mem: 11521M
[02/27 21:08:11 d2.utils.events]:  eta: 1 day, 1:09:43  iter: 10299  total_loss: 1.307  loss_cls: 0.4052  loss_box_reg: 0.3219  loss_mask: 0.3868  loss_rpn_cls: 0.09343  loss_rpn_loc: 0.1184  time: 0.5214  last_time: 0.5082  data_time: 0.0168  last_data_time: 0.0148   lr: 0.0001  max_mem: 11521M
[02/27 21:08:22 d2.utils.events]:  eta: 1 day, 1:09:26  iter: 10319  total_loss: 1.355  loss_cls: 0.3426  loss_box_reg: 0.2423  loss_mask: 0.4032  loss_rpn_cls: 0.06535  loss_rpn_loc: 0.1422  time: 0.5214  last_time: 0.5312  data_time: 0.0201  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 21:08:32 d2.utils.events]:  eta: 1 day, 1:09:10  iter: 10339  total_loss: 1.344  loss_cls: 0.3886  loss_box_reg: 0.2796  loss_mask: 0.4002  loss_rpn_cls: 0.05517  loss_rpn_loc: 0.1265  time: 0.5214  last_time: 0.5259  data_time: 0.0148  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 21:08:43 d2.utils.events]:  eta: 1 day, 1:09:28  iter: 10359  total_loss: 1.323  loss_cls: 0.397  loss_box_reg: 0.2626  loss_mask: 0.3828  loss_rpn_cls: 0.07385  loss_rpn_loc: 0.1514  time: 0.5214  last_time: 0.5475  data_time: 0.0248  last_data_time: 0.0435   lr: 0.0001  max_mem: 11521M
[02/27 21:08:54 d2.utils.events]:  eta: 1 day, 1:09:25  iter: 10379  total_loss: 1.129  loss_cls: 0.3287  loss_box_reg: 0.2274  loss_mask: 0.3838  loss_rpn_cls: 0.0767  loss_rpn_loc: 0.1538  time: 0.5214  last_time: 0.5333  data_time: 0.0203  last_data_time: 0.0288   lr: 0.0001  max_mem: 11521M
[02/27 21:09:05 d2.utils.events]:  eta: 1 day, 1:09:40  iter: 10399  total_loss: 1.206  loss_cls: 0.3559  loss_box_reg: 0.263  loss_mask: 0.3504  loss_rpn_cls: 0.07634  loss_rpn_loc: 0.1537  time: 0.5214  last_time: 0.5348  data_time: 0.0218  last_data_time: 0.0406   lr: 0.0001  max_mem: 11521M
[02/27 21:09:15 d2.utils.events]:  eta: 1 day, 1:10:54  iter: 10419  total_loss: 1.079  loss_cls: 0.2833  loss_box_reg: 0.2047  loss_mask: 0.3667  loss_rpn_cls: 0.06701  loss_rpn_loc: 0.09441  time: 0.5215  last_time: 0.5827  data_time: 0.0164  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 21:09:26 d2.utils.events]:  eta: 1 day, 1:12:01  iter: 10439  total_loss: 1.297  loss_cls: 0.366  loss_box_reg: 0.2851  loss_mask: 0.3524  loss_rpn_cls: 0.07282  loss_rpn_loc: 0.1864  time: 0.5215  last_time: 0.5473  data_time: 0.0211  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 21:09:37 d2.utils.events]:  eta: 1 day, 1:11:51  iter: 10459  total_loss: 1.127  loss_cls: 0.3138  loss_box_reg: 0.2285  loss_mask: 0.3656  loss_rpn_cls: 0.06505  loss_rpn_loc: 0.1205  time: 0.5215  last_time: 0.5164  data_time: 0.0250  last_data_time: 0.0157   lr: 0.0001  max_mem: 11521M
[02/27 21:09:47 d2.utils.events]:  eta: 1 day, 1:12:00  iter: 10479  total_loss: 1.067  loss_cls: 0.2737  loss_box_reg: 0.1915  loss_mask: 0.3702  loss_rpn_cls: 0.07599  loss_rpn_loc: 0.1231  time: 0.5215  last_time: 0.5301  data_time: 0.0136  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 21:09:58 d2.utils.events]:  eta: 1 day, 1:12:30  iter: 10499  total_loss: 1.015  loss_cls: 0.2942  loss_box_reg: 0.1863  loss_mask: 0.3556  loss_rpn_cls: 0.05094  loss_rpn_loc: 0.07906  time: 0.5215  last_time: 0.5495  data_time: 0.0208  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 21:10:09 d2.utils.events]:  eta: 1 day, 1:13:29  iter: 10519  total_loss: 1.219  loss_cls: 0.3894  loss_box_reg: 0.2869  loss_mask: 0.3965  loss_rpn_cls: 0.05219  loss_rpn_loc: 0.1339  time: 0.5215  last_time: 0.4920  data_time: 0.0218  last_data_time: 0.0523   lr: 0.0001  max_mem: 11521M
[02/27 21:10:19 d2.utils.events]:  eta: 1 day, 1:12:58  iter: 10539  total_loss: 1.198  loss_cls: 0.3465  loss_box_reg: 0.2365  loss_mask: 0.3612  loss_rpn_cls: 0.07686  loss_rpn_loc: 0.1539  time: 0.5215  last_time: 0.4981  data_time: 0.0133  last_data_time: 0.0072   lr: 0.0001  max_mem: 11521M
[02/27 21:10:30 d2.utils.events]:  eta: 1 day, 1:12:48  iter: 10559  total_loss: 1.102  loss_cls: 0.3558  loss_box_reg: 0.253  loss_mask: 0.3497  loss_rpn_cls: 0.06336  loss_rpn_loc: 0.1146  time: 0.5215  last_time: 0.5298  data_time: 0.0171  last_data_time: 0.0104   lr: 0.0001  max_mem: 11521M
[02/27 21:10:40 d2.utils.events]:  eta: 1 day, 1:12:18  iter: 10579  total_loss: 1.014  loss_cls: 0.2957  loss_box_reg: 0.1937  loss_mask: 0.3482  loss_rpn_cls: 0.05804  loss_rpn_loc: 0.1202  time: 0.5215  last_time: 0.4927  data_time: 0.0216  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 21:10:50 d2.utils.events]:  eta: 1 day, 1:12:47  iter: 10599  total_loss: 1.113  loss_cls: 0.2923  loss_box_reg: 0.2078  loss_mask: 0.3885  loss_rpn_cls: 0.05073  loss_rpn_loc: 0.1474  time: 0.5215  last_time: 0.5035  data_time: 0.0205  last_data_time: 0.0080   lr: 0.0001  max_mem: 11521M
[02/27 21:11:01 d2.utils.events]:  eta: 1 day, 1:12:00  iter: 10619  total_loss: 1.043  loss_cls: 0.3128  loss_box_reg: 0.2098  loss_mask: 0.3721  loss_rpn_cls: 0.06603  loss_rpn_loc: 0.133  time: 0.5215  last_time: 0.5137  data_time: 0.0119  last_data_time: 0.0213   lr: 0.0001  max_mem: 11521M
[02/27 21:11:11 d2.utils.events]:  eta: 1 day, 1:10:30  iter: 10639  total_loss: 1.281  loss_cls: 0.3714  loss_box_reg: 0.2608  loss_mask: 0.421  loss_rpn_cls: 0.0776  loss_rpn_loc: 0.1685  time: 0.5215  last_time: 0.5227  data_time: 0.0235  last_data_time: 0.0245   lr: 0.0001  max_mem: 11521M
[02/27 21:11:22 d2.utils.events]:  eta: 1 day, 1:10:38  iter: 10659  total_loss: 1.147  loss_cls: 0.3667  loss_box_reg: 0.2335  loss_mask: 0.3777  loss_rpn_cls: 0.05994  loss_rpn_loc: 0.166  time: 0.5215  last_time: 0.5262  data_time: 0.0150  last_data_time: 0.0071   lr: 0.0001  max_mem: 11521M
[02/27 21:11:32 d2.utils.events]:  eta: 1 day, 1:11:22  iter: 10679  total_loss: 1.174  loss_cls: 0.3538  loss_box_reg: 0.2528  loss_mask: 0.3473  loss_rpn_cls: 0.06619  loss_rpn_loc: 0.1011  time: 0.5215  last_time: 0.5345  data_time: 0.0170  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 21:11:43 d2.utils.events]:  eta: 1 day, 1:11:17  iter: 10699  total_loss: 1.153  loss_cls: 0.3475  loss_box_reg: 0.275  loss_mask: 0.3281  loss_rpn_cls: 0.056  loss_rpn_loc: 0.08918  time: 0.5215  last_time: 0.5112  data_time: 0.0233  last_data_time: 0.0109   lr: 0.0001  max_mem: 11521M
[02/27 21:11:53 d2.utils.events]:  eta: 1 day, 1:09:30  iter: 10719  total_loss: 1.201  loss_cls: 0.3481  loss_box_reg: 0.2718  loss_mask: 0.3706  loss_rpn_cls: 0.05447  loss_rpn_loc: 0.1242  time: 0.5215  last_time: 0.5132  data_time: 0.0215  last_data_time: 0.0278   lr: 0.0001  max_mem: 11521M
[02/27 21:12:04 d2.utils.events]:  eta: 1 day, 1:08:59  iter: 10739  total_loss: 1.192  loss_cls: 0.3215  loss_box_reg: 0.2384  loss_mask: 0.3775  loss_rpn_cls: 0.06564  loss_rpn_loc: 0.1775  time: 0.5215  last_time: 0.5071  data_time: 0.0267  last_data_time: 0.0137   lr: 0.0001  max_mem: 11521M
[02/27 21:12:14 d2.utils.events]:  eta: 1 day, 1:08:28  iter: 10759  total_loss: 1.192  loss_cls: 0.3411  loss_box_reg: 0.2259  loss_mask: 0.4103  loss_rpn_cls: 0.07683  loss_rpn_loc: 0.1034  time: 0.5215  last_time: 0.5151  data_time: 0.0181  last_data_time: 0.0108   lr: 0.0001  max_mem: 11521M
[02/27 21:12:25 d2.utils.events]:  eta: 1 day, 1:08:37  iter: 10779  total_loss: 1.221  loss_cls: 0.3609  loss_box_reg: 0.2458  loss_mask: 0.3917  loss_rpn_cls: 0.06682  loss_rpn_loc: 0.1249  time: 0.5215  last_time: 0.5259  data_time: 0.0275  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 21:12:35 d2.utils.events]:  eta: 1 day, 1:08:11  iter: 10799  total_loss: 1.214  loss_cls: 0.3611  loss_box_reg: 0.2591  loss_mask: 0.3715  loss_rpn_cls: 0.05638  loss_rpn_loc: 0.1625  time: 0.5215  last_time: 0.5316  data_time: 0.0193  last_data_time: 0.0449   lr: 0.0001  max_mem: 11521M
[02/27 21:12:46 d2.utils.events]:  eta: 1 day, 1:07:06  iter: 10819  total_loss: 1.407  loss_cls: 0.4429  loss_box_reg: 0.3032  loss_mask: 0.4073  loss_rpn_cls: 0.08021  loss_rpn_loc: 0.1468  time: 0.5215  last_time: 0.5157  data_time: 0.0223  last_data_time: 0.0400   lr: 0.0001  max_mem: 11521M
[02/27 21:12:56 d2.utils.events]:  eta: 1 day, 1:07:07  iter: 10839  total_loss: 1.204  loss_cls: 0.3265  loss_box_reg: 0.265  loss_mask: 0.3716  loss_rpn_cls: 0.0583  loss_rpn_loc: 0.1379  time: 0.5215  last_time: 0.5385  data_time: 0.0254  last_data_time: 0.0622   lr: 0.0001  max_mem: 11521M
[02/27 21:13:07 d2.utils.events]:  eta: 1 day, 1:07:45  iter: 10859  total_loss: 1.384  loss_cls: 0.3992  loss_box_reg: 0.277  loss_mask: 0.376  loss_rpn_cls: 0.07763  loss_rpn_loc: 0.1617  time: 0.5215  last_time: 0.5139  data_time: 0.0146  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 21:13:17 d2.utils.events]:  eta: 1 day, 1:07:17  iter: 10879  total_loss: 1.171  loss_cls: 0.3273  loss_box_reg: 0.2111  loss_mask: 0.3725  loss_rpn_cls: 0.05546  loss_rpn_loc: 0.1176  time: 0.5215  last_time: 0.5711  data_time: 0.0219  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 21:13:28 d2.utils.events]:  eta: 1 day, 1:07:00  iter: 10899  total_loss: 1.258  loss_cls: 0.3683  loss_box_reg: 0.254  loss_mask: 0.4028  loss_rpn_cls: 0.06234  loss_rpn_loc: 0.1526  time: 0.5215  last_time: 0.5216  data_time: 0.0164  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 21:13:39 d2.utils.events]:  eta: 1 day, 1:06:18  iter: 10919  total_loss: 1.147  loss_cls: 0.3103  loss_box_reg: 0.2028  loss_mask: 0.3981  loss_rpn_cls: 0.06436  loss_rpn_loc: 0.1683  time: 0.5215  last_time: 0.5385  data_time: 0.0223  last_data_time: 0.0585   lr: 0.0001  max_mem: 11521M
[02/27 21:13:49 d2.utils.events]:  eta: 1 day, 1:06:58  iter: 10939  total_loss: 1.143  loss_cls: 0.3145  loss_box_reg: 0.2362  loss_mask: 0.3568  loss_rpn_cls: 0.06383  loss_rpn_loc: 0.1421  time: 0.5215  last_time: 0.5543  data_time: 0.0153  last_data_time: 0.0070   lr: 0.0001  max_mem: 11521M
[02/27 21:14:00 d2.utils.events]:  eta: 1 day, 1:07:03  iter: 10959  total_loss: 1.2  loss_cls: 0.3344  loss_box_reg: 0.2405  loss_mask: 0.3939  loss_rpn_cls: 0.06314  loss_rpn_loc: 0.2074  time: 0.5215  last_time: 0.5517  data_time: 0.0125  last_data_time: 0.0105   lr: 0.0001  max_mem: 11521M
[02/27 21:14:10 d2.utils.events]:  eta: 1 day, 1:06:37  iter: 10979  total_loss: 1.15  loss_cls: 0.2872  loss_box_reg: 0.1713  loss_mask: 0.3948  loss_rpn_cls: 0.06224  loss_rpn_loc: 0.1039  time: 0.5215  last_time: 0.5146  data_time: 0.0104  last_data_time: 0.0114   lr: 0.0001  max_mem: 11521M
[02/27 21:14:21 d2.utils.events]:  eta: 1 day, 1:06:42  iter: 10999  total_loss: 1.153  loss_cls: 0.3139  loss_box_reg: 0.2231  loss_mask: 0.4038  loss_rpn_cls: 0.06498  loss_rpn_loc: 0.1562  time: 0.5215  last_time: 0.5154  data_time: 0.0107  last_data_time: 0.0108   lr: 0.0001  max_mem: 11521M
[02/27 21:14:32 d2.utils.events]:  eta: 1 day, 1:06:28  iter: 11019  total_loss: 1.323  loss_cls: 0.3746  loss_box_reg: 0.2673  loss_mask: 0.365  loss_rpn_cls: 0.08268  loss_rpn_loc: 0.2101  time: 0.5215  last_time: 0.5149  data_time: 0.0221  last_data_time: 0.0057   lr: 0.0001  max_mem: 11521M
[02/27 21:14:42 d2.utils.events]:  eta: 1 day, 1:06:34  iter: 11039  total_loss: 1.224  loss_cls: 0.3016  loss_box_reg: 0.2258  loss_mask: 0.3767  loss_rpn_cls: 0.06479  loss_rpn_loc: 0.1466  time: 0.5215  last_time: 0.4858  data_time: 0.0151  last_data_time: 0.0058   lr: 0.0001  max_mem: 11521M
[02/27 21:14:53 d2.utils.events]:  eta: 1 day, 1:06:21  iter: 11059  total_loss: 1.189  loss_cls: 0.3161  loss_box_reg: 0.2307  loss_mask: 0.3521  loss_rpn_cls: 0.07346  loss_rpn_loc: 0.122  time: 0.5215  last_time: 0.4969  data_time: 0.0185  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 21:15:03 d2.utils.events]:  eta: 1 day, 1:06:23  iter: 11079  total_loss: 1.202  loss_cls: 0.3254  loss_box_reg: 0.2162  loss_mask: 0.3656  loss_rpn_cls: 0.07401  loss_rpn_loc: 0.1679  time: 0.5215  last_time: 0.5346  data_time: 0.0224  last_data_time: 0.0495   lr: 0.0001  max_mem: 11521M
[02/27 21:15:14 d2.utils.events]:  eta: 1 day, 1:06:12  iter: 11099  total_loss: 1.242  loss_cls: 0.3654  loss_box_reg: 0.2442  loss_mask: 0.4209  loss_rpn_cls: 0.08177  loss_rpn_loc: 0.1293  time: 0.5215  last_time: 0.5193  data_time: 0.0205  last_data_time: 0.0308   lr: 0.0001  max_mem: 11521M
[02/27 21:15:24 d2.utils.events]:  eta: 1 day, 1:05:52  iter: 11119  total_loss: 1.29  loss_cls: 0.3419  loss_box_reg: 0.2708  loss_mask: 0.3862  loss_rpn_cls: 0.06825  loss_rpn_loc: 0.1451  time: 0.5215  last_time: 0.5079  data_time: 0.0117  last_data_time: 0.0387   lr: 0.0001  max_mem: 11521M
[02/27 21:15:34 d2.utils.events]:  eta: 1 day, 1:05:30  iter: 11139  total_loss: 1.184  loss_cls: 0.344  loss_box_reg: 0.2491  loss_mask: 0.3851  loss_rpn_cls: 0.06211  loss_rpn_loc: 0.1342  time: 0.5215  last_time: 0.5245  data_time: 0.0151  last_data_time: 0.0095   lr: 0.0001  max_mem: 11521M
[02/27 21:15:45 d2.utils.events]:  eta: 1 day, 1:05:21  iter: 11159  total_loss: 1.206  loss_cls: 0.3755  loss_box_reg: 0.2655  loss_mask: 0.4049  loss_rpn_cls: 0.07999  loss_rpn_loc: 0.09549  time: 0.5215  last_time: 0.5207  data_time: 0.0164  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 21:15:55 d2.utils.events]:  eta: 1 day, 1:05:05  iter: 11179  total_loss: 1.248  loss_cls: 0.3484  loss_box_reg: 0.2315  loss_mask: 0.3647  loss_rpn_cls: 0.07748  loss_rpn_loc: 0.1541  time: 0.5215  last_time: 0.5349  data_time: 0.0139  last_data_time: 0.0682   lr: 0.0001  max_mem: 11521M
[02/27 21:16:06 d2.utils.events]:  eta: 1 day, 1:05:00  iter: 11199  total_loss: 1.472  loss_cls: 0.4628  loss_box_reg: 0.3087  loss_mask: 0.3663  loss_rpn_cls: 0.07494  loss_rpn_loc: 0.1406  time: 0.5215  last_time: 0.5151  data_time: 0.0153  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 21:16:16 d2.utils.events]:  eta: 1 day, 1:04:29  iter: 11219  total_loss: 1.157  loss_cls: 0.3283  loss_box_reg: 0.2126  loss_mask: 0.371  loss_rpn_cls: 0.06493  loss_rpn_loc: 0.1463  time: 0.5215  last_time: 0.5227  data_time: 0.0096  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 21:16:27 d2.utils.events]:  eta: 1 day, 1:04:26  iter: 11239  total_loss: 1.238  loss_cls: 0.3798  loss_box_reg: 0.2402  loss_mask: 0.3647  loss_rpn_cls: 0.07828  loss_rpn_loc: 0.1308  time: 0.5215  last_time: 0.5257  data_time: 0.0218  last_data_time: 0.0121   lr: 0.0001  max_mem: 11521M
[02/27 21:16:37 d2.utils.events]:  eta: 1 day, 1:03:58  iter: 11259  total_loss: 1.121  loss_cls: 0.3331  loss_box_reg: 0.2235  loss_mask: 0.3681  loss_rpn_cls: 0.06939  loss_rpn_loc: 0.13  time: 0.5215  last_time: 0.5516  data_time: 0.0166  last_data_time: 0.0290   lr: 0.0001  max_mem: 11521M
[02/27 21:16:48 d2.utils.events]:  eta: 1 day, 1:04:32  iter: 11279  total_loss: 1.209  loss_cls: 0.3174  loss_box_reg: 0.2171  loss_mask: 0.3429  loss_rpn_cls: 0.0538  loss_rpn_loc: 0.1353  time: 0.5215  last_time: 0.5499  data_time: 0.0141  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 21:16:58 d2.utils.events]:  eta: 1 day, 1:04:16  iter: 11299  total_loss: 1.377  loss_cls: 0.396  loss_box_reg: 0.292  loss_mask: 0.4131  loss_rpn_cls: 0.05833  loss_rpn_loc: 0.1908  time: 0.5215  last_time: 0.5385  data_time: 0.0232  last_data_time: 0.0764   lr: 0.0001  max_mem: 11521M
[02/27 21:17:08 d2.utils.events]:  eta: 1 day, 1:03:56  iter: 11319  total_loss: 1.136  loss_cls: 0.282  loss_box_reg: 0.2015  loss_mask: 0.386  loss_rpn_cls: 0.06273  loss_rpn_loc: 0.2071  time: 0.5215  last_time: 0.5231  data_time: 0.0277  last_data_time: 0.0249   lr: 0.0001  max_mem: 11521M
[02/27 21:17:19 d2.utils.events]:  eta: 1 day, 1:03:11  iter: 11339  total_loss: 1.258  loss_cls: 0.3786  loss_box_reg: 0.2852  loss_mask: 0.386  loss_rpn_cls: 0.06244  loss_rpn_loc: 0.109  time: 0.5215  last_time: 0.5122  data_time: 0.0109  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 21:17:29 d2.utils.events]:  eta: 1 day, 1:02:53  iter: 11359  total_loss: 1.237  loss_cls: 0.336  loss_box_reg: 0.2526  loss_mask: 0.3624  loss_rpn_cls: 0.05484  loss_rpn_loc: 0.1407  time: 0.5215  last_time: 0.5056  data_time: 0.0185  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 21:17:39 d2.utils.events]:  eta: 1 day, 1:01:32  iter: 11379  total_loss: 1.146  loss_cls: 0.3142  loss_box_reg: 0.2067  loss_mask: 0.378  loss_rpn_cls: 0.06395  loss_rpn_loc: 0.1458  time: 0.5214  last_time: 0.4894  data_time: 0.0153  last_data_time: 0.0125   lr: 0.0001  max_mem: 11521M
[02/27 21:17:50 d2.utils.events]:  eta: 1 day, 1:00:35  iter: 11399  total_loss: 1.122  loss_cls: 0.3153  loss_box_reg: 0.2083  loss_mask: 0.3808  loss_rpn_cls: 0.0703  loss_rpn_loc: 0.09791  time: 0.5214  last_time: 0.5220  data_time: 0.0215  last_data_time: 0.0432   lr: 0.0001  max_mem: 11521M
[02/27 21:18:00 d2.utils.events]:  eta: 1 day, 0:59:29  iter: 11419  total_loss: 1.125  loss_cls: 0.3312  loss_box_reg: 0.2418  loss_mask: 0.3815  loss_rpn_cls: 0.05693  loss_rpn_loc: 0.1582  time: 0.5214  last_time: 0.5264  data_time: 0.0206  last_data_time: 0.0049   lr: 0.0001  max_mem: 11521M
[02/27 21:18:10 d2.utils.events]:  eta: 1 day, 0:58:30  iter: 11439  total_loss: 1.164  loss_cls: 0.3656  loss_box_reg: 0.2399  loss_mask: 0.3646  loss_rpn_cls: 0.05386  loss_rpn_loc: 0.1323  time: 0.5214  last_time: 0.5498  data_time: 0.0079  last_data_time: 0.0087   lr: 0.0001  max_mem: 11521M
[02/27 21:18:21 d2.utils.events]:  eta: 1 day, 0:59:02  iter: 11459  total_loss: 1.138  loss_cls: 0.3571  loss_box_reg: 0.2467  loss_mask: 0.3685  loss_rpn_cls: 0.07267  loss_rpn_loc: 0.1264  time: 0.5214  last_time: 0.5078  data_time: 0.0303  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 21:18:32 d2.utils.events]:  eta: 1 day, 0:58:29  iter: 11479  total_loss: 1.371  loss_cls: 0.3926  loss_box_reg: 0.2613  loss_mask: 0.3564  loss_rpn_cls: 0.08629  loss_rpn_loc: 0.1136  time: 0.5214  last_time: 0.5107  data_time: 0.0233  last_data_time: 0.0118   lr: 0.0001  max_mem: 11521M
[02/27 21:18:42 d2.utils.events]:  eta: 1 day, 0:57:59  iter: 11499  total_loss: 1.266  loss_cls: 0.3542  loss_box_reg: 0.272  loss_mask: 0.3818  loss_rpn_cls: 0.06033  loss_rpn_loc: 0.2006  time: 0.5214  last_time: 0.4987  data_time: 0.0128  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 21:18:53 d2.utils.events]:  eta: 1 day, 0:56:57  iter: 11519  total_loss: 1.153  loss_cls: 0.3254  loss_box_reg: 0.2339  loss_mask: 0.3676  loss_rpn_cls: 0.07142  loss_rpn_loc: 0.1217  time: 0.5214  last_time: 0.5196  data_time: 0.0215  last_data_time: 0.0285   lr: 0.0001  max_mem: 11521M
[02/27 21:19:03 d2.utils.events]:  eta: 1 day, 0:56:59  iter: 11539  total_loss: 1.089  loss_cls: 0.3366  loss_box_reg: 0.2297  loss_mask: 0.3591  loss_rpn_cls: 0.04686  loss_rpn_loc: 0.09819  time: 0.5214  last_time: 0.4908  data_time: 0.0177  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 21:19:14 d2.utils.events]:  eta: 1 day, 0:57:19  iter: 11559  total_loss: 1.184  loss_cls: 0.3229  loss_box_reg: 0.2293  loss_mask: 0.4023  loss_rpn_cls: 0.0667  loss_rpn_loc: 0.1228  time: 0.5214  last_time: 0.4746  data_time: 0.0229  last_data_time: 0.0055   lr: 0.0001  max_mem: 11521M
[02/27 21:19:25 d2.utils.events]:  eta: 1 day, 0:58:05  iter: 11579  total_loss: 1.12  loss_cls: 0.3325  loss_box_reg: 0.2543  loss_mask: 0.3488  loss_rpn_cls: 0.04948  loss_rpn_loc: 0.09219  time: 0.5214  last_time: 0.5313  data_time: 0.0118  last_data_time: 0.0081   lr: 0.0001  max_mem: 11521M
[02/27 21:19:35 d2.utils.events]:  eta: 1 day, 0:58:04  iter: 11599  total_loss: 1.267  loss_cls: 0.3374  loss_box_reg: 0.2733  loss_mask: 0.3551  loss_rpn_cls: 0.079  loss_rpn_loc: 0.1368  time: 0.5214  last_time: 0.5344  data_time: 0.0223  last_data_time: 0.0268   lr: 0.0001  max_mem: 11521M
[02/27 21:19:46 d2.utils.events]:  eta: 1 day, 0:58:15  iter: 11619  total_loss: 1.193  loss_cls: 0.384  loss_box_reg: 0.2568  loss_mask: 0.3982  loss_rpn_cls: 0.07342  loss_rpn_loc: 0.119  time: 0.5214  last_time: 0.5295  data_time: 0.0223  last_data_time: 0.0067   lr: 0.0001  max_mem: 11521M
[02/27 21:19:56 d2.utils.events]:  eta: 1 day, 0:58:35  iter: 11639  total_loss: 1.215  loss_cls: 0.3745  loss_box_reg: 0.2387  loss_mask: 0.3511  loss_rpn_cls: 0.0623  loss_rpn_loc: 0.14  time: 0.5215  last_time: 0.5343  data_time: 0.0078  last_data_time: 0.0026   lr: 0.0001  max_mem: 11521M
[02/27 21:20:07 d2.utils.events]:  eta: 1 day, 0:58:27  iter: 11659  total_loss: 1.191  loss_cls: 0.3651  loss_box_reg: 0.2546  loss_mask: 0.3996  loss_rpn_cls: 0.05923  loss_rpn_loc: 0.1254  time: 0.5215  last_time: 0.4873  data_time: 0.0123  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 21:20:17 d2.utils.events]:  eta: 1 day, 0:59:30  iter: 11679  total_loss: 1.282  loss_cls: 0.3336  loss_box_reg: 0.2248  loss_mask: 0.3957  loss_rpn_cls: 0.0719  loss_rpn_loc: 0.1572  time: 0.5215  last_time: 0.5361  data_time: 0.0157  last_data_time: 0.0508   lr: 0.0001  max_mem: 11521M
[02/27 21:20:28 d2.utils.events]:  eta: 1 day, 0:59:32  iter: 11699  total_loss: 1.249  loss_cls: 0.3365  loss_box_reg: 0.2339  loss_mask: 0.3635  loss_rpn_cls: 0.06846  loss_rpn_loc: 0.1266  time: 0.5215  last_time: 0.4890  data_time: 0.0217  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 21:20:38 d2.utils.events]:  eta: 1 day, 0:59:10  iter: 11719  total_loss: 1.273  loss_cls: 0.3638  loss_box_reg: 0.2615  loss_mask: 0.3832  loss_rpn_cls: 0.06449  loss_rpn_loc: 0.1244  time: 0.5215  last_time: 0.5218  data_time: 0.0228  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 21:20:49 d2.utils.events]:  eta: 1 day, 0:59:21  iter: 11739  total_loss: 1.259  loss_cls: 0.3931  loss_box_reg: 0.2885  loss_mask: 0.3582  loss_rpn_cls: 0.07598  loss_rpn_loc: 0.1078  time: 0.5215  last_time: 0.5191  data_time: 0.0208  last_data_time: 0.0212   lr: 0.0001  max_mem: 11521M
[02/27 21:20:59 d2.utils.events]:  eta: 1 day, 0:59:46  iter: 11759  total_loss: 1.282  loss_cls: 0.3989  loss_box_reg: 0.2843  loss_mask: 0.3801  loss_rpn_cls: 0.0641  loss_rpn_loc: 0.1341  time: 0.5215  last_time: 0.5236  data_time: 0.0208  last_data_time: 0.0420   lr: 0.0001  max_mem: 11521M
[02/27 21:21:10 d2.utils.events]:  eta: 1 day, 0:59:22  iter: 11779  total_loss: 1.358  loss_cls: 0.355  loss_box_reg: 0.2667  loss_mask: 0.3799  loss_rpn_cls: 0.05245  loss_rpn_loc: 0.1115  time: 0.5215  last_time: 0.5536  data_time: 0.0104  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 21:21:21 d2.utils.events]:  eta: 1 day, 0:59:25  iter: 11799  total_loss: 1.141  loss_cls: 0.3294  loss_box_reg: 0.2669  loss_mask: 0.3706  loss_rpn_cls: 0.07113  loss_rpn_loc: 0.1156  time: 0.5215  last_time: 0.5519  data_time: 0.0164  last_data_time: 0.0024   lr: 0.0001  max_mem: 11521M
[02/27 21:21:31 d2.utils.events]:  eta: 1 day, 0:59:39  iter: 11819  total_loss: 1.184  loss_cls: 0.3513  loss_box_reg: 0.2579  loss_mask: 0.3467  loss_rpn_cls: 0.06958  loss_rpn_loc: 0.1433  time: 0.5215  last_time: 0.5283  data_time: 0.0183  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 21:21:42 d2.utils.events]:  eta: 1 day, 0:59:04  iter: 11839  total_loss: 1.264  loss_cls: 0.3527  loss_box_reg: 0.26  loss_mask: 0.3885  loss_rpn_cls: 0.08643  loss_rpn_loc: 0.09735  time: 0.5215  last_time: 0.5207  data_time: 0.0150  last_data_time: 0.0234   lr: 0.0001  max_mem: 11521M
[02/27 21:21:52 d2.utils.events]:  eta: 1 day, 0:58:15  iter: 11859  total_loss: 1.205  loss_cls: 0.3218  loss_box_reg: 0.2247  loss_mask: 0.3735  loss_rpn_cls: 0.04465  loss_rpn_loc: 0.115  time: 0.5215  last_time: 0.5122  data_time: 0.0145  last_data_time: 0.0101   lr: 0.0001  max_mem: 11521M
[02/27 21:22:02 d2.utils.events]:  eta: 1 day, 0:57:07  iter: 11879  total_loss: 1.265  loss_cls: 0.3628  loss_box_reg: 0.2651  loss_mask: 0.3733  loss_rpn_cls: 0.08676  loss_rpn_loc: 0.1428  time: 0.5215  last_time: 0.5182  data_time: 0.0165  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 21:22:13 d2.utils.events]:  eta: 1 day, 0:55:33  iter: 11899  total_loss: 1.123  loss_cls: 0.3158  loss_box_reg: 0.2403  loss_mask: 0.4161  loss_rpn_cls: 0.0555  loss_rpn_loc: 0.07328  time: 0.5214  last_time: 0.5146  data_time: 0.0168  last_data_time: 0.0530   lr: 0.0001  max_mem: 11521M
[02/27 21:22:23 d2.utils.events]:  eta: 1 day, 0:55:06  iter: 11919  total_loss: 1.295  loss_cls: 0.3582  loss_box_reg: 0.233  loss_mask: 0.3878  loss_rpn_cls: 0.07887  loss_rpn_loc: 0.1581  time: 0.5214  last_time: 0.4918  data_time: 0.0143  last_data_time: 0.0201   lr: 0.0001  max_mem: 11521M
[02/27 21:22:33 d2.utils.events]:  eta: 1 day, 0:54:41  iter: 11939  total_loss: 1.152  loss_cls: 0.3001  loss_box_reg: 0.222  loss_mask: 0.339  loss_rpn_cls: 0.06232  loss_rpn_loc: 0.1428  time: 0.5214  last_time: 0.5234  data_time: 0.0137  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 21:22:44 d2.utils.events]:  eta: 1 day, 0:54:14  iter: 11959  total_loss: 1.308  loss_cls: 0.3813  loss_box_reg: 0.2764  loss_mask: 0.3656  loss_rpn_cls: 0.06919  loss_rpn_loc: 0.1131  time: 0.5214  last_time: 0.5168  data_time: 0.0213  last_data_time: 0.0118   lr: 0.0001  max_mem: 11521M
[02/27 21:22:55 d2.utils.events]:  eta: 1 day, 0:54:20  iter: 11979  total_loss: 1.177  loss_cls: 0.3044  loss_box_reg: 0.2496  loss_mask: 0.3978  loss_rpn_cls: 0.07397  loss_rpn_loc: 0.1468  time: 0.5214  last_time: 0.5188  data_time: 0.0142  last_data_time: 0.0077   lr: 0.0001  max_mem: 11521M
[02/27 21:23:05 d2.utils.events]:  eta: 1 day, 0:53:43  iter: 11999  total_loss: 1.347  loss_cls: 0.3624  loss_box_reg: 0.2464  loss_mask: 0.4336  loss_rpn_cls: 0.06055  loss_rpn_loc: 0.148  time: 0.5214  last_time: 0.5367  data_time: 0.0116  last_data_time: 0.0263   lr: 0.0001  max_mem: 11521M
[02/27 21:23:16 d2.utils.events]:  eta: 1 day, 0:53:48  iter: 12019  total_loss: 1.335  loss_cls: 0.4229  loss_box_reg: 0.2776  loss_mask: 0.4042  loss_rpn_cls: 0.0541  loss_rpn_loc: 0.1275  time: 0.5214  last_time: 0.5398  data_time: 0.0159  last_data_time: 0.0105   lr: 0.0001  max_mem: 11521M
[02/27 21:23:26 d2.utils.events]:  eta: 1 day, 0:53:12  iter: 12039  total_loss: 1.129  loss_cls: 0.2871  loss_box_reg: 0.2061  loss_mask: 0.3564  loss_rpn_cls: 0.08615  loss_rpn_loc: 0.1379  time: 0.5214  last_time: 0.5005  data_time: 0.0212  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 21:23:37 d2.utils.events]:  eta: 1 day, 0:53:15  iter: 12059  total_loss: 1.065  loss_cls: 0.3173  loss_box_reg: 0.2306  loss_mask: 0.3361  loss_rpn_cls: 0.04509  loss_rpn_loc: 0.1211  time: 0.5214  last_time: 0.5096  data_time: 0.0204  last_data_time: 0.0093   lr: 0.0001  max_mem: 11521M
[02/27 21:23:47 d2.utils.events]:  eta: 1 day, 0:52:42  iter: 12079  total_loss: 1.166  loss_cls: 0.3536  loss_box_reg: 0.2481  loss_mask: 0.3718  loss_rpn_cls: 0.05348  loss_rpn_loc: 0.1283  time: 0.5214  last_time: 0.4941  data_time: 0.0153  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 21:23:58 d2.utils.events]:  eta: 1 day, 0:52:54  iter: 12099  total_loss: 1.123  loss_cls: 0.3438  loss_box_reg: 0.2186  loss_mask: 0.3615  loss_rpn_cls: 0.06268  loss_rpn_loc: 0.09913  time: 0.5214  last_time: 0.5053  data_time: 0.0260  last_data_time: 0.0168   lr: 0.0001  max_mem: 11521M
[02/27 21:24:08 d2.utils.events]:  eta: 1 day, 0:52:56  iter: 12119  total_loss: 1.179  loss_cls: 0.3367  loss_box_reg: 0.2516  loss_mask: 0.3585  loss_rpn_cls: 0.06028  loss_rpn_loc: 0.1061  time: 0.5214  last_time: 0.5684  data_time: 0.0180  last_data_time: 0.0250   lr: 0.0001  max_mem: 11521M
[02/27 21:24:19 d2.utils.events]:  eta: 1 day, 0:53:34  iter: 12139  total_loss: 1.33  loss_cls: 0.389  loss_box_reg: 0.2679  loss_mask: 0.4083  loss_rpn_cls: 0.07591  loss_rpn_loc: 0.1552  time: 0.5215  last_time: 0.5720  data_time: 0.0168  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 21:24:30 d2.utils.events]:  eta: 1 day, 0:54:11  iter: 12159  total_loss: 1.128  loss_cls: 0.351  loss_box_reg: 0.2475  loss_mask: 0.3678  loss_rpn_cls: 0.05826  loss_rpn_loc: 0.08915  time: 0.5215  last_time: 0.5263  data_time: 0.0193  last_data_time: 0.0370   lr: 0.0001  max_mem: 11521M
[02/27 21:24:40 d2.utils.events]:  eta: 1 day, 0:55:28  iter: 12179  total_loss: 1.261  loss_cls: 0.3538  loss_box_reg: 0.2774  loss_mask: 0.3483  loss_rpn_cls: 0.06958  loss_rpn_loc: 0.08614  time: 0.5215  last_time: 0.5233  data_time: 0.0132  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 21:24:51 d2.utils.events]:  eta: 1 day, 0:55:24  iter: 12199  total_loss: 1.213  loss_cls: 0.3539  loss_box_reg: 0.2568  loss_mask: 0.3508  loss_rpn_cls: 0.0686  loss_rpn_loc: 0.1488  time: 0.5215  last_time: 0.5118  data_time: 0.0130  last_data_time: 0.0029   lr: 0.0001  max_mem: 11521M
[02/27 21:25:01 d2.utils.events]:  eta: 1 day, 0:55:34  iter: 12219  total_loss: 1.304  loss_cls: 0.388  loss_box_reg: 0.2574  loss_mask: 0.4035  loss_rpn_cls: 0.07533  loss_rpn_loc: 0.1672  time: 0.5215  last_time: 0.4984  data_time: 0.0150  last_data_time: 0.0131   lr: 0.0001  max_mem: 11521M
[02/27 21:25:12 d2.utils.events]:  eta: 1 day, 0:55:28  iter: 12239  total_loss: 1.228  loss_cls: 0.3043  loss_box_reg: 0.2493  loss_mask: 0.3875  loss_rpn_cls: 0.06124  loss_rpn_loc: 0.1265  time: 0.5215  last_time: 0.5804  data_time: 0.0125  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:25:22 d2.utils.events]:  eta: 1 day, 0:55:53  iter: 12259  total_loss: 1.202  loss_cls: 0.3324  loss_box_reg: 0.2477  loss_mask: 0.3843  loss_rpn_cls: 0.05879  loss_rpn_loc: 0.1116  time: 0.5215  last_time: 0.5520  data_time: 0.0197  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 21:25:33 d2.utils.events]:  eta: 1 day, 0:55:12  iter: 12279  total_loss: 1.035  loss_cls: 0.3169  loss_box_reg: 0.182  loss_mask: 0.3421  loss_rpn_cls: 0.05222  loss_rpn_loc: 0.09493  time: 0.5215  last_time: 0.5318  data_time: 0.0203  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:25:44 d2.utils.events]:  eta: 1 day, 0:56:34  iter: 12299  total_loss: 1.124  loss_cls: 0.3094  loss_box_reg: 0.2022  loss_mask: 0.3584  loss_rpn_cls: 0.07077  loss_rpn_loc: 0.1302  time: 0.5215  last_time: 0.5333  data_time: 0.0127  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 21:25:54 d2.utils.events]:  eta: 1 day, 0:56:28  iter: 12319  total_loss: 1.25  loss_cls: 0.3773  loss_box_reg: 0.2709  loss_mask: 0.3986  loss_rpn_cls: 0.04955  loss_rpn_loc: 0.1545  time: 0.5215  last_time: 0.5543  data_time: 0.0241  last_data_time: 0.1117   lr: 0.0001  max_mem: 11521M
[02/27 21:26:04 d2.utils.events]:  eta: 1 day, 0:56:14  iter: 12339  total_loss: 1.285  loss_cls: 0.4114  loss_box_reg: 0.2541  loss_mask: 0.3836  loss_rpn_cls: 0.0789  loss_rpn_loc: 0.1519  time: 0.5215  last_time: 0.4928  data_time: 0.0171  last_data_time: 0.0103   lr: 0.0001  max_mem: 11521M
[02/27 21:26:15 d2.utils.events]:  eta: 1 day, 0:56:07  iter: 12359  total_loss: 1.148  loss_cls: 0.3258  loss_box_reg: 0.2356  loss_mask: 0.39  loss_rpn_cls: 0.0798  loss_rpn_loc: 0.1772  time: 0.5215  last_time: 0.5335  data_time: 0.0190  last_data_time: 0.0509   lr: 0.0001  max_mem: 11521M
[02/27 21:26:25 d2.utils.events]:  eta: 1 day, 0:56:22  iter: 12379  total_loss: 1.252  loss_cls: 0.3528  loss_box_reg: 0.2544  loss_mask: 0.3799  loss_rpn_cls: 0.07516  loss_rpn_loc: 0.1117  time: 0.5215  last_time: 0.5632  data_time: 0.0161  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 21:26:36 d2.utils.events]:  eta: 1 day, 0:56:26  iter: 12399  total_loss: 1.156  loss_cls: 0.3485  loss_box_reg: 0.2417  loss_mask: 0.3586  loss_rpn_cls: 0.06684  loss_rpn_loc: 0.1107  time: 0.5215  last_time: 0.5179  data_time: 0.0259  last_data_time: 0.0111   lr: 0.0001  max_mem: 11521M
[02/27 21:26:47 d2.utils.events]:  eta: 1 day, 0:57:23  iter: 12419  total_loss: 1.351  loss_cls: 0.3753  loss_box_reg: 0.2701  loss_mask: 0.3759  loss_rpn_cls: 0.07969  loss_rpn_loc: 0.1221  time: 0.5215  last_time: 0.5195  data_time: 0.0313  last_data_time: 0.0093   lr: 0.0001  max_mem: 11521M
[02/27 21:26:57 d2.utils.events]:  eta: 1 day, 0:57:31  iter: 12439  total_loss: 1.129  loss_cls: 0.3022  loss_box_reg: 0.2347  loss_mask: 0.3464  loss_rpn_cls: 0.05347  loss_rpn_loc: 0.1472  time: 0.5215  last_time: 0.5083  data_time: 0.0188  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:27:08 d2.utils.events]:  eta: 1 day, 0:56:07  iter: 12459  total_loss: 1.249  loss_cls: 0.3422  loss_box_reg: 0.2188  loss_mask: 0.39  loss_rpn_cls: 0.07925  loss_rpn_loc: 0.1807  time: 0.5215  last_time: 0.5142  data_time: 0.0185  last_data_time: 0.0170   lr: 0.0001  max_mem: 11521M
[02/27 21:27:18 d2.utils.events]:  eta: 1 day, 0:56:27  iter: 12479  total_loss: 1.184  loss_cls: 0.365  loss_box_reg: 0.258  loss_mask: 0.3774  loss_rpn_cls: 0.06406  loss_rpn_loc: 0.105  time: 0.5215  last_time: 0.5128  data_time: 0.0207  last_data_time: 0.1000   lr: 0.0001  max_mem: 11521M
[02/27 21:27:29 d2.utils.events]:  eta: 1 day, 0:55:34  iter: 12499  total_loss: 1.035  loss_cls: 0.27  loss_box_reg: 0.1964  loss_mask: 0.3555  loss_rpn_cls: 0.06056  loss_rpn_loc: 0.09998  time: 0.5215  last_time: 0.5728  data_time: 0.0171  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 21:27:39 d2.utils.events]:  eta: 1 day, 0:55:09  iter: 12519  total_loss: 1.094  loss_cls: 0.3005  loss_box_reg: 0.2067  loss_mask: 0.3817  loss_rpn_cls: 0.06121  loss_rpn_loc: 0.1099  time: 0.5214  last_time: 0.5206  data_time: 0.0194  last_data_time: 0.0656   lr: 0.0001  max_mem: 11521M
[02/27 21:27:50 d2.utils.events]:  eta: 1 day, 0:55:07  iter: 12539  total_loss: 1.285  loss_cls: 0.3569  loss_box_reg: 0.24  loss_mask: 0.3764  loss_rpn_cls: 0.05975  loss_rpn_loc: 0.1452  time: 0.5214  last_time: 0.5498  data_time: 0.0337  last_data_time: 0.0879   lr: 0.0001  max_mem: 11521M
[02/27 21:28:00 d2.utils.events]:  eta: 1 day, 0:55:12  iter: 12559  total_loss: 1.219  loss_cls: 0.3696  loss_box_reg: 0.2788  loss_mask: 0.3754  loss_rpn_cls: 0.06983  loss_rpn_loc: 0.1101  time: 0.5215  last_time: 0.5176  data_time: 0.0183  last_data_time: 0.0170   lr: 0.0001  max_mem: 11521M
[02/27 21:28:11 d2.utils.events]:  eta: 1 day, 0:54:08  iter: 12579  total_loss: 1.335  loss_cls: 0.4028  loss_box_reg: 0.2665  loss_mask: 0.3733  loss_rpn_cls: 0.06044  loss_rpn_loc: 0.1165  time: 0.5215  last_time: 0.5452  data_time: 0.0171  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:28:21 d2.utils.events]:  eta: 1 day, 0:54:16  iter: 12599  total_loss: 1.2  loss_cls: 0.3582  loss_box_reg: 0.249  loss_mask: 0.3478  loss_rpn_cls: 0.06927  loss_rpn_loc: 0.1586  time: 0.5215  last_time: 0.5382  data_time: 0.0162  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 21:28:32 d2.utils.events]:  eta: 1 day, 0:53:32  iter: 12619  total_loss: 1.115  loss_cls: 0.3282  loss_box_reg: 0.2345  loss_mask: 0.3899  loss_rpn_cls: 0.05735  loss_rpn_loc: 0.09749  time: 0.5215  last_time: 0.5182  data_time: 0.0132  last_data_time: 0.0078   lr: 0.0001  max_mem: 11521M
[02/27 21:28:42 d2.utils.events]:  eta: 1 day, 0:53:21  iter: 12639  total_loss: 1.112  loss_cls: 0.2975  loss_box_reg: 0.2144  loss_mask: 0.3648  loss_rpn_cls: 0.0632  loss_rpn_loc: 0.136  time: 0.5215  last_time: 0.5344  data_time: 0.0158  last_data_time: 0.0132   lr: 0.0001  max_mem: 11521M
[02/27 21:28:53 d2.utils.events]:  eta: 1 day, 0:52:59  iter: 12659  total_loss: 1.277  loss_cls: 0.3764  loss_box_reg: 0.2726  loss_mask: 0.3874  loss_rpn_cls: 0.07257  loss_rpn_loc: 0.1534  time: 0.5215  last_time: 0.5292  data_time: 0.0167  last_data_time: 0.0153   lr: 0.0001  max_mem: 11521M
[02/27 21:29:04 d2.utils.events]:  eta: 1 day, 0:53:03  iter: 12679  total_loss: 1.184  loss_cls: 0.3571  loss_box_reg: 0.242  loss_mask: 0.3731  loss_rpn_cls: 0.07148  loss_rpn_loc: 0.1123  time: 0.5215  last_time: 0.5263  data_time: 0.0301  last_data_time: 0.0127   lr: 0.0001  max_mem: 11521M
[02/27 21:29:14 d2.utils.events]:  eta: 1 day, 0:51:57  iter: 12699  total_loss: 1.145  loss_cls: 0.3366  loss_box_reg: 0.2437  loss_mask: 0.3651  loss_rpn_cls: 0.0609  loss_rpn_loc: 0.1128  time: 0.5215  last_time: 0.5213  data_time: 0.0166  last_data_time: 0.0139   lr: 0.0001  max_mem: 11521M
[02/27 21:29:25 d2.utils.events]:  eta: 1 day, 0:51:47  iter: 12719  total_loss: 1.236  loss_cls: 0.372  loss_box_reg: 0.2583  loss_mask: 0.4148  loss_rpn_cls: 0.06277  loss_rpn_loc: 0.1494  time: 0.5215  last_time: 0.5267  data_time: 0.0147  last_data_time: 0.0419   lr: 0.0001  max_mem: 11521M
[02/27 21:29:35 d2.utils.events]:  eta: 1 day, 0:51:05  iter: 12739  total_loss: 1.077  loss_cls: 0.2859  loss_box_reg: 0.2293  loss_mask: 0.3566  loss_rpn_cls: 0.05246  loss_rpn_loc: 0.1074  time: 0.5214  last_time: 0.5100  data_time: 0.0182  last_data_time: 0.0278   lr: 0.0001  max_mem: 11521M
[02/27 21:29:45 d2.utils.events]:  eta: 1 day, 0:51:02  iter: 12759  total_loss: 1.239  loss_cls: 0.3153  loss_box_reg: 0.2264  loss_mask: 0.3821  loss_rpn_cls: 0.04739  loss_rpn_loc: 0.1747  time: 0.5214  last_time: 0.4862  data_time: 0.0137  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 21:29:56 d2.utils.events]:  eta: 1 day, 0:51:08  iter: 12779  total_loss: 1.192  loss_cls: 0.3643  loss_box_reg: 0.2511  loss_mask: 0.3739  loss_rpn_cls: 0.05192  loss_rpn_loc: 0.1218  time: 0.5215  last_time: 0.5120  data_time: 0.0242  last_data_time: 0.0064   lr: 0.0001  max_mem: 11521M
[02/27 21:30:07 d2.utils.events]:  eta: 1 day, 0:50:38  iter: 12799  total_loss: 1.036  loss_cls: 0.2688  loss_box_reg: 0.2188  loss_mask: 0.3148  loss_rpn_cls: 0.06618  loss_rpn_loc: 0.124  time: 0.5215  last_time: 0.5333  data_time: 0.0309  last_data_time: 0.0424   lr: 0.0001  max_mem: 11521M
[02/27 21:30:17 d2.utils.events]:  eta: 1 day, 0:50:28  iter: 12819  total_loss: 1.038  loss_cls: 0.3006  loss_box_reg: 0.2283  loss_mask: 0.3663  loss_rpn_cls: 0.07143  loss_rpn_loc: 0.1259  time: 0.5215  last_time: 0.5209  data_time: 0.0091  last_data_time: 0.0280   lr: 0.0001  max_mem: 11521M
[02/27 21:30:28 d2.utils.events]:  eta: 1 day, 0:50:37  iter: 12839  total_loss: 1.261  loss_cls: 0.3463  loss_box_reg: 0.2148  loss_mask: 0.3879  loss_rpn_cls: 0.07851  loss_rpn_loc: 0.1219  time: 0.5215  last_time: 0.5205  data_time: 0.0103  last_data_time: 0.0101   lr: 0.0001  max_mem: 11521M
[02/27 21:30:39 d2.utils.events]:  eta: 1 day, 0:51:18  iter: 12859  total_loss: 1.168  loss_cls: 0.3336  loss_box_reg: 0.2596  loss_mask: 0.3677  loss_rpn_cls: 0.05329  loss_rpn_loc: 0.1195  time: 0.5215  last_time: 0.5424  data_time: 0.0241  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 21:30:49 d2.utils.events]:  eta: 1 day, 0:52:11  iter: 12879  total_loss: 1.194  loss_cls: 0.3507  loss_box_reg: 0.2096  loss_mask: 0.3691  loss_rpn_cls: 0.06683  loss_rpn_loc: 0.1306  time: 0.5215  last_time: 0.4886  data_time: 0.0189  last_data_time: 0.0130   lr: 0.0001  max_mem: 11521M
[02/27 21:31:00 d2.utils.events]:  eta: 1 day, 0:54:43  iter: 12899  total_loss: 1.206  loss_cls: 0.3586  loss_box_reg: 0.2346  loss_mask: 0.3664  loss_rpn_cls: 0.08481  loss_rpn_loc: 0.09611  time: 0.5215  last_time: 0.5454  data_time: 0.0199  last_data_time: 0.0403   lr: 0.0001  max_mem: 11521M
[02/27 21:31:10 d2.utils.events]:  eta: 1 day, 0:54:56  iter: 12919  total_loss: 1.049  loss_cls: 0.2705  loss_box_reg: 0.2072  loss_mask: 0.364  loss_rpn_cls: 0.0532  loss_rpn_loc: 0.1443  time: 0.5215  last_time: 0.5020  data_time: 0.0221  last_data_time: 0.0133   lr: 0.0001  max_mem: 11521M
[02/27 21:31:21 d2.utils.events]:  eta: 1 day, 0:54:22  iter: 12939  total_loss: 1.218  loss_cls: 0.3103  loss_box_reg: 0.2392  loss_mask: 0.3783  loss_rpn_cls: 0.06049  loss_rpn_loc: 0.1516  time: 0.5215  last_time: 0.5314  data_time: 0.0240  last_data_time: 0.0140   lr: 0.0001  max_mem: 11521M
[02/27 21:31:32 d2.utils.events]:  eta: 1 day, 0:54:53  iter: 12959  total_loss: 1.252  loss_cls: 0.3308  loss_box_reg: 0.2678  loss_mask: 0.3771  loss_rpn_cls: 0.06463  loss_rpn_loc: 0.1661  time: 0.5215  last_time: 0.5388  data_time: 0.0126  last_data_time: 0.0129   lr: 0.0001  max_mem: 11521M
[02/27 21:31:42 d2.utils.events]:  eta: 1 day, 0:54:24  iter: 12979  total_loss: 1.132  loss_cls: 0.3587  loss_box_reg: 0.225  loss_mask: 0.3596  loss_rpn_cls: 0.04834  loss_rpn_loc: 0.1086  time: 0.5215  last_time: 0.5127  data_time: 0.0136  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 21:31:53 d2.utils.events]:  eta: 1 day, 0:55:19  iter: 12999  total_loss: 1.119  loss_cls: 0.3247  loss_box_reg: 0.2573  loss_mask: 0.3302  loss_rpn_cls: 0.06112  loss_rpn_loc: 0.08358  time: 0.5215  last_time: 0.5966  data_time: 0.0216  last_data_time: 0.0284   lr: 0.0001  max_mem: 11521M
[02/27 21:32:04 d2.utils.events]:  eta: 1 day, 0:54:29  iter: 13019  total_loss: 1.113  loss_cls: 0.3145  loss_box_reg: 0.2185  loss_mask: 0.3833  loss_rpn_cls: 0.07016  loss_rpn_loc: 0.08815  time: 0.5215  last_time: 0.5389  data_time: 0.0139  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 21:32:14 d2.utils.events]:  eta: 1 day, 0:54:11  iter: 13039  total_loss: 1.209  loss_cls: 0.37  loss_box_reg: 0.2587  loss_mask: 0.3892  loss_rpn_cls: 0.06102  loss_rpn_loc: 0.1227  time: 0.5215  last_time: 0.5301  data_time: 0.0248  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 21:32:24 d2.utils.events]:  eta: 1 day, 0:54:06  iter: 13059  total_loss: 1.166  loss_cls: 0.3847  loss_box_reg: 0.2413  loss_mask: 0.3857  loss_rpn_cls: 0.07309  loss_rpn_loc: 0.1185  time: 0.5215  last_time: 0.5302  data_time: 0.0172  last_data_time: 0.0510   lr: 0.0001  max_mem: 11521M
[02/27 21:32:35 d2.utils.events]:  eta: 1 day, 0:54:25  iter: 13079  total_loss: 1.156  loss_cls: 0.3135  loss_box_reg: 0.2281  loss_mask: 0.3682  loss_rpn_cls: 0.06595  loss_rpn_loc: 0.1482  time: 0.5215  last_time: 0.5264  data_time: 0.0128  last_data_time: 0.0408   lr: 0.0001  max_mem: 11521M
[02/27 21:32:46 d2.utils.events]:  eta: 1 day, 0:55:11  iter: 13099  total_loss: 1.089  loss_cls: 0.3626  loss_box_reg: 0.2267  loss_mask: 0.3929  loss_rpn_cls: 0.0706  loss_rpn_loc: 0.1126  time: 0.5216  last_time: 0.5125  data_time: 0.0253  last_data_time: 0.0138   lr: 0.0001  max_mem: 11521M
[02/27 21:32:57 d2.utils.events]:  eta: 1 day, 0:54:22  iter: 13119  total_loss: 1.307  loss_cls: 0.3737  loss_box_reg: 0.2481  loss_mask: 0.4027  loss_rpn_cls: 0.08565  loss_rpn_loc: 0.1441  time: 0.5216  last_time: 0.5447  data_time: 0.0133  last_data_time: 0.0668   lr: 0.0001  max_mem: 11521M
[02/27 21:33:07 d2.utils.events]:  eta: 1 day, 0:53:04  iter: 13139  total_loss: 1.231  loss_cls: 0.2991  loss_box_reg: 0.2149  loss_mask: 0.402  loss_rpn_cls: 0.04952  loss_rpn_loc: 0.2116  time: 0.5215  last_time: 0.5100  data_time: 0.0282  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 21:33:18 d2.utils.events]:  eta: 1 day, 0:52:41  iter: 13159  total_loss: 1.356  loss_cls: 0.3271  loss_box_reg: 0.2694  loss_mask: 0.3801  loss_rpn_cls: 0.07623  loss_rpn_loc: 0.2135  time: 0.5216  last_time: 0.4866  data_time: 0.0208  last_data_time: 0.0151   lr: 0.0001  max_mem: 11521M
[02/27 21:33:28 d2.utils.events]:  eta: 1 day, 0:52:03  iter: 13179  total_loss: 1.066  loss_cls: 0.3043  loss_box_reg: 0.2188  loss_mask: 0.3531  loss_rpn_cls: 0.05953  loss_rpn_loc: 0.1064  time: 0.5215  last_time: 0.5010  data_time: 0.0146  last_data_time: 0.0409   lr: 0.0001  max_mem: 11521M
[02/27 21:33:39 d2.utils.events]:  eta: 1 day, 0:51:39  iter: 13199  total_loss: 1.16  loss_cls: 0.3067  loss_box_reg: 0.2066  loss_mask: 0.3695  loss_rpn_cls: 0.05914  loss_rpn_loc: 0.1452  time: 0.5215  last_time: 0.5304  data_time: 0.0164  last_data_time: 0.0395   lr: 0.0001  max_mem: 11521M
[02/27 21:33:49 d2.utils.events]:  eta: 1 day, 0:50:27  iter: 13219  total_loss: 1.17  loss_cls: 0.2806  loss_box_reg: 0.2135  loss_mask: 0.3847  loss_rpn_cls: 0.08915  loss_rpn_loc: 0.1376  time: 0.5215  last_time: 0.5026  data_time: 0.0275  last_data_time: 0.0290   lr: 0.0001  max_mem: 11521M
[02/27 21:34:00 d2.utils.events]:  eta: 1 day, 0:50:38  iter: 13239  total_loss: 1.284  loss_cls: 0.3573  loss_box_reg: 0.2497  loss_mask: 0.4339  loss_rpn_cls: 0.05177  loss_rpn_loc: 0.1628  time: 0.5215  last_time: 0.5192  data_time: 0.0173  last_data_time: 0.0029   lr: 0.0001  max_mem: 11521M
[02/27 21:34:10 d2.utils.events]:  eta: 1 day, 0:51:11  iter: 13259  total_loss: 1.306  loss_cls: 0.4115  loss_box_reg: 0.2639  loss_mask: 0.3783  loss_rpn_cls: 0.07615  loss_rpn_loc: 0.1153  time: 0.5215  last_time: 0.4763  data_time: 0.0130  last_data_time: 0.0116   lr: 0.0001  max_mem: 11521M
[02/27 21:34:21 d2.utils.events]:  eta: 1 day, 0:51:10  iter: 13279  total_loss: 1.143  loss_cls: 0.3223  loss_box_reg: 0.2294  loss_mask: 0.375  loss_rpn_cls: 0.05136  loss_rpn_loc: 0.1059  time: 0.5215  last_time: 0.5421  data_time: 0.0130  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 21:34:31 d2.utils.events]:  eta: 1 day, 0:51:02  iter: 13299  total_loss: 1.216  loss_cls: 0.3574  loss_box_reg: 0.2555  loss_mask: 0.4088  loss_rpn_cls: 0.0721  loss_rpn_loc: 0.1401  time: 0.5215  last_time: 0.4833  data_time: 0.0230  last_data_time: 0.0129   lr: 0.0001  max_mem: 11521M
[02/27 21:34:42 d2.utils.events]:  eta: 1 day, 0:51:17  iter: 13319  total_loss: 1.286  loss_cls: 0.3843  loss_box_reg: 0.3046  loss_mask: 0.3818  loss_rpn_cls: 0.07283  loss_rpn_loc: 0.1366  time: 0.5215  last_time: 0.5550  data_time: 0.0414  last_data_time: 0.0735   lr: 0.0001  max_mem: 11521M
[02/27 21:34:52 d2.utils.events]:  eta: 1 day, 0:52:21  iter: 13339  total_loss: 1.188  loss_cls: 0.3254  loss_box_reg: 0.2486  loss_mask: 0.4066  loss_rpn_cls: 0.05846  loss_rpn_loc: 0.145  time: 0.5215  last_time: 0.5071  data_time: 0.0150  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 21:35:03 d2.utils.events]:  eta: 1 day, 0:51:31  iter: 13359  total_loss: 1.269  loss_cls: 0.364  loss_box_reg: 0.2376  loss_mask: 0.4117  loss_rpn_cls: 0.06689  loss_rpn_loc: 0.1477  time: 0.5215  last_time: 0.5310  data_time: 0.0192  last_data_time: 0.0116   lr: 0.0001  max_mem: 11521M
[02/27 21:35:13 d2.utils.events]:  eta: 1 day, 0:52:06  iter: 13379  total_loss: 1.226  loss_cls: 0.3134  loss_box_reg: 0.2447  loss_mask: 0.3628  loss_rpn_cls: 0.07745  loss_rpn_loc: 0.1349  time: 0.5215  last_time: 0.5471  data_time: 0.0146  last_data_time: 0.0147   lr: 0.0001  max_mem: 11521M
[02/27 21:35:24 d2.utils.events]:  eta: 1 day, 0:52:33  iter: 13399  total_loss: 1.114  loss_cls: 0.3213  loss_box_reg: 0.2352  loss_mask: 0.3471  loss_rpn_cls: 0.07162  loss_rpn_loc: 0.09641  time: 0.5215  last_time: 0.5464  data_time: 0.0178  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 21:35:35 d2.utils.events]:  eta: 1 day, 0:52:24  iter: 13419  total_loss: 1.229  loss_cls: 0.3637  loss_box_reg: 0.2541  loss_mask: 0.3616  loss_rpn_cls: 0.06334  loss_rpn_loc: 0.1268  time: 0.5216  last_time: 0.5502  data_time: 0.0270  last_data_time: 0.0300   lr: 0.0001  max_mem: 11521M
[02/27 21:35:46 d2.utils.events]:  eta: 1 day, 0:52:13  iter: 13439  total_loss: 1.177  loss_cls: 0.3273  loss_box_reg: 0.1954  loss_mask: 0.3701  loss_rpn_cls: 0.0612  loss_rpn_loc: 0.1518  time: 0.5216  last_time: 0.5621  data_time: 0.0243  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 21:35:56 d2.utils.events]:  eta: 1 day, 0:53:01  iter: 13459  total_loss: 1.186  loss_cls: 0.3077  loss_box_reg: 0.2319  loss_mask: 0.3836  loss_rpn_cls: 0.05853  loss_rpn_loc: 0.1368  time: 0.5216  last_time: 0.5282  data_time: 0.0311  last_data_time: 0.0137   lr: 0.0001  max_mem: 11521M
[02/27 21:36:07 d2.utils.events]:  eta: 1 day, 0:53:29  iter: 13479  total_loss: 1.228  loss_cls: 0.3423  loss_box_reg: 0.2565  loss_mask: 0.3788  loss_rpn_cls: 0.06002  loss_rpn_loc: 0.1496  time: 0.5216  last_time: 0.4828  data_time: 0.0137  last_data_time: 0.0113   lr: 0.0001  max_mem: 11521M
[02/27 21:36:17 d2.utils.events]:  eta: 1 day, 0:54:06  iter: 13499  total_loss: 1.2  loss_cls: 0.3549  loss_box_reg: 0.2657  loss_mask: 0.3804  loss_rpn_cls: 0.06258  loss_rpn_loc: 0.1276  time: 0.5216  last_time: 0.5211  data_time: 0.0111  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 21:36:28 d2.utils.events]:  eta: 1 day, 0:54:15  iter: 13519  total_loss: 1.16  loss_cls: 0.3037  loss_box_reg: 0.2143  loss_mask: 0.4025  loss_rpn_cls: 0.06303  loss_rpn_loc: 0.1131  time: 0.5216  last_time: 0.5526  data_time: 0.0386  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 21:36:38 d2.utils.events]:  eta: 1 day, 0:53:45  iter: 13539  total_loss: 1.173  loss_cls: 0.3356  loss_box_reg: 0.2252  loss_mask: 0.3483  loss_rpn_cls: 0.05585  loss_rpn_loc: 0.1057  time: 0.5216  last_time: 0.5392  data_time: 0.0123  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 21:36:49 d2.utils.events]:  eta: 1 day, 0:53:34  iter: 13559  total_loss: 0.9939  loss_cls: 0.2808  loss_box_reg: 0.2051  loss_mask: 0.3453  loss_rpn_cls: 0.05375  loss_rpn_loc: 0.1131  time: 0.5216  last_time: 0.6727  data_time: 0.0274  last_data_time: 0.0466   lr: 0.0001  max_mem: 11521M
[02/27 21:37:00 d2.utils.events]:  eta: 1 day, 0:54:24  iter: 13579  total_loss: 1.227  loss_cls: 0.3091  loss_box_reg: 0.2259  loss_mask: 0.3916  loss_rpn_cls: 0.06454  loss_rpn_loc: 0.1325  time: 0.5216  last_time: 0.5294  data_time: 0.0189  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 21:37:10 d2.utils.events]:  eta: 1 day, 0:53:33  iter: 13599  total_loss: 1.023  loss_cls: 0.3157  loss_box_reg: 0.2046  loss_mask: 0.3545  loss_rpn_cls: 0.05092  loss_rpn_loc: 0.1228  time: 0.5216  last_time: 0.5017  data_time: 0.0183  last_data_time: 0.0097   lr: 0.0001  max_mem: 11521M
[02/27 21:37:21 d2.utils.events]:  eta: 1 day, 0:53:37  iter: 13619  total_loss: 1.35  loss_cls: 0.4203  loss_box_reg: 0.2724  loss_mask: 0.3979  loss_rpn_cls: 0.06555  loss_rpn_loc: 0.1435  time: 0.5216  last_time: 0.5098  data_time: 0.0238  last_data_time: 0.0432   lr: 0.0001  max_mem: 11521M
[02/27 21:37:31 d2.utils.events]:  eta: 1 day, 0:52:40  iter: 13639  total_loss: 1.343  loss_cls: 0.4302  loss_box_reg: 0.3087  loss_mask: 0.3807  loss_rpn_cls: 0.08006  loss_rpn_loc: 0.1991  time: 0.5216  last_time: 0.5252  data_time: 0.0157  last_data_time: 0.0321   lr: 0.0001  max_mem: 11521M
[02/27 21:37:42 d2.utils.events]:  eta: 1 day, 0:52:45  iter: 13659  total_loss: 1.312  loss_cls: 0.3795  loss_box_reg: 0.2824  loss_mask: 0.3782  loss_rpn_cls: 0.06987  loss_rpn_loc: 0.1157  time: 0.5216  last_time: 0.5095  data_time: 0.0199  last_data_time: 0.0129   lr: 0.0001  max_mem: 11521M
[02/27 21:37:52 d2.utils.events]:  eta: 1 day, 0:51:45  iter: 13679  total_loss: 1.244  loss_cls: 0.3607  loss_box_reg: 0.2475  loss_mask: 0.3853  loss_rpn_cls: 0.0659  loss_rpn_loc: 0.1196  time: 0.5216  last_time: 0.4888  data_time: 0.0310  last_data_time: 0.0107   lr: 0.0001  max_mem: 11521M
[02/27 21:38:03 d2.utils.events]:  eta: 1 day, 0:52:37  iter: 13699  total_loss: 1.164  loss_cls: 0.3475  loss_box_reg: 0.248  loss_mask: 0.341  loss_rpn_cls: 0.06257  loss_rpn_loc: 0.1207  time: 0.5216  last_time: 0.5130  data_time: 0.0177  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 21:38:14 d2.utils.events]:  eta: 1 day, 0:53:20  iter: 13719  total_loss: 1.044  loss_cls: 0.3154  loss_box_reg: 0.2385  loss_mask: 0.3579  loss_rpn_cls: 0.05801  loss_rpn_loc: 0.1161  time: 0.5216  last_time: 0.5487  data_time: 0.0113  last_data_time: 0.0583   lr: 0.0001  max_mem: 11521M
[02/27 21:38:24 d2.utils.events]:  eta: 1 day, 0:54:10  iter: 13739  total_loss: 1.218  loss_cls: 0.2996  loss_box_reg: 0.213  loss_mask: 0.3618  loss_rpn_cls: 0.05714  loss_rpn_loc: 0.1426  time: 0.5216  last_time: 0.5261  data_time: 0.0193  last_data_time: 0.0664   lr: 0.0001  max_mem: 11521M
[02/27 21:38:35 d2.utils.events]:  eta: 1 day, 0:53:50  iter: 13759  total_loss: 1.254  loss_cls: 0.358  loss_box_reg: 0.2716  loss_mask: 0.3819  loss_rpn_cls: 0.05362  loss_rpn_loc: 0.1341  time: 0.5216  last_time: 0.5528  data_time: 0.0157  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 21:38:45 d2.utils.events]:  eta: 1 day, 0:53:39  iter: 13779  total_loss: 1.232  loss_cls: 0.4014  loss_box_reg: 0.2463  loss_mask: 0.3691  loss_rpn_cls: 0.05962  loss_rpn_loc: 0.1832  time: 0.5216  last_time: 0.5247  data_time: 0.0254  last_data_time: 0.0275   lr: 0.0001  max_mem: 11521M
[02/27 21:38:56 d2.utils.events]:  eta: 1 day, 0:55:06  iter: 13799  total_loss: 1.183  loss_cls: 0.3269  loss_box_reg: 0.2405  loss_mask: 0.3593  loss_rpn_cls: 0.07739  loss_rpn_loc: 0.1073  time: 0.5216  last_time: 0.5276  data_time: 0.0126  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 21:39:07 d2.utils.events]:  eta: 1 day, 0:54:32  iter: 13819  total_loss: 1.011  loss_cls: 0.3224  loss_box_reg: 0.2306  loss_mask: 0.3898  loss_rpn_cls: 0.05002  loss_rpn_loc: 0.0823  time: 0.5216  last_time: 0.5003  data_time: 0.0087  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 21:39:18 d2.utils.events]:  eta: 1 day, 0:54:45  iter: 13839  total_loss: 1.05  loss_cls: 0.2914  loss_box_reg: 0.2097  loss_mask: 0.3606  loss_rpn_cls: 0.06123  loss_rpn_loc: 0.1256  time: 0.5216  last_time: 0.5197  data_time: 0.0202  last_data_time: 0.0266   lr: 0.0001  max_mem: 11521M
[02/27 21:39:28 d2.utils.events]:  eta: 1 day, 0:54:43  iter: 13859  total_loss: 1.222  loss_cls: 0.3726  loss_box_reg: 0.2652  loss_mask: 0.3514  loss_rpn_cls: 0.05637  loss_rpn_loc: 0.1245  time: 0.5216  last_time: 0.5299  data_time: 0.0157  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 21:39:39 d2.utils.events]:  eta: 1 day, 0:54:51  iter: 13879  total_loss: 1.018  loss_cls: 0.2681  loss_box_reg: 0.202  loss_mask: 0.3598  loss_rpn_cls: 0.05692  loss_rpn_loc: 0.1223  time: 0.5217  last_time: 0.5014  data_time: 0.0080  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 21:39:50 d2.utils.events]:  eta: 1 day, 0:54:13  iter: 13899  total_loss: 1.168  loss_cls: 0.3905  loss_box_reg: 0.2993  loss_mask: 0.3641  loss_rpn_cls: 0.05401  loss_rpn_loc: 0.1077  time: 0.5217  last_time: 0.5267  data_time: 0.0135  last_data_time: 0.0069   lr: 0.0001  max_mem: 11521M
[02/27 21:40:00 d2.utils.events]:  eta: 1 day, 0:54:38  iter: 13919  total_loss: 1.239  loss_cls: 0.3629  loss_box_reg: 0.2785  loss_mask: 0.3649  loss_rpn_cls: 0.0586  loss_rpn_loc: 0.1155  time: 0.5217  last_time: 0.5459  data_time: 0.0290  last_data_time: 0.0173   lr: 0.0001  max_mem: 11521M
[02/27 21:40:11 d2.utils.events]:  eta: 1 day, 0:54:23  iter: 13939  total_loss: 1.352  loss_cls: 0.4082  loss_box_reg: 0.2826  loss_mask: 0.3697  loss_rpn_cls: 0.05781  loss_rpn_loc: 0.1431  time: 0.5217  last_time: 0.5172  data_time: 0.0152  last_data_time: 0.0067   lr: 0.0001  max_mem: 11521M
[02/27 21:40:22 d2.utils.events]:  eta: 1 day, 0:54:09  iter: 13959  total_loss: 1.168  loss_cls: 0.3536  loss_box_reg: 0.2485  loss_mask: 0.3586  loss_rpn_cls: 0.04848  loss_rpn_loc: 0.1084  time: 0.5217  last_time: 0.5636  data_time: 0.0398  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:40:32 d2.utils.events]:  eta: 1 day, 0:54:25  iter: 13979  total_loss: 1.248  loss_cls: 0.3677  loss_box_reg: 0.2398  loss_mask: 0.3977  loss_rpn_cls: 0.07736  loss_rpn_loc: 0.1312  time: 0.5217  last_time: 0.5963  data_time: 0.0222  last_data_time: 0.0371   lr: 0.0001  max_mem: 11521M
[02/27 21:40:43 d2.utils.events]:  eta: 1 day, 0:53:40  iter: 13999  total_loss: 1.242  loss_cls: 0.3434  loss_box_reg: 0.2542  loss_mask: 0.3781  loss_rpn_cls: 0.06931  loss_rpn_loc: 0.191  time: 0.5217  last_time: 0.5204  data_time: 0.0132  last_data_time: 0.0128   lr: 0.0001  max_mem: 11521M
[02/27 21:40:53 d2.utils.events]:  eta: 1 day, 0:52:58  iter: 14019  total_loss: 1.198  loss_cls: 0.3795  loss_box_reg: 0.2637  loss_mask: 0.3725  loss_rpn_cls: 0.05925  loss_rpn_loc: 0.1314  time: 0.5217  last_time: 0.4942  data_time: 0.0318  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 21:41:04 d2.utils.events]:  eta: 1 day, 0:53:01  iter: 14039  total_loss: 1.262  loss_cls: 0.334  loss_box_reg: 0.2442  loss_mask: 0.3586  loss_rpn_cls: 0.07802  loss_rpn_loc: 0.1249  time: 0.5217  last_time: 0.5372  data_time: 0.0098  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:41:14 d2.utils.events]:  eta: 1 day, 0:53:01  iter: 14059  total_loss: 1.034  loss_cls: 0.2836  loss_box_reg: 0.187  loss_mask: 0.3647  loss_rpn_cls: 0.06216  loss_rpn_loc: 0.1151  time: 0.5217  last_time: 0.5082  data_time: 0.0078  last_data_time: 0.0280   lr: 0.0001  max_mem: 11521M
[02/27 21:41:25 d2.utils.events]:  eta: 1 day, 0:52:17  iter: 14079  total_loss: 1.308  loss_cls: 0.3454  loss_box_reg: 0.2613  loss_mask: 0.3836  loss_rpn_cls: 0.1175  loss_rpn_loc: 0.1796  time: 0.5217  last_time: 0.5200  data_time: 0.0169  last_data_time: 0.0053   lr: 0.0001  max_mem: 11521M
[02/27 21:41:35 d2.utils.events]:  eta: 1 day, 0:50:22  iter: 14099  total_loss: 1.254  loss_cls: 0.3505  loss_box_reg: 0.2651  loss_mask: 0.3688  loss_rpn_cls: 0.07801  loss_rpn_loc: 0.1074  time: 0.5217  last_time: 0.4897  data_time: 0.0285  last_data_time: 0.0277   lr: 0.0001  max_mem: 11521M
[02/27 21:41:46 d2.utils.events]:  eta: 1 day, 0:49:05  iter: 14119  total_loss: 1.21  loss_cls: 0.3452  loss_box_reg: 0.2344  loss_mask: 0.3697  loss_rpn_cls: 0.06401  loss_rpn_loc: 0.1406  time: 0.5217  last_time: 0.5165  data_time: 0.0166  last_data_time: 0.0085   lr: 0.0001  max_mem: 11521M
[02/27 21:41:56 d2.utils.events]:  eta: 1 day, 0:48:59  iter: 14139  total_loss: 1.152  loss_cls: 0.3392  loss_box_reg: 0.2726  loss_mask: 0.3497  loss_rpn_cls: 0.08556  loss_rpn_loc: 0.1304  time: 0.5217  last_time: 0.5457  data_time: 0.0215  last_data_time: 0.0288   lr: 0.0001  max_mem: 11521M
[02/27 21:42:07 d2.utils.events]:  eta: 1 day, 0:48:25  iter: 14159  total_loss: 1.221  loss_cls: 0.3814  loss_box_reg: 0.2748  loss_mask: 0.3507  loss_rpn_cls: 0.07069  loss_rpn_loc: 0.1147  time: 0.5217  last_time: 0.5098  data_time: 0.0208  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 21:42:17 d2.utils.events]:  eta: 1 day, 0:48:20  iter: 14179  total_loss: 1.192  loss_cls: 0.3381  loss_box_reg: 0.2339  loss_mask: 0.4015  loss_rpn_cls: 0.07095  loss_rpn_loc: 0.1486  time: 0.5217  last_time: 0.5394  data_time: 0.0161  last_data_time: 0.0460   lr: 0.0001  max_mem: 11521M
[02/27 21:42:28 d2.utils.events]:  eta: 1 day, 0:49:09  iter: 14199  total_loss: 1.248  loss_cls: 0.404  loss_box_reg: 0.2609  loss_mask: 0.3798  loss_rpn_cls: 0.06389  loss_rpn_loc: 0.1325  time: 0.5217  last_time: 0.5600  data_time: 0.0111  last_data_time: 0.0076   lr: 0.0001  max_mem: 11521M
[02/27 21:42:38 d2.utils.events]:  eta: 1 day, 0:48:35  iter: 14219  total_loss: 1.07  loss_cls: 0.3376  loss_box_reg: 0.2117  loss_mask: 0.3392  loss_rpn_cls: 0.06402  loss_rpn_loc: 0.1224  time: 0.5216  last_time: 0.5066  data_time: 0.0107  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 21:42:48 d2.utils.events]:  eta: 1 day, 0:48:01  iter: 14239  total_loss: 1.2  loss_cls: 0.3174  loss_box_reg: 0.2295  loss_mask: 0.3569  loss_rpn_cls: 0.05173  loss_rpn_loc: 0.1281  time: 0.5216  last_time: 0.5348  data_time: 0.0171  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 21:42:59 d2.utils.events]:  eta: 1 day, 0:48:20  iter: 14259  total_loss: 1.302  loss_cls: 0.3721  loss_box_reg: 0.2682  loss_mask: 0.3868  loss_rpn_cls: 0.05995  loss_rpn_loc: 0.1481  time: 0.5217  last_time: 0.6789  data_time: 0.0179  last_data_time: 0.0515   lr: 0.0001  max_mem: 11521M
[02/27 21:43:10 d2.utils.events]:  eta: 1 day, 0:47:09  iter: 14279  total_loss: 1.136  loss_cls: 0.3828  loss_box_reg: 0.2452  loss_mask: 0.3678  loss_rpn_cls: 0.06503  loss_rpn_loc: 0.09516  time: 0.5216  last_time: 0.5111  data_time: 0.0191  last_data_time: 0.0291   lr: 0.0001  max_mem: 11521M
[02/27 21:43:20 d2.utils.events]:  eta: 1 day, 0:46:33  iter: 14299  total_loss: 1.162  loss_cls: 0.3503  loss_box_reg: 0.2627  loss_mask: 0.344  loss_rpn_cls: 0.06795  loss_rpn_loc: 0.08533  time: 0.5217  last_time: 0.5550  data_time: 0.0174  last_data_time: 0.0049   lr: 0.0001  max_mem: 11521M
[02/27 21:43:31 d2.utils.events]:  eta: 1 day, 0:46:34  iter: 14319  total_loss: 1.108  loss_cls: 0.3232  loss_box_reg: 0.2542  loss_mask: 0.3523  loss_rpn_cls: 0.05215  loss_rpn_loc: 0.1274  time: 0.5217  last_time: 0.5383  data_time: 0.0246  last_data_time: 0.0109   lr: 0.0001  max_mem: 11521M
[02/27 21:43:41 d2.utils.events]:  eta: 1 day, 0:46:02  iter: 14339  total_loss: 1.286  loss_cls: 0.3282  loss_box_reg: 0.2306  loss_mask: 0.3743  loss_rpn_cls: 0.06978  loss_rpn_loc: 0.1044  time: 0.5216  last_time: 0.5223  data_time: 0.0175  last_data_time: 0.0097   lr: 0.0001  max_mem: 11521M
[02/27 21:43:52 d2.utils.events]:  eta: 1 day, 0:45:52  iter: 14359  total_loss: 1.075  loss_cls: 0.3329  loss_box_reg: 0.2186  loss_mask: 0.3385  loss_rpn_cls: 0.06453  loss_rpn_loc: 0.1131  time: 0.5216  last_time: 0.5117  data_time: 0.0209  last_data_time: 0.0414   lr: 0.0001  max_mem: 11521M
[02/27 21:44:02 d2.utils.events]:  eta: 1 day, 0:46:35  iter: 14379  total_loss: 1.122  loss_cls: 0.3377  loss_box_reg: 0.2251  loss_mask: 0.358  loss_rpn_cls: 0.05473  loss_rpn_loc: 0.1249  time: 0.5217  last_time: 0.5255  data_time: 0.0124  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 21:44:13 d2.utils.events]:  eta: 1 day, 0:45:31  iter: 14399  total_loss: 1.283  loss_cls: 0.4064  loss_box_reg: 0.272  loss_mask: 0.3537  loss_rpn_cls: 0.06697  loss_rpn_loc: 0.1772  time: 0.5217  last_time: 0.5120  data_time: 0.0122  last_data_time: 0.0135   lr: 0.0001  max_mem: 11521M
[02/27 21:44:23 d2.utils.events]:  eta: 1 day, 0:45:20  iter: 14419  total_loss: 1.261  loss_cls: 0.3383  loss_box_reg: 0.2544  loss_mask: 0.4028  loss_rpn_cls: 0.06322  loss_rpn_loc: 0.2239  time: 0.5217  last_time: 0.5308  data_time: 0.0161  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 21:44:34 d2.utils.events]:  eta: 1 day, 0:44:55  iter: 14439  total_loss: 1.128  loss_cls: 0.3306  loss_box_reg: 0.2298  loss_mask: 0.3564  loss_rpn_cls: 0.04936  loss_rpn_loc: 0.1073  time: 0.5217  last_time: 0.5738  data_time: 0.0196  last_data_time: 0.0168   lr: 0.0001  max_mem: 11521M
[02/27 21:44:45 d2.utils.events]:  eta: 1 day, 0:45:13  iter: 14459  total_loss: 1.176  loss_cls: 0.3517  loss_box_reg: 0.2604  loss_mask: 0.3767  loss_rpn_cls: 0.05218  loss_rpn_loc: 0.1226  time: 0.5217  last_time: 0.5267  data_time: 0.0173  last_data_time: 0.0116   lr: 0.0001  max_mem: 11521M
[02/27 21:44:55 d2.utils.events]:  eta: 1 day, 0:44:46  iter: 14479  total_loss: 1.157  loss_cls: 0.3712  loss_box_reg: 0.2761  loss_mask: 0.3517  loss_rpn_cls: 0.05583  loss_rpn_loc: 0.09818  time: 0.5217  last_time: 0.5570  data_time: 0.0222  last_data_time: 0.0545   lr: 0.0001  max_mem: 11521M
[02/27 21:45:06 d2.utils.events]:  eta: 1 day, 0:44:06  iter: 14499  total_loss: 1.056  loss_cls: 0.3536  loss_box_reg: 0.2052  loss_mask: 0.3257  loss_rpn_cls: 0.04751  loss_rpn_loc: 0.08898  time: 0.5217  last_time: 0.5022  data_time: 0.0200  last_data_time: 0.0132   lr: 0.0001  max_mem: 11521M
[02/27 21:45:16 d2.utils.events]:  eta: 1 day, 0:44:13  iter: 14519  total_loss: 1.197  loss_cls: 0.3726  loss_box_reg: 0.2399  loss_mask: 0.3831  loss_rpn_cls: 0.06024  loss_rpn_loc: 0.177  time: 0.5217  last_time: 0.5258  data_time: 0.0096  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 21:45:27 d2.utils.events]:  eta: 1 day, 0:43:58  iter: 14539  total_loss: 1.041  loss_cls: 0.2949  loss_box_reg: 0.2304  loss_mask: 0.3562  loss_rpn_cls: 0.04758  loss_rpn_loc: 0.1118  time: 0.5217  last_time: 0.5059  data_time: 0.0260  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 21:45:37 d2.utils.events]:  eta: 1 day, 0:43:21  iter: 14559  total_loss: 1.006  loss_cls: 0.2626  loss_box_reg: 0.1724  loss_mask: 0.3738  loss_rpn_cls: 0.07403  loss_rpn_loc: 0.08379  time: 0.5217  last_time: 0.5270  data_time: 0.0183  last_data_time: 0.0294   lr: 0.0001  max_mem: 11521M
[02/27 21:45:48 d2.utils.events]:  eta: 1 day, 0:42:56  iter: 14579  total_loss: 1.128  loss_cls: 0.3329  loss_box_reg: 0.2169  loss_mask: 0.3719  loss_rpn_cls: 0.08429  loss_rpn_loc: 0.1107  time: 0.5217  last_time: 0.5217  data_time: 0.0240  last_data_time: 0.0112   lr: 0.0001  max_mem: 11521M
[02/27 21:45:58 d2.utils.events]:  eta: 1 day, 0:42:21  iter: 14599  total_loss: 1.209  loss_cls: 0.3018  loss_box_reg: 0.2396  loss_mask: 0.3313  loss_rpn_cls: 0.08056  loss_rpn_loc: 0.1289  time: 0.5217  last_time: 0.5838  data_time: 0.0174  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:46:09 d2.utils.events]:  eta: 1 day, 0:42:35  iter: 14619  total_loss: 1.042  loss_cls: 0.3041  loss_box_reg: 0.2092  loss_mask: 0.3514  loss_rpn_cls: 0.0651  loss_rpn_loc: 0.1456  time: 0.5217  last_time: 0.5128  data_time: 0.0234  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 21:46:19 d2.utils.events]:  eta: 1 day, 0:42:00  iter: 14639  total_loss: 1.064  loss_cls: 0.2579  loss_box_reg: 0.2052  loss_mask: 0.36  loss_rpn_cls: 0.06502  loss_rpn_loc: 0.1308  time: 0.5217  last_time: 0.5018  data_time: 0.0288  last_data_time: 0.0122   lr: 0.0001  max_mem: 11521M
[02/27 21:46:30 d2.utils.events]:  eta: 1 day, 0:42:30  iter: 14659  total_loss: 1.206  loss_cls: 0.3141  loss_box_reg: 0.2268  loss_mask: 0.3894  loss_rpn_cls: 0.06369  loss_rpn_loc: 0.131  time: 0.5217  last_time: 0.5299  data_time: 0.0292  last_data_time: 0.0393   lr: 0.0001  max_mem: 11521M
[02/27 21:46:40 d2.utils.events]:  eta: 1 day, 0:42:05  iter: 14679  total_loss: 1.081  loss_cls: 0.3254  loss_box_reg: 0.2123  loss_mask: 0.3693  loss_rpn_cls: 0.06073  loss_rpn_loc: 0.1112  time: 0.5217  last_time: 0.5326  data_time: 0.0132  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 21:46:51 d2.utils.events]:  eta: 1 day, 0:41:29  iter: 14699  total_loss: 0.98  loss_cls: 0.2719  loss_box_reg: 0.2019  loss_mask: 0.3359  loss_rpn_cls: 0.06132  loss_rpn_loc: 0.1518  time: 0.5217  last_time: 0.5505  data_time: 0.0118  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 21:47:02 d2.utils.events]:  eta: 1 day, 0:41:30  iter: 14719  total_loss: 1.209  loss_cls: 0.3632  loss_box_reg: 0.2425  loss_mask: 0.3643  loss_rpn_cls: 0.06818  loss_rpn_loc: 0.1584  time: 0.5217  last_time: 0.5470  data_time: 0.0156  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 21:47:12 d2.utils.events]:  eta: 1 day, 0:41:08  iter: 14739  total_loss: 1.182  loss_cls: 0.322  loss_box_reg: 0.2605  loss_mask: 0.3656  loss_rpn_cls: 0.06781  loss_rpn_loc: 0.1141  time: 0.5217  last_time: 0.4971  data_time: 0.0107  last_data_time: 0.0088   lr: 0.0001  max_mem: 11521M
[02/27 21:47:23 d2.utils.events]:  eta: 1 day, 0:40:40  iter: 14759  total_loss: 1.037  loss_cls: 0.2869  loss_box_reg: 0.2334  loss_mask: 0.3683  loss_rpn_cls: 0.05459  loss_rpn_loc: 0.1362  time: 0.5217  last_time: 0.5092  data_time: 0.0151  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 21:47:33 d2.utils.events]:  eta: 1 day, 0:39:39  iter: 14779  total_loss: 1.191  loss_cls: 0.334  loss_box_reg: 0.2474  loss_mask: 0.3896  loss_rpn_cls: 0.06047  loss_rpn_loc: 0.1344  time: 0.5217  last_time: 0.5675  data_time: 0.0253  last_data_time: 0.0520   lr: 0.0001  max_mem: 11521M
[02/27 21:47:44 d2.utils.events]:  eta: 1 day, 0:37:54  iter: 14799  total_loss: 1.123  loss_cls: 0.3144  loss_box_reg: 0.2448  loss_mask: 0.3322  loss_rpn_cls: 0.0606  loss_rpn_loc: 0.07924  time: 0.5217  last_time: 0.5103  data_time: 0.0245  last_data_time: 0.0139   lr: 0.0001  max_mem: 11521M
[02/27 21:47:54 d2.utils.events]:  eta: 1 day, 0:37:32  iter: 14819  total_loss: 1.289  loss_cls: 0.3467  loss_box_reg: 0.2583  loss_mask: 0.3706  loss_rpn_cls: 0.06615  loss_rpn_loc: 0.1681  time: 0.5217  last_time: 0.5362  data_time: 0.0156  last_data_time: 0.0059   lr: 0.0001  max_mem: 11521M
[02/27 21:48:05 d2.utils.events]:  eta: 1 day, 0:37:33  iter: 14839  total_loss: 1.051  loss_cls: 0.3049  loss_box_reg: 0.2209  loss_mask: 0.336  loss_rpn_cls: 0.05333  loss_rpn_loc: 0.08368  time: 0.5217  last_time: 0.5339  data_time: 0.0226  last_data_time: 0.0634   lr: 0.0001  max_mem: 11521M
[02/27 21:48:15 d2.utils.events]:  eta: 1 day, 0:37:21  iter: 14859  total_loss: 1.145  loss_cls: 0.3244  loss_box_reg: 0.1955  loss_mask: 0.3925  loss_rpn_cls: 0.05627  loss_rpn_loc: 0.1057  time: 0.5217  last_time: 0.5003  data_time: 0.0168  last_data_time: 0.0164   lr: 0.0001  max_mem: 11521M
[02/27 21:48:26 d2.utils.events]:  eta: 1 day, 0:37:15  iter: 14879  total_loss: 1.113  loss_cls: 0.3247  loss_box_reg: 0.216  loss_mask: 0.3684  loss_rpn_cls: 0.05488  loss_rpn_loc: 0.1557  time: 0.5217  last_time: 0.5630  data_time: 0.0197  last_data_time: 0.0252   lr: 0.0001  max_mem: 11521M
[02/27 21:48:36 d2.utils.events]:  eta: 1 day, 0:35:36  iter: 14899  total_loss: 1.201  loss_cls: 0.319  loss_box_reg: 0.2082  loss_mask: 0.3781  loss_rpn_cls: 0.04822  loss_rpn_loc: 0.1527  time: 0.5217  last_time: 0.5156  data_time: 0.0184  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 21:48:47 d2.utils.events]:  eta: 1 day, 0:35:15  iter: 14919  total_loss: 1.254  loss_cls: 0.3589  loss_box_reg: 0.2535  loss_mask: 0.378  loss_rpn_cls: 0.05525  loss_rpn_loc: 0.1157  time: 0.5217  last_time: 0.4869  data_time: 0.0106  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 21:48:57 d2.utils.events]:  eta: 1 day, 0:35:15  iter: 14939  total_loss: 1.24  loss_cls: 0.3297  loss_box_reg: 0.2594  loss_mask: 0.3755  loss_rpn_cls: 0.07891  loss_rpn_loc: 0.1407  time: 0.5217  last_time: 0.5356  data_time: 0.0164  last_data_time: 0.0032   lr: 0.0001  max_mem: 11521M
[02/27 21:49:08 d2.utils.events]:  eta: 1 day, 0:35:33  iter: 14959  total_loss: 1.149  loss_cls: 0.3204  loss_box_reg: 0.2369  loss_mask: 0.38  loss_rpn_cls: 0.06568  loss_rpn_loc: 0.1221  time: 0.5217  last_time: 0.5245  data_time: 0.0151  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:49:19 d2.utils.events]:  eta: 1 day, 0:34:54  iter: 14979  total_loss: 1.175  loss_cls: 0.3206  loss_box_reg: 0.2398  loss_mask: 0.385  loss_rpn_cls: 0.05231  loss_rpn_loc: 0.09692  time: 0.5217  last_time: 0.5823  data_time: 0.0172  last_data_time: 0.0094   lr: 0.0001  max_mem: 11521M
[02/27 21:49:29 fvcore.common.checkpoint]: Saving checkpoint to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/model_0014999.pth
[02/27 21:49:31 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_val2017.json
[02/27 21:49:31 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]
[02/27 21:49:31 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/27 21:49:31 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...
[02/27 21:49:32 d2.data.common]: Serialized dataset takes 19.19 MiB
[02/27 21:49:32 d2.evaluation.evaluator]: Start inference on 1250 batches
[02/27 21:49:39 d2.evaluation.evaluator]: Inference done 11/1250. Dataloading: 0.0022 s/iter. Inference: 0.2994 s/iter. Eval: 0.0311 s/iter. Total: 0.3326 s/iter. ETA=0:06:52
[02/27 21:49:44 d2.evaluation.evaluator]: Inference done 27/1250. Dataloading: 0.0030 s/iter. Inference: 0.2899 s/iter. Eval: 0.0314 s/iter. Total: 0.3247 s/iter. ETA=0:06:37
[02/27 21:49:49 d2.evaluation.evaluator]: Inference done 43/1250. Dataloading: 0.0030 s/iter. Inference: 0.2841 s/iter. Eval: 0.0350 s/iter. Total: 0.3224 s/iter. ETA=0:06:29
[02/27 21:49:54 d2.evaluation.evaluator]: Inference done 58/1250. Dataloading: 0.0026 s/iter. Inference: 0.2937 s/iter. Eval: 0.0312 s/iter. Total: 0.3277 s/iter. ETA=0:06:30
[02/27 21:49:59 d2.evaluation.evaluator]: Inference done 73/1250. Dataloading: 0.0026 s/iter. Inference: 0.2963 s/iter. Eval: 0.0301 s/iter. Total: 0.3291 s/iter. ETA=0:06:27
[02/27 21:50:04 d2.evaluation.evaluator]: Inference done 87/1250. Dataloading: 0.0026 s/iter. Inference: 0.3008 s/iter. Eval: 0.0314 s/iter. Total: 0.3349 s/iter. ETA=0:06:29
[02/27 21:50:09 d2.evaluation.evaluator]: Inference done 102/1250. Dataloading: 0.0025 s/iter. Inference: 0.2992 s/iter. Eval: 0.0331 s/iter. Total: 0.3350 s/iter. ETA=0:06:24
[02/27 21:50:14 d2.evaluation.evaluator]: Inference done 118/1250. Dataloading: 0.0023 s/iter. Inference: 0.2994 s/iter. Eval: 0.0325 s/iter. Total: 0.3343 s/iter. ETA=0:06:18
[02/27 21:50:20 d2.evaluation.evaluator]: Inference done 133/1250. Dataloading: 0.0022 s/iter. Inference: 0.3015 s/iter. Eval: 0.0321 s/iter. Total: 0.3359 s/iter. ETA=0:06:15
[02/27 21:50:25 d2.evaluation.evaluator]: Inference done 148/1250. Dataloading: 0.0023 s/iter. Inference: 0.3001 s/iter. Eval: 0.0332 s/iter. Total: 0.3357 s/iter. ETA=0:06:09
[02/27 21:50:30 d2.evaluation.evaluator]: Inference done 165/1250. Dataloading: 0.0023 s/iter. Inference: 0.2968 s/iter. Eval: 0.0330 s/iter. Total: 0.3322 s/iter. ETA=0:06:00
[02/27 21:50:35 d2.evaluation.evaluator]: Inference done 180/1250. Dataloading: 0.0024 s/iter. Inference: 0.2973 s/iter. Eval: 0.0334 s/iter. Total: 0.3331 s/iter. ETA=0:05:56
[02/27 21:50:40 d2.evaluation.evaluator]: Inference done 194/1250. Dataloading: 0.0023 s/iter. Inference: 0.2974 s/iter. Eval: 0.0356 s/iter. Total: 0.3356 s/iter. ETA=0:05:54
[02/27 21:50:45 d2.evaluation.evaluator]: Inference done 210/1250. Dataloading: 0.0023 s/iter. Inference: 0.2973 s/iter. Eval: 0.0343 s/iter. Total: 0.3341 s/iter. ETA=0:05:47
[02/27 21:50:50 d2.evaluation.evaluator]: Inference done 226/1250. Dataloading: 0.0023 s/iter. Inference: 0.2974 s/iter. Eval: 0.0336 s/iter. Total: 0.3334 s/iter. ETA=0:05:41
[02/27 21:50:56 d2.evaluation.evaluator]: Inference done 242/1250. Dataloading: 0.0025 s/iter. Inference: 0.2964 s/iter. Eval: 0.0342 s/iter. Total: 0.3332 s/iter. ETA=0:05:35
[02/27 21:51:01 d2.evaluation.evaluator]: Inference done 258/1250. Dataloading: 0.0024 s/iter. Inference: 0.2953 s/iter. Eval: 0.0341 s/iter. Total: 0.3320 s/iter. ETA=0:05:29
[02/27 21:51:06 d2.evaluation.evaluator]: Inference done 274/1250. Dataloading: 0.0024 s/iter. Inference: 0.2955 s/iter. Eval: 0.0335 s/iter. Total: 0.3315 s/iter. ETA=0:05:23
[02/27 21:51:11 d2.evaluation.evaluator]: Inference done 289/1250. Dataloading: 0.0024 s/iter. Inference: 0.2963 s/iter. Eval: 0.0338 s/iter. Total: 0.3326 s/iter. ETA=0:05:19
[02/27 21:51:16 d2.evaluation.evaluator]: Inference done 303/1250. Dataloading: 0.0024 s/iter. Inference: 0.2974 s/iter. Eval: 0.0340 s/iter. Total: 0.3339 s/iter. ETA=0:05:16
[02/27 21:51:21 d2.evaluation.evaluator]: Inference done 318/1250. Dataloading: 0.0024 s/iter. Inference: 0.2984 s/iter. Eval: 0.0336 s/iter. Total: 0.3345 s/iter. ETA=0:05:11
[02/27 21:51:27 d2.evaluation.evaluator]: Inference done 333/1250. Dataloading: 0.0024 s/iter. Inference: 0.2989 s/iter. Eval: 0.0339 s/iter. Total: 0.3353 s/iter. ETA=0:05:07
[02/27 21:51:32 d2.evaluation.evaluator]: Inference done 348/1250. Dataloading: 0.0024 s/iter. Inference: 0.2987 s/iter. Eval: 0.0342 s/iter. Total: 0.3353 s/iter. ETA=0:05:02
[02/27 21:51:37 d2.evaluation.evaluator]: Inference done 363/1250. Dataloading: 0.0024 s/iter. Inference: 0.2985 s/iter. Eval: 0.0346 s/iter. Total: 0.3356 s/iter. ETA=0:04:57
[02/27 21:51:42 d2.evaluation.evaluator]: Inference done 378/1250. Dataloading: 0.0024 s/iter. Inference: 0.2986 s/iter. Eval: 0.0348 s/iter. Total: 0.3359 s/iter. ETA=0:04:52
[02/27 21:51:47 d2.evaluation.evaluator]: Inference done 394/1250. Dataloading: 0.0024 s/iter. Inference: 0.2984 s/iter. Eval: 0.0344 s/iter. Total: 0.3354 s/iter. ETA=0:04:47
[02/27 21:51:52 d2.evaluation.evaluator]: Inference done 410/1250. Dataloading: 0.0024 s/iter. Inference: 0.2979 s/iter. Eval: 0.0345 s/iter. Total: 0.3350 s/iter. ETA=0:04:41
[02/27 21:51:57 d2.evaluation.evaluator]: Inference done 426/1250. Dataloading: 0.0024 s/iter. Inference: 0.2975 s/iter. Eval: 0.0341 s/iter. Total: 0.3342 s/iter. ETA=0:04:35
[02/27 21:52:02 d2.evaluation.evaluator]: Inference done 442/1250. Dataloading: 0.0025 s/iter. Inference: 0.2973 s/iter. Eval: 0.0340 s/iter. Total: 0.3338 s/iter. ETA=0:04:29
[02/27 21:52:08 d2.evaluation.evaluator]: Inference done 456/1250. Dataloading: 0.0025 s/iter. Inference: 0.2984 s/iter. Eval: 0.0338 s/iter. Total: 0.3348 s/iter. ETA=0:04:25
[02/27 21:52:13 d2.evaluation.evaluator]: Inference done 470/1250. Dataloading: 0.0025 s/iter. Inference: 0.3000 s/iter. Eval: 0.0335 s/iter. Total: 0.3360 s/iter. ETA=0:04:22
[02/27 21:52:18 d2.evaluation.evaluator]: Inference done 485/1250. Dataloading: 0.0025 s/iter. Inference: 0.2999 s/iter. Eval: 0.0339 s/iter. Total: 0.3364 s/iter. ETA=0:04:17
[02/27 21:52:23 d2.evaluation.evaluator]: Inference done 501/1250. Dataloading: 0.0025 s/iter. Inference: 0.2994 s/iter. Eval: 0.0339 s/iter. Total: 0.3360 s/iter. ETA=0:04:11
[02/27 21:52:28 d2.evaluation.evaluator]: Inference done 518/1250. Dataloading: 0.0025 s/iter. Inference: 0.2985 s/iter. Eval: 0.0338 s/iter. Total: 0.3349 s/iter. ETA=0:04:05
[02/27 21:52:34 d2.evaluation.evaluator]: Inference done 534/1250. Dataloading: 0.0025 s/iter. Inference: 0.2984 s/iter. Eval: 0.0336 s/iter. Total: 0.3346 s/iter. ETA=0:03:59
[02/27 21:52:39 d2.evaluation.evaluator]: Inference done 549/1250. Dataloading: 0.0025 s/iter. Inference: 0.2987 s/iter. Eval: 0.0334 s/iter. Total: 0.3347 s/iter. ETA=0:03:54
[02/27 21:52:44 d2.evaluation.evaluator]: Inference done 564/1250. Dataloading: 0.0025 s/iter. Inference: 0.2990 s/iter. Eval: 0.0336 s/iter. Total: 0.3352 s/iter. ETA=0:03:49
[02/27 21:52:49 d2.evaluation.evaluator]: Inference done 579/1250. Dataloading: 0.0026 s/iter. Inference: 0.2990 s/iter. Eval: 0.0336 s/iter. Total: 0.3354 s/iter. ETA=0:03:45
[02/27 21:52:54 d2.evaluation.evaluator]: Inference done 595/1250. Dataloading: 0.0026 s/iter. Inference: 0.2986 s/iter. Eval: 0.0338 s/iter. Total: 0.3351 s/iter. ETA=0:03:39
[02/27 21:52:59 d2.evaluation.evaluator]: Inference done 611/1250. Dataloading: 0.0027 s/iter. Inference: 0.2982 s/iter. Eval: 0.0339 s/iter. Total: 0.3348 s/iter. ETA=0:03:33
[02/27 21:53:05 d2.evaluation.evaluator]: Inference done 626/1250. Dataloading: 0.0027 s/iter. Inference: 0.2980 s/iter. Eval: 0.0343 s/iter. Total: 0.3351 s/iter. ETA=0:03:29
[02/27 21:53:10 d2.evaluation.evaluator]: Inference done 642/1250. Dataloading: 0.0027 s/iter. Inference: 0.2977 s/iter. Eval: 0.0345 s/iter. Total: 0.3350 s/iter. ETA=0:03:23
[02/27 21:53:15 d2.evaluation.evaluator]: Inference done 657/1250. Dataloading: 0.0027 s/iter. Inference: 0.2976 s/iter. Eval: 0.0346 s/iter. Total: 0.3351 s/iter. ETA=0:03:18
[02/27 21:53:20 d2.evaluation.evaluator]: Inference done 674/1250. Dataloading: 0.0027 s/iter. Inference: 0.2970 s/iter. Eval: 0.0345 s/iter. Total: 0.3344 s/iter. ETA=0:03:12
[02/27 21:53:25 d2.evaluation.evaluator]: Inference done 689/1250. Dataloading: 0.0027 s/iter. Inference: 0.2976 s/iter. Eval: 0.0342 s/iter. Total: 0.3346 s/iter. ETA=0:03:07
[02/27 21:53:31 d2.evaluation.evaluator]: Inference done 706/1250. Dataloading: 0.0027 s/iter. Inference: 0.2972 s/iter. Eval: 0.0340 s/iter. Total: 0.3339 s/iter. ETA=0:03:01
[02/27 21:53:36 d2.evaluation.evaluator]: Inference done 722/1250. Dataloading: 0.0027 s/iter. Inference: 0.2968 s/iter. Eval: 0.0342 s/iter. Total: 0.3337 s/iter. ETA=0:02:56
[02/27 21:53:41 d2.evaluation.evaluator]: Inference done 736/1250. Dataloading: 0.0027 s/iter. Inference: 0.2971 s/iter. Eval: 0.0345 s/iter. Total: 0.3344 s/iter. ETA=0:02:51
[02/27 21:53:46 d2.evaluation.evaluator]: Inference done 752/1250. Dataloading: 0.0027 s/iter. Inference: 0.2969 s/iter. Eval: 0.0343 s/iter. Total: 0.3340 s/iter. ETA=0:02:46
[02/27 21:53:51 d2.evaluation.evaluator]: Inference done 768/1250. Dataloading: 0.0027 s/iter. Inference: 0.2965 s/iter. Eval: 0.0343 s/iter. Total: 0.3336 s/iter. ETA=0:02:40
[02/27 21:53:56 d2.evaluation.evaluator]: Inference done 784/1250. Dataloading: 0.0027 s/iter. Inference: 0.2964 s/iter. Eval: 0.0341 s/iter. Total: 0.3333 s/iter. ETA=0:02:35
[02/27 21:54:01 d2.evaluation.evaluator]: Inference done 799/1250. Dataloading: 0.0027 s/iter. Inference: 0.2964 s/iter. Eval: 0.0342 s/iter. Total: 0.3334 s/iter. ETA=0:02:30
[02/27 21:54:06 d2.evaluation.evaluator]: Inference done 813/1250. Dataloading: 0.0028 s/iter. Inference: 0.2967 s/iter. Eval: 0.0343 s/iter. Total: 0.3338 s/iter. ETA=0:02:25
[02/27 21:54:11 d2.evaluation.evaluator]: Inference done 829/1250. Dataloading: 0.0027 s/iter. Inference: 0.2967 s/iter. Eval: 0.0339 s/iter. Total: 0.3335 s/iter. ETA=0:02:20
[02/27 21:54:16 d2.evaluation.evaluator]: Inference done 844/1250. Dataloading: 0.0027 s/iter. Inference: 0.2966 s/iter. Eval: 0.0340 s/iter. Total: 0.3335 s/iter. ETA=0:02:15
[02/27 21:54:22 d2.evaluation.evaluator]: Inference done 860/1250. Dataloading: 0.0027 s/iter. Inference: 0.2963 s/iter. Eval: 0.0341 s/iter. Total: 0.3332 s/iter. ETA=0:02:09
[02/27 21:54:27 d2.evaluation.evaluator]: Inference done 875/1250. Dataloading: 0.0027 s/iter. Inference: 0.2964 s/iter. Eval: 0.0340 s/iter. Total: 0.3333 s/iter. ETA=0:02:04
[02/27 21:54:32 d2.evaluation.evaluator]: Inference done 891/1250. Dataloading: 0.0027 s/iter. Inference: 0.2963 s/iter. Eval: 0.0340 s/iter. Total: 0.3331 s/iter. ETA=0:01:59
[02/27 21:54:37 d2.evaluation.evaluator]: Inference done 906/1250. Dataloading: 0.0027 s/iter. Inference: 0.2962 s/iter. Eval: 0.0343 s/iter. Total: 0.3334 s/iter. ETA=0:01:54
[02/27 21:54:42 d2.evaluation.evaluator]: Inference done 921/1250. Dataloading: 0.0027 s/iter. Inference: 0.2967 s/iter. Eval: 0.0341 s/iter. Total: 0.3336 s/iter. ETA=0:01:49
[02/27 21:54:47 d2.evaluation.evaluator]: Inference done 936/1250. Dataloading: 0.0027 s/iter. Inference: 0.2970 s/iter. Eval: 0.0340 s/iter. Total: 0.3338 s/iter. ETA=0:01:44
[02/27 21:54:52 d2.evaluation.evaluator]: Inference done 951/1250. Dataloading: 0.0027 s/iter. Inference: 0.2970 s/iter. Eval: 0.0341 s/iter. Total: 0.3339 s/iter. ETA=0:01:39
[02/27 21:54:58 d2.evaluation.evaluator]: Inference done 968/1250. Dataloading: 0.0028 s/iter. Inference: 0.2965 s/iter. Eval: 0.0342 s/iter. Total: 0.3336 s/iter. ETA=0:01:34
[02/27 21:55:03 d2.evaluation.evaluator]: Inference done 982/1250. Dataloading: 0.0028 s/iter. Inference: 0.2967 s/iter. Eval: 0.0344 s/iter. Total: 0.3340 s/iter. ETA=0:01:29
[02/27 21:55:08 d2.evaluation.evaluator]: Inference done 997/1250. Dataloading: 0.0028 s/iter. Inference: 0.2971 s/iter. Eval: 0.0343 s/iter. Total: 0.3343 s/iter. ETA=0:01:24
[02/27 21:55:13 d2.evaluation.evaluator]: Inference done 1012/1250. Dataloading: 0.0028 s/iter. Inference: 0.2971 s/iter. Eval: 0.0343 s/iter. Total: 0.3343 s/iter. ETA=0:01:19
[02/27 21:55:18 d2.evaluation.evaluator]: Inference done 1027/1250. Dataloading: 0.0027 s/iter. Inference: 0.2971 s/iter. Eval: 0.0345 s/iter. Total: 0.3345 s/iter. ETA=0:01:14
[02/27 21:55:24 d2.evaluation.evaluator]: Inference done 1043/1250. Dataloading: 0.0027 s/iter. Inference: 0.2969 s/iter. Eval: 0.0345 s/iter. Total: 0.3343 s/iter. ETA=0:01:09
[02/27 21:55:29 d2.evaluation.evaluator]: Inference done 1059/1250. Dataloading: 0.0027 s/iter. Inference: 0.2967 s/iter. Eval: 0.0346 s/iter. Total: 0.3341 s/iter. ETA=0:01:03
[02/27 21:55:34 d2.evaluation.evaluator]: Inference done 1074/1250. Dataloading: 0.0027 s/iter. Inference: 0.2969 s/iter. Eval: 0.0345 s/iter. Total: 0.3342 s/iter. ETA=0:00:58
[02/27 21:55:39 d2.evaluation.evaluator]: Inference done 1090/1250. Dataloading: 0.0027 s/iter. Inference: 0.2966 s/iter. Eval: 0.0345 s/iter. Total: 0.3340 s/iter. ETA=0:00:53
[02/27 21:55:44 d2.evaluation.evaluator]: Inference done 1105/1250. Dataloading: 0.0027 s/iter. Inference: 0.2970 s/iter. Eval: 0.0343 s/iter. Total: 0.3342 s/iter. ETA=0:00:48
[02/27 21:55:49 d2.evaluation.evaluator]: Inference done 1120/1250. Dataloading: 0.0027 s/iter. Inference: 0.2971 s/iter. Eval: 0.0343 s/iter. Total: 0.3343 s/iter. ETA=0:00:43
[02/27 21:55:55 d2.evaluation.evaluator]: Inference done 1136/1250. Dataloading: 0.0027 s/iter. Inference: 0.2971 s/iter. Eval: 0.0342 s/iter. Total: 0.3342 s/iter. ETA=0:00:38
[02/27 21:56:00 d2.evaluation.evaluator]: Inference done 1152/1250. Dataloading: 0.0027 s/iter. Inference: 0.2969 s/iter. Eval: 0.0342 s/iter. Total: 0.3340 s/iter. ETA=0:00:32
[02/27 21:56:05 d2.evaluation.evaluator]: Inference done 1168/1250. Dataloading: 0.0027 s/iter. Inference: 0.2968 s/iter. Eval: 0.0342 s/iter. Total: 0.3339 s/iter. ETA=0:00:27
[02/27 21:56:10 d2.evaluation.evaluator]: Inference done 1184/1250. Dataloading: 0.0027 s/iter. Inference: 0.2966 s/iter. Eval: 0.0343 s/iter. Total: 0.3337 s/iter. ETA=0:00:22
[02/27 21:56:15 d2.evaluation.evaluator]: Inference done 1200/1250. Dataloading: 0.0027 s/iter. Inference: 0.2964 s/iter. Eval: 0.0343 s/iter. Total: 0.3335 s/iter. ETA=0:00:16
[02/27 21:56:20 d2.evaluation.evaluator]: Inference done 1215/1250. Dataloading: 0.0027 s/iter. Inference: 0.2965 s/iter. Eval: 0.0344 s/iter. Total: 0.3337 s/iter. ETA=0:00:11
[02/27 21:56:26 d2.evaluation.evaluator]: Inference done 1232/1250. Dataloading: 0.0027 s/iter. Inference: 0.2962 s/iter. Eval: 0.0344 s/iter. Total: 0.3334 s/iter. ETA=0:00:06
[02/27 21:56:31 d2.evaluation.evaluator]: Inference done 1248/1250. Dataloading: 0.0027 s/iter. Inference: 0.2958 s/iter. Eval: 0.0346 s/iter. Total: 0.3333 s/iter. ETA=0:00:00
[02/27 21:56:32 d2.evaluation.evaluator]: Total inference time: 0:06:55.350388 (0.333615 s / iter per device, on 4 devices)
[02/27 21:56:32 d2.evaluation.evaluator]: Total inference pure compute time: 0:06:08 (0.295738 s / iter per device, on 4 devices)
[02/27 21:56:56 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[02/27 21:56:56 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
[02/27 21:56:56 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[02/27 21:57:03 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 7.13 seconds.
[02/27 21:57:03 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/27 21:57:04 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.69 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.080
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.069
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.037
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.084
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.115
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.328
[02/27 21:57:04 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:-----:|:------:|:------:|:-----:|:-----:|:------:|
| 7.982 | 16.430 | 6.930  | 3.656 | 8.368 | 11.523 |
[02/27 21:57:04 d2.evaluation.coco_evaluation]: Per-category bbox AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 27.850 | bicycle      | 3.274  | car            | 15.253 |
| motorcycle    | 11.597 | airplane     | 17.670 | bus            | 22.727 |
| train         | 19.603 | truck        | 5.028  | boat           | 2.919  |
| traffic light | 4.708  | fire hydrant | 24.116 | stop sign      | 30.824 |
| parking meter | 9.956  | bench        | 2.265  | bird           | 5.088  |
| cat           | 15.772 | dog          | 5.997  | horse          | 12.931 |
| sheep         | 12.539 | cow          | 12.756 | elephant       | 15.445 |
| bear          | 15.447 | zebra        | 25.176 | giraffe        | 23.589 |
| backpack      | 0.749  | umbrella     | 7.337  | handbag        | 0.232  |
| tie           | 4.906  | suitcase     | 1.007  | frisbee        | 7.197  |
| skis          | 2.228  | snowboard    | 0.269  | sports ball    | 17.427 |
| kite          | 11.901 | baseball bat | 1.877  | baseball glove | 3.007  |
| skateboard    | 4.874  | surfboard    | 2.571  | tennis racket  | 7.722  |
| bottle        | 6.697  | wine glass   | 2.367  | cup            | 7.806  |
| fork          | 0.057  | knife        | 0.303  | spoon          | 0.010  |
| bowl          | 8.055  | banana       | 0.490  | apple          | 1.425  |
| sandwich      | 3.052  | orange       | 7.550  | broccoli       | 3.685  |
| carrot        | 1.726  | hot dog      | 0.754  | pizza          | 15.876 |
| donut         | 3.353  | cake         | 2.707  | chair          | 2.165  |
| couch         | 7.218  | potted plant | 3.885  | bed            | 13.233 |
| dining table  | 11.229 | toilet       | 21.130 | tv             | 17.812 |
| laptop        | 7.424  | mouse        | 3.648  | remote         | 0.488  |
| keyboard      | 6.614  | cell phone   | 2.921  | microwave      | 11.670 |
| oven          | 5.139  | toaster      | 0.000  | sink           | 5.221  |
| refrigerator  | 8.350  | book         | 1.319  | clock          | 17.230 |
| vase          | 2.958  | scissors     | 0.000  | teddy bear     | 7.123  |
| hair drier    | 0.000  | toothbrush   | 0.000  |                |        |
Loading and preparing results...
DONE (t=1.58s)
creating index...
index created!
[02/27 21:57:10 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*
[02/27 21:57:18 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 8.46 seconds.
[02/27 21:57:18 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/27 21:57:19 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.74 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.081
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.077
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.023
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.079
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.129
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.138
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.335
[02/27 21:57:19 d2.evaluation.coco_evaluation]: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:-----:|:------:|:------:|:-----:|:-----:|:------:|
| 8.054 | 15.253 | 7.684  | 2.255 | 7.911 | 12.891 |
[02/27 21:57:19 d2.evaluation.coco_evaluation]: Per-category segm AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 23.431 | bicycle      | 1.530  | car            | 14.353 |
| motorcycle    | 6.779  | airplane     | 17.632 | bus            | 25.280 |
| train         | 23.293 | truck        | 5.426  | boat           | 2.766  |
| traffic light | 4.997  | fire hydrant | 27.632 | stop sign      | 36.940 |
| parking meter | 11.826 | bench        | 1.749  | bird           | 5.427  |
| cat           | 18.845 | dog          | 6.700  | horse          | 7.051  |
| sheep         | 10.096 | cow          | 11.552 | elephant       | 12.817 |
| bear          | 17.771 | zebra        | 19.927 | giraffe        | 19.060 |
| backpack      | 0.684  | umbrella     | 12.318 | handbag        | 0.476  |
| tie           | 5.027  | suitcase     | 1.167  | frisbee        | 8.437  |
| skis          | 0.316  | snowboard    | 0.415  | sports ball    | 20.126 |
| kite          | 9.998  | baseball bat | 1.533  | baseball glove | 4.247  |
| skateboard    | 2.009  | surfboard    | 3.398  | tennis racket  | 15.474 |
| bottle        | 7.226  | wine glass   | 1.558  | cup            | 8.723  |
| fork          | 0.000  | knife        | 0.067  | spoon          | 0.000  |
| bowl          | 7.695  | banana       | 0.459  | apple          | 1.585  |
| sandwich      | 3.065  | orange       | 8.472  | broccoli       | 4.114  |
| carrot        | 1.534  | hot dog      | 0.619  | pizza          | 16.802 |
| donut         | 3.725  | cake         | 2.771  | chair          | 1.342  |
| couch         | 4.865  | potted plant | 3.244  | bed            | 9.877  |
| dining table  | 4.521  | toilet       | 25.206 | tv             | 20.012 |
| laptop        | 9.782  | mouse        | 4.678  | remote         | 0.546  |
| keyboard      | 6.444  | cell phone   | 4.773  | microwave      | 11.452 |
| oven          | 4.743  | toaster      | 0.000  | sink           | 5.879  |
| refrigerator  | 9.240  | book         | 0.432  | clock          | 18.940 |
| vase          | 3.614  | scissors     | 0.000  | teddy bear     | 7.802  |
| hair drier    | 0.000  | toothbrush   | 0.000  |                |        |
[02/27 21:57:20 d2.evaluation.testing]: copypaste: Task: bbox
[02/27 21:57:20 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[02/27 21:57:20 d2.evaluation.testing]: copypaste: 7.9816,16.4296,6.9305,3.6558,8.3677,11.5232
[02/27 21:57:20 d2.evaluation.testing]: copypaste: Task: segm
[02/27 21:57:20 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[02/27 21:57:20 d2.evaluation.testing]: copypaste: 8.0539,15.2525,7.6841,2.2553,7.9106,12.8915
[02/27 21:57:20 d2.utils.events]:  eta: 1 day, 0:35:21  iter: 14999  total_loss: 1.216  loss_cls: 0.319  loss_box_reg: 0.2329  loss_mask: 0.3733  loss_rpn_cls: 0.05237  loss_rpn_loc: 0.1238  time: 0.5217  last_time: 0.5352  data_time: 0.0110  last_data_time: 0.0082   lr: 0.0001  max_mem: 11521M
[02/27 21:57:31 d2.utils.events]:  eta: 1 day, 0:35:58  iter: 15019  total_loss: 1.324  loss_cls: 0.419  loss_box_reg: 0.275  loss_mask: 0.3921  loss_rpn_cls: 0.06011  loss_rpn_loc: 0.1342  time: 0.5217  last_time: 0.5001  data_time: 0.0196  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 21:57:41 d2.utils.events]:  eta: 1 day, 0:34:23  iter: 15039  total_loss: 1.251  loss_cls: 0.3308  loss_box_reg: 0.2681  loss_mask: 0.3876  loss_rpn_cls: 0.08633  loss_rpn_loc: 0.1822  time: 0.5217  last_time: 0.5243  data_time: 0.0133  last_data_time: 0.0124   lr: 0.0001  max_mem: 11521M
[02/27 21:57:51 d2.utils.events]:  eta: 1 day, 0:34:13  iter: 15059  total_loss: 1.026  loss_cls: 0.2922  loss_box_reg: 0.1991  loss_mask: 0.3531  loss_rpn_cls: 0.05484  loss_rpn_loc: 0.09093  time: 0.5217  last_time: 0.5105  data_time: 0.0158  last_data_time: 0.0551   lr: 0.0001  max_mem: 11521M
[02/27 21:58:02 d2.utils.events]:  eta: 1 day, 0:34:58  iter: 15079  total_loss: 1.267  loss_cls: 0.3298  loss_box_reg: 0.2023  loss_mask: 0.4191  loss_rpn_cls: 0.06139  loss_rpn_loc: 0.206  time: 0.5217  last_time: 0.5199  data_time: 0.0231  last_data_time: 0.0130   lr: 0.0001  max_mem: 11521M
[02/27 21:58:12 d2.utils.events]:  eta: 1 day, 0:34:46  iter: 15099  total_loss: 1.227  loss_cls: 0.328  loss_box_reg: 0.2456  loss_mask: 0.3676  loss_rpn_cls: 0.06029  loss_rpn_loc: 0.1587  time: 0.5217  last_time: 0.5105  data_time: 0.0127  last_data_time: 0.0094   lr: 0.0001  max_mem: 11521M
[02/27 21:58:23 d2.utils.events]:  eta: 1 day, 0:35:05  iter: 15119  total_loss: 1.139  loss_cls: 0.3288  loss_box_reg: 0.2264  loss_mask: 0.3901  loss_rpn_cls: 0.0726  loss_rpn_loc: 0.1446  time: 0.5217  last_time: 0.5049  data_time: 0.0140  last_data_time: 0.0130   lr: 0.0001  max_mem: 11521M
[02/27 21:58:34 d2.utils.events]:  eta: 1 day, 0:35:29  iter: 15139  total_loss: 1.283  loss_cls: 0.3792  loss_box_reg: 0.2911  loss_mask: 0.3559  loss_rpn_cls: 0.06399  loss_rpn_loc: 0.117  time: 0.5217  last_time: 0.5002  data_time: 0.0206  last_data_time: 0.0109   lr: 0.0001  max_mem: 11521M
[02/27 21:58:44 d2.utils.events]:  eta: 1 day, 0:35:05  iter: 15159  total_loss: 0.9351  loss_cls: 0.2796  loss_box_reg: 0.1966  loss_mask: 0.3382  loss_rpn_cls: 0.03775  loss_rpn_loc: 0.05867  time: 0.5217  last_time: 0.5390  data_time: 0.0187  last_data_time: 0.0039   lr: 0.0001  max_mem: 11521M
[02/27 21:58:55 d2.utils.events]:  eta: 1 day, 0:35:28  iter: 15179  total_loss: 1.313  loss_cls: 0.3523  loss_box_reg: 0.276  loss_mask: 0.3981  loss_rpn_cls: 0.0689  loss_rpn_loc: 0.1643  time: 0.5217  last_time: 0.5394  data_time: 0.0132  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 21:59:05 d2.utils.events]:  eta: 1 day, 0:34:27  iter: 15199  total_loss: 1.137  loss_cls: 0.3318  loss_box_reg: 0.2602  loss_mask: 0.3643  loss_rpn_cls: 0.06301  loss_rpn_loc: 0.1427  time: 0.5217  last_time: 0.5455  data_time: 0.0228  last_data_time: 0.0666   lr: 0.0001  max_mem: 11521M
[02/27 21:59:16 d2.utils.events]:  eta: 1 day, 0:34:24  iter: 15219  total_loss: 1.228  loss_cls: 0.329  loss_box_reg: 0.2603  loss_mask: 0.3622  loss_rpn_cls: 0.06672  loss_rpn_loc: 0.1417  time: 0.5217  last_time: 0.5435  data_time: 0.0170  last_data_time: 0.0056   lr: 0.0001  max_mem: 11521M
[02/27 21:59:26 d2.utils.events]:  eta: 1 day, 0:34:37  iter: 15239  total_loss: 1.114  loss_cls: 0.3117  loss_box_reg: 0.2612  loss_mask: 0.3555  loss_rpn_cls: 0.05487  loss_rpn_loc: 0.07494  time: 0.5217  last_time: 0.5614  data_time: 0.0106  last_data_time: 0.0106   lr: 0.0001  max_mem: 11521M
[02/27 21:59:37 d2.utils.events]:  eta: 1 day, 0:33:52  iter: 15259  total_loss: 0.949  loss_cls: 0.2676  loss_box_reg: 0.2072  loss_mask: 0.336  loss_rpn_cls: 0.0572  loss_rpn_loc: 0.1056  time: 0.5217  last_time: 0.5172  data_time: 0.0140  last_data_time: 0.0066   lr: 0.0001  max_mem: 11521M
[02/27 21:59:47 d2.utils.events]:  eta: 1 day, 0:34:16  iter: 15279  total_loss: 1.111  loss_cls: 0.2912  loss_box_reg: 0.2372  loss_mask: 0.3783  loss_rpn_cls: 0.08176  loss_rpn_loc: 0.1158  time: 0.5217  last_time: 0.7166  data_time: 0.0174  last_data_time: 0.0028   lr: 0.0001  max_mem: 11521M
[02/27 21:59:58 d2.utils.events]:  eta: 1 day, 0:33:02  iter: 15299  total_loss: 1.058  loss_cls: 0.2788  loss_box_reg: 0.2341  loss_mask: 0.3647  loss_rpn_cls: 0.06169  loss_rpn_loc: 0.1435  time: 0.5217  last_time: 0.4911  data_time: 0.0112  last_data_time: 0.0150   lr: 0.0001  max_mem: 11521M
[02/27 22:00:08 d2.utils.events]:  eta: 1 day, 0:31:46  iter: 15319  total_loss: 1.123  loss_cls: 0.3273  loss_box_reg: 0.2372  loss_mask: 0.3481  loss_rpn_cls: 0.07103  loss_rpn_loc: 0.1182  time: 0.5217  last_time: 0.4890  data_time: 0.0140  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 22:00:19 d2.utils.events]:  eta: 1 day, 0:31:36  iter: 15339  total_loss: 1.214  loss_cls: 0.3594  loss_box_reg: 0.2789  loss_mask: 0.3658  loss_rpn_cls: 0.06953  loss_rpn_loc: 0.1317  time: 0.5217  last_time: 0.5620  data_time: 0.0209  last_data_time: 0.0673   lr: 0.0001  max_mem: 11521M
[02/27 22:00:29 d2.utils.events]:  eta: 1 day, 0:31:19  iter: 15359  total_loss: 1.22  loss_cls: 0.3946  loss_box_reg: 0.2832  loss_mask: 0.3354  loss_rpn_cls: 0.07241  loss_rpn_loc: 0.1117  time: 0.5217  last_time: 0.5035  data_time: 0.0135  last_data_time: 0.0435   lr: 0.0001  max_mem: 11521M
[02/27 22:00:39 d2.utils.events]:  eta: 1 day, 0:30:02  iter: 15379  total_loss: 1.247  loss_cls: 0.3967  loss_box_reg: 0.2851  loss_mask: 0.354  loss_rpn_cls: 0.06451  loss_rpn_loc: 0.119  time: 0.5217  last_time: 0.5344  data_time: 0.0173  last_data_time: 0.0121   lr: 0.0001  max_mem: 11521M
[02/27 22:00:49 d2.utils.events]:  eta: 1 day, 0:29:21  iter: 15399  total_loss: 1.317  loss_cls: 0.3673  loss_box_reg: 0.2875  loss_mask: 0.3642  loss_rpn_cls: 0.06333  loss_rpn_loc: 0.1273  time: 0.5217  last_time: 0.5115  data_time: 0.0165  last_data_time: 0.0104   lr: 0.0001  max_mem: 11521M
[02/27 22:01:00 d2.utils.events]:  eta: 1 day, 0:28:06  iter: 15419  total_loss: 1.127  loss_cls: 0.3138  loss_box_reg: 0.2191  loss_mask: 0.347  loss_rpn_cls: 0.06369  loss_rpn_loc: 0.1112  time: 0.5216  last_time: 0.5336  data_time: 0.0124  last_data_time: 0.0181   lr: 0.0001  max_mem: 11521M
[02/27 22:01:10 d2.utils.events]:  eta: 1 day, 0:27:22  iter: 15439  total_loss: 1.159  loss_cls: 0.3026  loss_box_reg: 0.2502  loss_mask: 0.3754  loss_rpn_cls: 0.0776  loss_rpn_loc: 0.1261  time: 0.5216  last_time: 0.5240  data_time: 0.0115  last_data_time: 0.0121   lr: 0.0001  max_mem: 11521M
[02/27 22:01:20 d2.utils.events]:  eta: 1 day, 0:26:32  iter: 15459  total_loss: 1.167  loss_cls: 0.351  loss_box_reg: 0.2276  loss_mask: 0.331  loss_rpn_cls: 0.08029  loss_rpn_loc: 0.08039  time: 0.5216  last_time: 0.5413  data_time: 0.0120  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 22:01:31 d2.utils.events]:  eta: 1 day, 0:25:40  iter: 15479  total_loss: 1.135  loss_cls: 0.2965  loss_box_reg: 0.2116  loss_mask: 0.3469  loss_rpn_cls: 0.06084  loss_rpn_loc: 0.1283  time: 0.5216  last_time: 0.5143  data_time: 0.0110  last_data_time: 0.0250   lr: 0.0001  max_mem: 11521M
[02/27 22:01:41 d2.utils.events]:  eta: 1 day, 0:24:38  iter: 15499  total_loss: 1.097  loss_cls: 0.2871  loss_box_reg: 0.2333  loss_mask: 0.3569  loss_rpn_cls: 0.06487  loss_rpn_loc: 0.1269  time: 0.5216  last_time: 0.4989  data_time: 0.0107  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 22:01:51 d2.utils.events]:  eta: 1 day, 0:23:26  iter: 15519  total_loss: 1.148  loss_cls: 0.2964  loss_box_reg: 0.2168  loss_mask: 0.364  loss_rpn_cls: 0.07212  loss_rpn_loc: 0.1321  time: 0.5216  last_time: 0.5044  data_time: 0.0190  last_data_time: 0.0044   lr: 0.0001  max_mem: 11521M
[02/27 22:02:02 d2.utils.events]:  eta: 1 day, 0:22:43  iter: 15539  total_loss: 1.318  loss_cls: 0.3315  loss_box_reg: 0.2281  loss_mask: 0.398  loss_rpn_cls: 0.06445  loss_rpn_loc: 0.1424  time: 0.5216  last_time: 0.5185  data_time: 0.0111  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 22:02:12 d2.utils.events]:  eta: 1 day, 0:23:05  iter: 15559  total_loss: 1.107  loss_cls: 0.3062  loss_box_reg: 0.2083  loss_mask: 0.3246  loss_rpn_cls: 0.05495  loss_rpn_loc: 0.1226  time: 0.5216  last_time: 0.4996  data_time: 0.0208  last_data_time: 0.0377   lr: 0.0001  max_mem: 11521M
[02/27 22:02:22 d2.utils.events]:  eta: 1 day, 0:21:25  iter: 15579  total_loss: 1.19  loss_cls: 0.3728  loss_box_reg: 0.2708  loss_mask: 0.3491  loss_rpn_cls: 0.06966  loss_rpn_loc: 0.1237  time: 0.5215  last_time: 0.5025  data_time: 0.0118  last_data_time: 0.0268   lr: 0.0001  max_mem: 11521M
[02/27 22:02:33 d2.utils.events]:  eta: 1 day, 0:21:38  iter: 15599  total_loss: 1.176  loss_cls: 0.3027  loss_box_reg: 0.235  loss_mask: 0.3606  loss_rpn_cls: 0.05144  loss_rpn_loc: 0.122  time: 0.5215  last_time: 0.5332  data_time: 0.0192  last_data_time: 0.0123   lr: 0.0001  max_mem: 11521M
[02/27 22:02:43 d2.utils.events]:  eta: 1 day, 0:21:28  iter: 15619  total_loss: 1.158  loss_cls: 0.3384  loss_box_reg: 0.2408  loss_mask: 0.4095  loss_rpn_cls: 0.06335  loss_rpn_loc: 0.1222  time: 0.5215  last_time: 0.5817  data_time: 0.0239  last_data_time: 0.0219   lr: 0.0001  max_mem: 11521M
[02/27 22:02:54 d2.utils.events]:  eta: 1 day, 0:22:12  iter: 15639  total_loss: 1.166  loss_cls: 0.3113  loss_box_reg: 0.2276  loss_mask: 0.3473  loss_rpn_cls: 0.06581  loss_rpn_loc: 0.1466  time: 0.5215  last_time: 0.5307  data_time: 0.0172  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 22:03:05 d2.utils.events]:  eta: 1 day, 0:21:26  iter: 15659  total_loss: 1.153  loss_cls: 0.3523  loss_box_reg: 0.2414  loss_mask: 0.3616  loss_rpn_cls: 0.06246  loss_rpn_loc: 0.1254  time: 0.5215  last_time: 0.5163  data_time: 0.0101  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 22:03:15 d2.utils.events]:  eta: 1 day, 0:21:15  iter: 15679  total_loss: 1.139  loss_cls: 0.3215  loss_box_reg: 0.2146  loss_mask: 0.3694  loss_rpn_cls: 0.04604  loss_rpn_loc: 0.1379  time: 0.5215  last_time: 0.4959  data_time: 0.0068  last_data_time: 0.0051   lr: 0.0001  max_mem: 11521M
[02/27 22:03:26 d2.utils.events]:  eta: 1 day, 0:21:05  iter: 15699  total_loss: 1.117  loss_cls: 0.2995  loss_box_reg: 0.2465  loss_mask: 0.3568  loss_rpn_cls: 0.05187  loss_rpn_loc: 0.1291  time: 0.5215  last_time: 0.5223  data_time: 0.0160  last_data_time: 0.0116   lr: 0.0001  max_mem: 11521M
[02/27 22:03:36 d2.utils.events]:  eta: 1 day, 0:20:17  iter: 15719  total_loss: 1.229  loss_cls: 0.3169  loss_box_reg: 0.2609  loss_mask: 0.363  loss_rpn_cls: 0.09218  loss_rpn_loc: 0.1594  time: 0.5215  last_time: 0.5210  data_time: 0.0142  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 22:03:47 d2.utils.events]:  eta: 1 day, 0:20:07  iter: 15739  total_loss: 1.164  loss_cls: 0.3733  loss_box_reg: 0.2301  loss_mask: 0.3843  loss_rpn_cls: 0.06795  loss_rpn_loc: 0.1284  time: 0.5215  last_time: 0.5528  data_time: 0.0185  last_data_time: 0.0552   lr: 0.0001  max_mem: 11521M
[02/27 22:03:57 d2.utils.events]:  eta: 1 day, 0:19:21  iter: 15759  total_loss: 1.213  loss_cls: 0.3361  loss_box_reg: 0.2737  loss_mask: 0.3582  loss_rpn_cls: 0.05976  loss_rpn_loc: 0.13  time: 0.5215  last_time: 0.4938  data_time: 0.0167  last_data_time: 0.0108   lr: 0.0001  max_mem: 11521M
[02/27 22:04:08 d2.utils.events]:  eta: 1 day, 0:19:15  iter: 15779  total_loss: 1.19  loss_cls: 0.3394  loss_box_reg: 0.2275  loss_mask: 0.3686  loss_rpn_cls: 0.07708  loss_rpn_loc: 0.1567  time: 0.5215  last_time: 0.5532  data_time: 0.0122  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 22:04:18 d2.utils.events]:  eta: 1 day, 0:19:41  iter: 15799  total_loss: 1.348  loss_cls: 0.3787  loss_box_reg: 0.2937  loss_mask: 0.3624  loss_rpn_cls: 0.07222  loss_rpn_loc: 0.176  time: 0.5215  last_time: 0.5514  data_time: 0.0133  last_data_time: 0.0054   lr: 0.0001  max_mem: 11521M
[02/27 22:04:28 d2.utils.events]:  eta: 1 day, 0:18:50  iter: 15819  total_loss: 1.088  loss_cls: 0.3411  loss_box_reg: 0.2382  loss_mask: 0.354  loss_rpn_cls: 0.06699  loss_rpn_loc: 0.1162  time: 0.5215  last_time: 0.5286  data_time: 0.0185  last_data_time: 0.0274   lr: 0.0001  max_mem: 11521M
[02/27 22:04:39 d2.utils.events]:  eta: 1 day, 0:17:31  iter: 15839  total_loss: 1.136  loss_cls: 0.2567  loss_box_reg: 0.2069  loss_mask: 0.3717  loss_rpn_cls: 0.06332  loss_rpn_loc: 0.159  time: 0.5215  last_time: 0.4976  data_time: 0.0131  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 22:04:49 d2.utils.events]:  eta: 1 day, 0:17:20  iter: 15859  total_loss: 1.172  loss_cls: 0.3126  loss_box_reg: 0.2553  loss_mask: 0.3334  loss_rpn_cls: 0.04567  loss_rpn_loc: 0.08237  time: 0.5215  last_time: 0.5267  data_time: 0.0203  last_data_time: 0.0097   lr: 0.0001  max_mem: 11521M
[02/27 22:05:00 d2.utils.events]:  eta: 1 day, 0:16:09  iter: 15879  total_loss: 1.237  loss_cls: 0.3263  loss_box_reg: 0.2369  loss_mask: 0.3438  loss_rpn_cls: 0.07039  loss_rpn_loc: 0.1797  time: 0.5215  last_time: 0.5200  data_time: 0.0175  last_data_time: 0.0155   lr: 0.0001  max_mem: 11521M
[02/27 22:05:10 d2.utils.events]:  eta: 1 day, 0:17:38  iter: 15899  total_loss: 1.364  loss_cls: 0.3719  loss_box_reg: 0.2876  loss_mask: 0.3983  loss_rpn_cls: 0.08484  loss_rpn_loc: 0.1507  time: 0.5215  last_time: 0.5097  data_time: 0.0202  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 22:05:21 d2.utils.events]:  eta: 1 day, 0:16:49  iter: 15919  total_loss: 1.03  loss_cls: 0.3141  loss_box_reg: 0.2007  loss_mask: 0.3542  loss_rpn_cls: 0.04917  loss_rpn_loc: 0.1006  time: 0.5215  last_time: 0.5349  data_time: 0.0138  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 22:05:31 d2.utils.events]:  eta: 1 day, 0:15:35  iter: 15939  total_loss: 1.201  loss_cls: 0.3266  loss_box_reg: 0.2037  loss_mask: 0.3638  loss_rpn_cls: 0.08332  loss_rpn_loc: 0.1788  time: 0.5215  last_time: 0.5421  data_time: 0.0119  last_data_time: 0.0125   lr: 0.0001  max_mem: 11521M
[02/27 22:05:41 d2.utils.events]:  eta: 1 day, 0:15:20  iter: 15959  total_loss: 1.022  loss_cls: 0.2651  loss_box_reg: 0.215  loss_mask: 0.3541  loss_rpn_cls: 0.06209  loss_rpn_loc: 0.1258  time: 0.5215  last_time: 0.5363  data_time: 0.0251  last_data_time: 0.0093   lr: 0.0001  max_mem: 11521M
[02/27 22:05:52 d2.utils.events]:  eta: 1 day, 0:15:02  iter: 15979  total_loss: 1.077  loss_cls: 0.2955  loss_box_reg: 0.2027  loss_mask: 0.3615  loss_rpn_cls: 0.05589  loss_rpn_loc: 0.1017  time: 0.5215  last_time: 0.5161  data_time: 0.0148  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 22:06:03 d2.utils.events]:  eta: 1 day, 0:14:59  iter: 15999  total_loss: 0.9977  loss_cls: 0.29  loss_box_reg: 0.2182  loss_mask: 0.3505  loss_rpn_cls: 0.05515  loss_rpn_loc: 0.07825  time: 0.5215  last_time: 0.5318  data_time: 0.0208  last_data_time: 0.0202   lr: 0.0001  max_mem: 11521M
[02/27 22:06:13 d2.utils.events]:  eta: 1 day, 0:14:41  iter: 16019  total_loss: 1.337  loss_cls: 0.444  loss_box_reg: 0.3019  loss_mask: 0.3728  loss_rpn_cls: 0.07548  loss_rpn_loc: 0.1116  time: 0.5215  last_time: 0.5399  data_time: 0.0135  last_data_time: 0.0032   lr: 0.0001  max_mem: 11521M
[02/27 22:06:24 d2.utils.events]:  eta: 1 day, 0:14:38  iter: 16039  total_loss: 1.126  loss_cls: 0.3086  loss_box_reg: 0.2317  loss_mask: 0.359  loss_rpn_cls: 0.07652  loss_rpn_loc: 0.07614  time: 0.5215  last_time: 0.5515  data_time: 0.0125  last_data_time: 0.0303   lr: 0.0001  max_mem: 11521M
[02/27 22:06:34 d2.utils.events]:  eta: 1 day, 0:13:37  iter: 16059  total_loss: 1.085  loss_cls: 0.3135  loss_box_reg: 0.2338  loss_mask: 0.3712  loss_rpn_cls: 0.06711  loss_rpn_loc: 0.1307  time: 0.5215  last_time: 0.5433  data_time: 0.0158  last_data_time: 0.0204   lr: 0.0001  max_mem: 11521M
[02/27 22:06:44 d2.utils.events]:  eta: 1 day, 0:12:36  iter: 16079  total_loss: 1.19  loss_cls: 0.3368  loss_box_reg: 0.2247  loss_mask: 0.3798  loss_rpn_cls: 0.05322  loss_rpn_loc: 0.1253  time: 0.5215  last_time: 0.5135  data_time: 0.0179  last_data_time: 0.0757   lr: 0.0001  max_mem: 11521M
[02/27 22:06:55 d2.utils.events]:  eta: 1 day, 0:13:00  iter: 16099  total_loss: 1.272  loss_cls: 0.3989  loss_box_reg: 0.2805  loss_mask: 0.3553  loss_rpn_cls: 0.05945  loss_rpn_loc: 0.1252  time: 0.5215  last_time: 0.5438  data_time: 0.0141  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 22:07:05 d2.utils.events]:  eta: 1 day, 0:12:50  iter: 16119  total_loss: 1.261  loss_cls: 0.3235  loss_box_reg: 0.2592  loss_mask: 0.3853  loss_rpn_cls: 0.06254  loss_rpn_loc: 0.1455  time: 0.5215  last_time: 0.5584  data_time: 0.0085  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 22:07:16 d2.utils.events]:  eta: 1 day, 0:12:16  iter: 16139  total_loss: 1.131  loss_cls: 0.3332  loss_box_reg: 0.2044  loss_mask: 0.3645  loss_rpn_cls: 0.06776  loss_rpn_loc: 0.1263  time: 0.5215  last_time: 0.5101  data_time: 0.0132  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 22:07:26 d2.utils.events]:  eta: 1 day, 0:12:19  iter: 16159  total_loss: 1.095  loss_cls: 0.3071  loss_box_reg: 0.2121  loss_mask: 0.371  loss_rpn_cls: 0.04815  loss_rpn_loc: 0.1241  time: 0.5215  last_time: 0.5523  data_time: 0.0235  last_data_time: 0.0638   lr: 0.0001  max_mem: 11521M
[02/27 22:07:37 d2.utils.events]:  eta: 1 day, 0:11:32  iter: 16179  total_loss: 1.189  loss_cls: 0.2995  loss_box_reg: 0.1946  loss_mask: 0.3879  loss_rpn_cls: 0.05954  loss_rpn_loc: 0.1269  time: 0.5215  last_time: 0.5097  data_time: 0.0163  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 22:07:48 d2.utils.events]:  eta: 1 day, 0:12:50  iter: 16199  total_loss: 1.17  loss_cls: 0.3356  loss_box_reg: 0.2153  loss_mask: 0.3844  loss_rpn_cls: 0.0496  loss_rpn_loc: 0.1528  time: 0.5215  last_time: 0.5457  data_time: 0.0101  last_data_time: 0.0099   lr: 0.0001  max_mem: 11521M
[02/27 22:07:58 d2.utils.events]:  eta: 1 day, 0:12:53  iter: 16219  total_loss: 1.165  loss_cls: 0.299  loss_box_reg: 0.216  loss_mask: 0.3652  loss_rpn_cls: 0.08467  loss_rpn_loc: 0.1373  time: 0.5215  last_time: 0.5283  data_time: 0.0167  last_data_time: 0.0179   lr: 0.0001  max_mem: 11521M
[02/27 22:08:08 d2.utils.events]:  eta: 1 day, 0:12:15  iter: 16239  total_loss: 1.234  loss_cls: 0.3571  loss_box_reg: 0.2692  loss_mask: 0.3661  loss_rpn_cls: 0.07203  loss_rpn_loc: 0.1248  time: 0.5215  last_time: 0.5679  data_time: 0.0222  last_data_time: 0.0031   lr: 0.0001  max_mem: 11521M
[02/27 22:08:19 d2.utils.events]:  eta: 1 day, 0:13:02  iter: 16259  total_loss: 1.088  loss_cls: 0.2944  loss_box_reg: 0.2217  loss_mask: 0.382  loss_rpn_cls: 0.05294  loss_rpn_loc: 0.1398  time: 0.5215  last_time: 0.5293  data_time: 0.0156  last_data_time: 0.0410   lr: 0.0001  max_mem: 11521M
[02/27 22:08:30 d2.utils.events]:  eta: 1 day, 0:13:25  iter: 16279  total_loss: 1.186  loss_cls: 0.3322  loss_box_reg: 0.2495  loss_mask: 0.3943  loss_rpn_cls: 0.06474  loss_rpn_loc: 0.159  time: 0.5215  last_time: 0.5217  data_time: 0.0205  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 22:08:40 d2.utils.events]:  eta: 1 day, 0:13:36  iter: 16299  total_loss: 1.094  loss_cls: 0.318  loss_box_reg: 0.2383  loss_mask: 0.3601  loss_rpn_cls: 0.06452  loss_rpn_loc: 0.1369  time: 0.5215  last_time: 0.5373  data_time: 0.0129  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 22:08:51 d2.utils.events]:  eta: 1 day, 0:14:13  iter: 16319  total_loss: 1.185  loss_cls: 0.336  loss_box_reg: 0.2439  loss_mask: 0.3704  loss_rpn_cls: 0.06196  loss_rpn_loc: 0.1093  time: 0.5215  last_time: 0.5407  data_time: 0.0158  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 22:09:02 d2.utils.events]:  eta: 1 day, 0:14:19  iter: 16339  total_loss: 1.323  loss_cls: 0.3745  loss_box_reg: 0.2845  loss_mask: 0.3657  loss_rpn_cls: 0.05831  loss_rpn_loc: 0.1739  time: 0.5215  last_time: 0.5169  data_time: 0.0231  last_data_time: 0.0049   lr: 0.0001  max_mem: 11521M
[02/27 22:09:12 d2.utils.events]:  eta: 1 day, 0:13:05  iter: 16359  total_loss: 1.158  loss_cls: 0.3069  loss_box_reg: 0.232  loss_mask: 0.3815  loss_rpn_cls: 0.05249  loss_rpn_loc: 0.1056  time: 0.5215  last_time: 0.4903  data_time: 0.0153  last_data_time: 0.0099   lr: 0.0001  max_mem: 11521M
[02/27 22:09:22 d2.utils.events]:  eta: 1 day, 0:13:12  iter: 16379  total_loss: 1.103  loss_cls: 0.3199  loss_box_reg: 0.2341  loss_mask: 0.3438  loss_rpn_cls: 0.05651  loss_rpn_loc: 0.1588  time: 0.5215  last_time: 0.5393  data_time: 0.0194  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 22:09:33 d2.utils.events]:  eta: 1 day, 0:13:54  iter: 16399  total_loss: 1.091  loss_cls: 0.3293  loss_box_reg: 0.2025  loss_mask: 0.3422  loss_rpn_cls: 0.05159  loss_rpn_loc: 0.1226  time: 0.5215  last_time: 0.5113  data_time: 0.0203  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 22:09:44 d2.utils.events]:  eta: 1 day, 0:15:37  iter: 16419  total_loss: 1.091  loss_cls: 0.3109  loss_box_reg: 0.2173  loss_mask: 0.332  loss_rpn_cls: 0.05287  loss_rpn_loc: 0.09175  time: 0.5215  last_time: 0.5242  data_time: 0.0189  last_data_time: 0.0267   lr: 0.0001  max_mem: 11521M
[02/27 22:09:54 d2.utils.events]:  eta: 1 day, 0:15:07  iter: 16439  total_loss: 1.01  loss_cls: 0.2522  loss_box_reg: 0.2014  loss_mask: 0.3543  loss_rpn_cls: 0.05565  loss_rpn_loc: 0.09125  time: 0.5215  last_time: 0.5206  data_time: 0.0129  last_data_time: 0.0258   lr: 0.0001  max_mem: 11521M
[02/27 22:10:05 d2.utils.events]:  eta: 1 day, 0:15:33  iter: 16459  total_loss: 1.241  loss_cls: 0.3761  loss_box_reg: 0.3065  loss_mask: 0.3596  loss_rpn_cls: 0.06471  loss_rpn_loc: 0.119  time: 0.5215  last_time: 0.5296  data_time: 0.0176  last_data_time: 0.0401   lr: 0.0001  max_mem: 11521M
[02/27 22:10:15 d2.utils.events]:  eta: 1 day, 0:15:22  iter: 16479  total_loss: 1.151  loss_cls: 0.339  loss_box_reg: 0.2647  loss_mask: 0.3264  loss_rpn_cls: 0.06915  loss_rpn_loc: 0.1072  time: 0.5215  last_time: 0.5128  data_time: 0.0188  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 22:10:26 d2.utils.events]:  eta: 1 day, 0:16:25  iter: 16499  total_loss: 1.277  loss_cls: 0.3597  loss_box_reg: 0.2517  loss_mask: 0.3607  loss_rpn_cls: 0.05867  loss_rpn_loc: 0.2133  time: 0.5215  last_time: 0.5026  data_time: 0.0148  last_data_time: 0.0027   lr: 0.0001  max_mem: 11521M
[02/27 22:10:36 d2.utils.events]:  eta: 1 day, 0:16:33  iter: 16519  total_loss: 1.067  loss_cls: 0.2917  loss_box_reg: 0.1994  loss_mask: 0.343  loss_rpn_cls: 0.06667  loss_rpn_loc: 0.1279  time: 0.5215  last_time: 0.5134  data_time: 0.0248  last_data_time: 0.0382   lr: 0.0001  max_mem: 11521M
[02/27 22:10:47 d2.utils.events]:  eta: 1 day, 0:16:47  iter: 16539  total_loss: 1.049  loss_cls: 0.2871  loss_box_reg: 0.2281  loss_mask: 0.3436  loss_rpn_cls: 0.06545  loss_rpn_loc: 0.1202  time: 0.5215  last_time: 0.4740  data_time: 0.0283  last_data_time: 0.0066   lr: 0.0001  max_mem: 11521M
[02/27 22:10:57 d2.utils.events]:  eta: 1 day, 0:16:28  iter: 16559  total_loss: 1.108  loss_cls: 0.3025  loss_box_reg: 0.2401  loss_mask: 0.363  loss_rpn_cls: 0.053  loss_rpn_loc: 0.1262  time: 0.5215  last_time: 0.5147  data_time: 0.0074  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 22:11:08 d2.utils.events]:  eta: 1 day, 0:17:16  iter: 16579  total_loss: 1.239  loss_cls: 0.3679  loss_box_reg: 0.2551  loss_mask: 0.3602  loss_rpn_cls: 0.06409  loss_rpn_loc: 0.1187  time: 0.5215  last_time: 0.5104  data_time: 0.0174  last_data_time: 0.0085   lr: 0.0001  max_mem: 11521M
[02/27 22:11:18 d2.utils.events]:  eta: 1 day, 0:15:59  iter: 16599  total_loss: 1.429  loss_cls: 0.3591  loss_box_reg: 0.2622  loss_mask: 0.3886  loss_rpn_cls: 0.09002  loss_rpn_loc: 0.1789  time: 0.5215  last_time: 0.5105  data_time: 0.0176  last_data_time: 0.0125   lr: 0.0001  max_mem: 11521M
[02/27 22:11:29 d2.utils.events]:  eta: 1 day, 0:15:52  iter: 16619  total_loss: 1.165  loss_cls: 0.3012  loss_box_reg: 0.2461  loss_mask: 0.3626  loss_rpn_cls: 0.05661  loss_rpn_loc: 0.09065  time: 0.5215  last_time: 0.5294  data_time: 0.0163  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 22:11:39 d2.utils.events]:  eta: 1 day, 0:15:27  iter: 16639  total_loss: 1.218  loss_cls: 0.337  loss_box_reg: 0.2739  loss_mask: 0.3428  loss_rpn_cls: 0.05494  loss_rpn_loc: 0.1467  time: 0.5215  last_time: 0.5447  data_time: 0.0242  last_data_time: 0.0024   lr: 0.0001  max_mem: 11521M
[02/27 22:11:50 d2.utils.events]:  eta: 1 day, 0:15:08  iter: 16659  total_loss: 1.268  loss_cls: 0.3831  loss_box_reg: 0.2667  loss_mask: 0.3866  loss_rpn_cls: 0.07048  loss_rpn_loc: 0.1486  time: 0.5215  last_time: 0.4946  data_time: 0.0145  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 22:12:00 d2.utils.events]:  eta: 1 day, 0:15:21  iter: 16679  total_loss: 1.178  loss_cls: 0.3546  loss_box_reg: 0.2275  loss_mask: 0.3318  loss_rpn_cls: 0.06819  loss_rpn_loc: 0.08841  time: 0.5215  last_time: 0.5022  data_time: 0.0218  last_data_time: 0.0272   lr: 0.0001  max_mem: 11521M
[02/27 22:12:10 d2.utils.events]:  eta: 1 day, 0:14:02  iter: 16699  total_loss: 1.186  loss_cls: 0.3604  loss_box_reg: 0.248  loss_mask: 0.3965  loss_rpn_cls: 0.04844  loss_rpn_loc: 0.1292  time: 0.5215  last_time: 0.5203  data_time: 0.0134  last_data_time: 0.0147   lr: 0.0001  max_mem: 11521M
[02/27 22:12:21 d2.utils.events]:  eta: 1 day, 0:13:31  iter: 16719  total_loss: 1.253  loss_cls: 0.3546  loss_box_reg: 0.2524  loss_mask: 0.3759  loss_rpn_cls: 0.06118  loss_rpn_loc: 0.1636  time: 0.5215  last_time: 0.5435  data_time: 0.0117  last_data_time: 0.0144   lr: 0.0001  max_mem: 11521M
[02/27 22:12:31 d2.utils.events]:  eta: 1 day, 0:12:15  iter: 16739  total_loss: 1.389  loss_cls: 0.3813  loss_box_reg: 0.2277  loss_mask: 0.4471  loss_rpn_cls: 0.06906  loss_rpn_loc: 0.1793  time: 0.5215  last_time: 0.5463  data_time: 0.0212  last_data_time: 0.0053   lr: 0.0001  max_mem: 11521M
[02/27 22:12:42 d2.utils.events]:  eta: 1 day, 0:13:10  iter: 16759  total_loss: 1.121  loss_cls: 0.3166  loss_box_reg: 0.2282  loss_mask: 0.3881  loss_rpn_cls: 0.06977  loss_rpn_loc: 0.1168  time: 0.5215  last_time: 0.5173  data_time: 0.0171  last_data_time: 0.0547   lr: 0.0001  max_mem: 11521M
[02/27 22:12:52 d2.utils.events]:  eta: 1 day, 0:12:42  iter: 16779  total_loss: 1.4  loss_cls: 0.389  loss_box_reg: 0.2896  loss_mask: 0.404  loss_rpn_cls: 0.07185  loss_rpn_loc: 0.1501  time: 0.5215  last_time: 0.4987  data_time: 0.0268  last_data_time: 0.0052   lr: 0.0001  max_mem: 11521M
[02/27 22:13:03 d2.utils.events]:  eta: 1 day, 0:12:07  iter: 16799  total_loss: 1.286  loss_cls: 0.4069  loss_box_reg: 0.3114  loss_mask: 0.3972  loss_rpn_cls: 0.07762  loss_rpn_loc: 0.1004  time: 0.5215  last_time: 0.5039  data_time: 0.0132  last_data_time: 0.0040   lr: 0.0001  max_mem: 11521M
[02/27 22:13:13 d2.utils.events]:  eta: 1 day, 0:12:39  iter: 16819  total_loss: 1.147  loss_cls: 0.3011  loss_box_reg: 0.2134  loss_mask: 0.3714  loss_rpn_cls: 0.06352  loss_rpn_loc: 0.176  time: 0.5215  last_time: 0.5059  data_time: 0.0143  last_data_time: 0.0097   lr: 0.0001  max_mem: 11521M
[02/27 22:13:24 d2.utils.events]:  eta: 1 day, 0:13:01  iter: 16839  total_loss: 1.04  loss_cls: 0.2515  loss_box_reg: 0.1908  loss_mask: 0.3742  loss_rpn_cls: 0.0493  loss_rpn_loc: 0.1538  time: 0.5215  last_time: 0.5424  data_time: 0.0151  last_data_time: 0.0744   lr: 0.0001  max_mem: 11521M
[02/27 22:13:34 d2.utils.events]:  eta: 1 day, 0:13:09  iter: 16859  total_loss: 1.127  loss_cls: 0.3105  loss_box_reg: 0.2068  loss_mask: 0.3786  loss_rpn_cls: 0.06929  loss_rpn_loc: 0.09667  time: 0.5215  last_time: 0.5209  data_time: 0.0173  last_data_time: 0.0797   lr: 0.0001  max_mem: 11521M
[02/27 22:13:45 d2.utils.events]:  eta: 1 day, 0:14:06  iter: 16879  total_loss: 1.201  loss_cls: 0.264  loss_box_reg: 0.201  loss_mask: 0.4101  loss_rpn_cls: 0.06219  loss_rpn_loc: 0.1981  time: 0.5215  last_time: 0.5302  data_time: 0.0183  last_data_time: 0.0699   lr: 0.0001  max_mem: 11521M
[02/27 22:13:55 d2.utils.events]:  eta: 1 day, 0:13:17  iter: 16899  total_loss: 1.136  loss_cls: 0.3429  loss_box_reg: 0.2696  loss_mask: 0.3274  loss_rpn_cls: 0.05799  loss_rpn_loc: 0.1395  time: 0.5215  last_time: 0.4857  data_time: 0.0274  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 22:14:06 d2.utils.events]:  eta: 1 day, 0:12:58  iter: 16919  total_loss: 1.125  loss_cls: 0.3191  loss_box_reg: 0.2541  loss_mask: 0.3827  loss_rpn_cls: 0.05268  loss_rpn_loc: 0.1156  time: 0.5215  last_time: 0.4992  data_time: 0.0161  last_data_time: 0.0183   lr: 0.0001  max_mem: 11521M
[02/27 22:14:16 d2.utils.events]:  eta: 1 day, 0:12:56  iter: 16939  total_loss: 1.189  loss_cls: 0.3728  loss_box_reg: 0.2705  loss_mask: 0.3828  loss_rpn_cls: 0.06141  loss_rpn_loc: 0.1194  time: 0.5215  last_time: 0.5021  data_time: 0.0175  last_data_time: 0.0112   lr: 0.0001  max_mem: 11521M
[02/27 22:14:27 d2.utils.events]:  eta: 1 day, 0:12:57  iter: 16959  total_loss: 1.029  loss_cls: 0.2886  loss_box_reg: 0.2168  loss_mask: 0.3287  loss_rpn_cls: 0.05291  loss_rpn_loc: 0.1216  time: 0.5215  last_time: 0.5264  data_time: 0.0178  last_data_time: 0.0052   lr: 0.0001  max_mem: 11521M
[02/27 22:14:38 d2.utils.events]:  eta: 1 day, 0:12:55  iter: 16979  total_loss: 1.014  loss_cls: 0.2977  loss_box_reg: 0.1995  loss_mask: 0.3784  loss_rpn_cls: 0.04902  loss_rpn_loc: 0.08367  time: 0.5215  last_time: 0.5057  data_time: 0.0195  last_data_time: 0.0531   lr: 0.0001  max_mem: 11521M
[02/27 22:14:48 d2.utils.events]:  eta: 1 day, 0:12:11  iter: 16999  total_loss: 1.166  loss_cls: 0.3284  loss_box_reg: 0.2512  loss_mask: 0.3446  loss_rpn_cls: 0.07032  loss_rpn_loc: 0.1201  time: 0.5215  last_time: 0.5301  data_time: 0.0113  last_data_time: 0.0125   lr: 0.0001  max_mem: 11521M
[02/27 22:14:59 d2.utils.events]:  eta: 1 day, 0:12:00  iter: 17019  total_loss: 1.143  loss_cls: 0.323  loss_box_reg: 0.2668  loss_mask: 0.3783  loss_rpn_cls: 0.06186  loss_rpn_loc: 0.08963  time: 0.5215  last_time: 0.5357  data_time: 0.0154  last_data_time: 0.0405   lr: 0.0001  max_mem: 11521M
[02/27 22:15:09 d2.utils.events]:  eta: 1 day, 0:11:56  iter: 17039  total_loss: 1.265  loss_cls: 0.3619  loss_box_reg: 0.2498  loss_mask: 0.3605  loss_rpn_cls: 0.07202  loss_rpn_loc: 0.1321  time: 0.5215  last_time: 0.5035  data_time: 0.0227  last_data_time: 0.0052   lr: 0.0001  max_mem: 11521M
[02/27 22:15:20 d2.utils.events]:  eta: 1 day, 0:12:29  iter: 17059  total_loss: 1.231  loss_cls: 0.3735  loss_box_reg: 0.2864  loss_mask: 0.3437  loss_rpn_cls: 0.07547  loss_rpn_loc: 0.1418  time: 0.5215  last_time: 0.5037  data_time: 0.0222  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 22:15:30 d2.utils.events]:  eta: 1 day, 0:12:14  iter: 17079  total_loss: 1.329  loss_cls: 0.3775  loss_box_reg: 0.2592  loss_mask: 0.369  loss_rpn_cls: 0.05996  loss_rpn_loc: 0.1624  time: 0.5215  last_time: 0.4858  data_time: 0.0167  last_data_time: 0.0243   lr: 0.0001  max_mem: 11521M
[02/27 22:15:41 d2.utils.events]:  eta: 1 day, 0:11:53  iter: 17099  total_loss: 1.176  loss_cls: 0.3162  loss_box_reg: 0.2204  loss_mask: 0.376  loss_rpn_cls: 0.06084  loss_rpn_loc: 0.1126  time: 0.5215  last_time: 0.5008  data_time: 0.0240  last_data_time: 0.0058   lr: 0.0001  max_mem: 11521M
[02/27 22:15:51 d2.utils.events]:  eta: 1 day, 0:11:53  iter: 17119  total_loss: 1.128  loss_cls: 0.3258  loss_box_reg: 0.241  loss_mask: 0.3718  loss_rpn_cls: 0.05325  loss_rpn_loc: 0.1134  time: 0.5215  last_time: 0.5417  data_time: 0.0252  last_data_time: 0.0821   lr: 0.0001  max_mem: 11521M
[02/27 22:16:02 d2.utils.events]:  eta: 1 day, 0:11:12  iter: 17139  total_loss: 1.058  loss_cls: 0.286  loss_box_reg: 0.219  loss_mask: 0.3802  loss_rpn_cls: 0.06262  loss_rpn_loc: 0.1016  time: 0.5215  last_time: 0.5165  data_time: 0.0163  last_data_time: 0.0093   lr: 0.0001  max_mem: 11521M
[02/27 22:16:12 d2.utils.events]:  eta: 1 day, 0:11:13  iter: 17159  total_loss: 1.193  loss_cls: 0.317  loss_box_reg: 0.256  loss_mask: 0.3434  loss_rpn_cls: 0.08168  loss_rpn_loc: 0.1111  time: 0.5215  last_time: 0.5422  data_time: 0.0082  last_data_time: 0.0324   lr: 0.0001  max_mem: 11521M
[02/27 22:16:23 d2.utils.events]:  eta: 1 day, 0:11:22  iter: 17179  total_loss: 1.101  loss_cls: 0.3312  loss_box_reg: 0.2355  loss_mask: 0.3198  loss_rpn_cls: 0.05623  loss_rpn_loc: 0.08803  time: 0.5215  last_time: 0.5156  data_time: 0.0210  last_data_time: 0.0055   lr: 0.0001  max_mem: 11521M
[02/27 22:16:33 d2.utils.events]:  eta: 1 day, 0:11:01  iter: 17199  total_loss: 1.098  loss_cls: 0.3298  loss_box_reg: 0.2156  loss_mask: 0.3483  loss_rpn_cls: 0.05189  loss_rpn_loc: 0.1  time: 0.5215  last_time: 0.5729  data_time: 0.0165  last_data_time: 0.0036   lr: 0.0001  max_mem: 11521M
[02/27 22:16:44 d2.utils.events]:  eta: 1 day, 0:11:19  iter: 17219  total_loss: 1.23  loss_cls: 0.3742  loss_box_reg: 0.2485  loss_mask: 0.357  loss_rpn_cls: 0.0671  loss_rpn_loc: 0.1745  time: 0.5215  last_time: 0.5150  data_time: 0.0177  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 22:16:55 d2.utils.events]:  eta: 1 day, 0:11:03  iter: 17239  total_loss: 1.335  loss_cls: 0.3537  loss_box_reg: 0.2577  loss_mask: 0.3985  loss_rpn_cls: 0.07749  loss_rpn_loc: 0.204  time: 0.5215  last_time: 0.4855  data_time: 0.0181  last_data_time: 0.0125   lr: 0.0001  max_mem: 11521M
[02/27 22:17:05 d2.utils.events]:  eta: 1 day, 0:10:24  iter: 17259  total_loss: 1.129  loss_cls: 0.2949  loss_box_reg: 0.2334  loss_mask: 0.3371  loss_rpn_cls: 0.07205  loss_rpn_loc: 0.1358  time: 0.5215  last_time: 0.5445  data_time: 0.0181  last_data_time: 0.0064   lr: 0.0001  max_mem: 11521M
[02/27 22:17:16 d2.utils.events]:  eta: 1 day, 0:10:13  iter: 17279  total_loss: 1.123  loss_cls: 0.3629  loss_box_reg: 0.3053  loss_mask: 0.3581  loss_rpn_cls: 0.05619  loss_rpn_loc: 0.1077  time: 0.5215  last_time: 0.5756  data_time: 0.0126  last_data_time: 0.0110   lr: 0.0001  max_mem: 11521M
[02/27 22:17:27 d2.utils.events]:  eta: 1 day, 0:10:27  iter: 17299  total_loss: 1.058  loss_cls: 0.3045  loss_box_reg: 0.2115  loss_mask: 0.3649  loss_rpn_cls: 0.07553  loss_rpn_loc: 0.1247  time: 0.5215  last_time: 0.5195  data_time: 0.0260  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 22:17:37 d2.utils.events]:  eta: 1 day, 0:09:43  iter: 17319  total_loss: 0.9783  loss_cls: 0.3018  loss_box_reg: 0.2088  loss_mask: 0.3289  loss_rpn_cls: 0.05588  loss_rpn_loc: 0.1032  time: 0.5215  last_time: 0.5229  data_time: 0.0134  last_data_time: 0.0108   lr: 0.0001  max_mem: 11521M
[02/27 22:17:48 d2.utils.events]:  eta: 1 day, 0:09:53  iter: 17339  total_loss: 1.055  loss_cls: 0.2936  loss_box_reg: 0.2221  loss_mask: 0.3745  loss_rpn_cls: 0.05057  loss_rpn_loc: 0.08977  time: 0.5215  last_time: 0.5033  data_time: 0.0205  last_data_time: 0.0048   lr: 0.0001  max_mem: 11521M
[02/27 22:17:58 d2.utils.events]:  eta: 1 day, 0:10:01  iter: 17359  total_loss: 1.086  loss_cls: 0.2945  loss_box_reg: 0.238  loss_mask: 0.3416  loss_rpn_cls: 0.05396  loss_rpn_loc: 0.1491  time: 0.5215  last_time: 0.5100  data_time: 0.0161  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 22:18:09 d2.utils.events]:  eta: 1 day, 0:09:55  iter: 17379  total_loss: 1.141  loss_cls: 0.3249  loss_box_reg: 0.2358  loss_mask: 0.3617  loss_rpn_cls: 0.05302  loss_rpn_loc: 0.1163  time: 0.5215  last_time: 0.5410  data_time: 0.0263  last_data_time: 0.0093   lr: 0.0001  max_mem: 11521M
[02/27 22:18:19 d2.utils.events]:  eta: 1 day, 0:09:11  iter: 17399  total_loss: 1.191  loss_cls: 0.3397  loss_box_reg: 0.2526  loss_mask: 0.3565  loss_rpn_cls: 0.06928  loss_rpn_loc: 0.1581  time: 0.5215  last_time: 0.5179  data_time: 0.0159  last_data_time: 0.0600   lr: 0.0001  max_mem: 11521M
[02/27 22:18:29 d2.utils.events]:  eta: 1 day, 0:07:51  iter: 17419  total_loss: 1.166  loss_cls: 0.2831  loss_box_reg: 0.1984  loss_mask: 0.3614  loss_rpn_cls: 0.07185  loss_rpn_loc: 0.1819  time: 0.5215  last_time: 0.5163  data_time: 0.0208  last_data_time: 0.0282   lr: 0.0001  max_mem: 11521M
[02/27 22:18:40 d2.utils.events]:  eta: 1 day, 0:08:26  iter: 17439  total_loss: 1.061  loss_cls: 0.3007  loss_box_reg: 0.2021  loss_mask: 0.3855  loss_rpn_cls: 0.0519  loss_rpn_loc: 0.1136  time: 0.5215  last_time: 0.5168  data_time: 0.0174  last_data_time: 0.0280   lr: 0.0001  max_mem: 11521M
[02/27 22:18:50 d2.utils.events]:  eta: 1 day, 0:07:51  iter: 17459  total_loss: 1.075  loss_cls: 0.3167  loss_box_reg: 0.2236  loss_mask: 0.3423  loss_rpn_cls: 0.05506  loss_rpn_loc: 0.1185  time: 0.5215  last_time: 0.5098  data_time: 0.0216  last_data_time: 0.0481   lr: 0.0001  max_mem: 11521M
[02/27 22:19:01 d2.utils.events]:  eta: 1 day, 0:08:50  iter: 17479  total_loss: 1.176  loss_cls: 0.3172  loss_box_reg: 0.2153  loss_mask: 0.3802  loss_rpn_cls: 0.06009  loss_rpn_loc: 0.1429  time: 0.5215  last_time: 0.5096  data_time: 0.0264  last_data_time: 0.0257   lr: 0.0001  max_mem: 11521M
[02/27 22:19:12 d2.utils.events]:  eta: 1 day, 0:08:10  iter: 17499  total_loss: 1.109  loss_cls: 0.3317  loss_box_reg: 0.2473  loss_mask: 0.3594  loss_rpn_cls: 0.04982  loss_rpn_loc: 0.1146  time: 0.5215  last_time: 0.5062  data_time: 0.0152  last_data_time: 0.0267   lr: 0.0001  max_mem: 11521M
[02/27 22:19:22 d2.utils.events]:  eta: 1 day, 0:06:54  iter: 17519  total_loss: 1.156  loss_cls: 0.332  loss_box_reg: 0.2488  loss_mask: 0.4169  loss_rpn_cls: 0.06725  loss_rpn_loc: 0.1711  time: 0.5215  last_time: 0.5353  data_time: 0.0153  last_data_time: 0.0090   lr: 0.0001  max_mem: 11521M
[02/27 22:19:32 d2.utils.events]:  eta: 1 day, 0:06:40  iter: 17539  total_loss: 1.083  loss_cls: 0.3177  loss_box_reg: 0.2156  loss_mask: 0.3631  loss_rpn_cls: 0.06186  loss_rpn_loc: 0.1305  time: 0.5215  last_time: 0.5548  data_time: 0.0152  last_data_time: 0.0395   lr: 0.0001  max_mem: 11521M
[02/27 22:19:43 d2.utils.events]:  eta: 1 day, 0:07:33  iter: 17559  total_loss: 1.162  loss_cls: 0.3455  loss_box_reg: 0.2184  loss_mask: 0.3174  loss_rpn_cls: 0.06749  loss_rpn_loc: 0.1254  time: 0.5215  last_time: 0.5081  data_time: 0.0170  last_data_time: 0.0099   lr: 0.0001  max_mem: 11521M
[02/27 22:19:54 d2.utils.events]:  eta: 1 day, 0:07:54  iter: 17579  total_loss: 1.056  loss_cls: 0.2838  loss_box_reg: 0.1721  loss_mask: 0.3954  loss_rpn_cls: 0.05366  loss_rpn_loc: 0.1048  time: 0.5215  last_time: 0.5127  data_time: 0.0318  last_data_time: 0.0107   lr: 0.0001  max_mem: 11521M
[02/27 22:20:04 d2.utils.events]:  eta: 1 day, 0:09:19  iter: 17599  total_loss: 1.267  loss_cls: 0.3711  loss_box_reg: 0.2716  loss_mask: 0.3697  loss_rpn_cls: 0.05912  loss_rpn_loc: 0.1185  time: 0.5215  last_time: 0.4843  data_time: 0.0141  last_data_time: 0.0281   lr: 0.0001  max_mem: 11521M
[02/27 22:20:15 d2.utils.events]:  eta: 1 day, 0:08:51  iter: 17619  total_loss: 1.3  loss_cls: 0.3471  loss_box_reg: 0.2483  loss_mask: 0.3973  loss_rpn_cls: 0.07484  loss_rpn_loc: 0.1704  time: 0.5215  last_time: 0.4985  data_time: 0.0150  last_data_time: 0.0034   lr: 0.0001  max_mem: 11521M
[02/27 22:20:25 d2.utils.events]:  eta: 1 day, 0:08:05  iter: 17639  total_loss: 1.096  loss_cls: 0.2853  loss_box_reg: 0.2089  loss_mask: 0.3398  loss_rpn_cls: 0.0557  loss_rpn_loc: 0.09687  time: 0.5215  last_time: 0.5788  data_time: 0.0125  last_data_time: 0.0037   lr: 0.0001  max_mem: 11521M
[02/27 22:20:36 d2.utils.events]:  eta: 1 day, 0:08:53  iter: 17659  total_loss: 1.328  loss_cls: 0.4089  loss_box_reg: 0.2996  loss_mask: 0.3583  loss_rpn_cls: 0.05992  loss_rpn_loc: 0.118  time: 0.5215  last_time: 0.5438  data_time: 0.0163  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 22:20:46 d2.utils.events]:  eta: 1 day, 0:08:19  iter: 17679  total_loss: 1.192  loss_cls: 0.3212  loss_box_reg: 0.2501  loss_mask: 0.3582  loss_rpn_cls: 0.05361  loss_rpn_loc: 0.1064  time: 0.5215  last_time: 0.5058  data_time: 0.0201  last_data_time: 0.0105   lr: 0.0001  max_mem: 11521M
[02/27 22:20:57 d2.utils.events]:  eta: 1 day, 0:09:38  iter: 17699  total_loss: 1.183  loss_cls: 0.333  loss_box_reg: 0.2697  loss_mask: 0.3466  loss_rpn_cls: 0.06866  loss_rpn_loc: 0.1488  time: 0.5215  last_time: 0.5309  data_time: 0.0238  last_data_time: 0.0126   lr: 0.0001  max_mem: 11521M
[02/27 22:21:07 d2.utils.events]:  eta: 1 day, 0:10:07  iter: 17719  total_loss: 1.164  loss_cls: 0.3581  loss_box_reg: 0.2438  loss_mask: 0.4006  loss_rpn_cls: 0.071  loss_rpn_loc: 0.1212  time: 0.5215  last_time: 0.4976  data_time: 0.0165  last_data_time: 0.0051   lr: 0.0001  max_mem: 11521M
[02/27 22:21:18 d2.utils.events]:  eta: 1 day, 0:10:35  iter: 17739  total_loss: 1.319  loss_cls: 0.3386  loss_box_reg: 0.2533  loss_mask: 0.3375  loss_rpn_cls: 0.08247  loss_rpn_loc: 0.1268  time: 0.5215  last_time: 0.5191  data_time: 0.0243  last_data_time: 0.0262   lr: 0.0001  max_mem: 11521M
[02/27 22:21:28 d2.utils.events]:  eta: 1 day, 0:09:46  iter: 17759  total_loss: 1.172  loss_cls: 0.3724  loss_box_reg: 0.2491  loss_mask: 0.355  loss_rpn_cls: 0.06089  loss_rpn_loc: 0.1166  time: 0.5215  last_time: 0.5123  data_time: 0.0218  last_data_time: 0.0525   lr: 0.0001  max_mem: 11521M
[02/27 22:21:39 d2.utils.events]:  eta: 1 day, 0:09:26  iter: 17779  total_loss: 1.224  loss_cls: 0.3171  loss_box_reg: 0.2506  loss_mask: 0.3413  loss_rpn_cls: 0.07408  loss_rpn_loc: 0.1205  time: 0.5215  last_time: 0.5385  data_time: 0.0185  last_data_time: 0.0279   lr: 0.0001  max_mem: 11521M
[02/27 22:21:49 d2.utils.events]:  eta: 1 day, 0:09:39  iter: 17799  total_loss: 1.18  loss_cls: 0.3383  loss_box_reg: 0.2365  loss_mask: 0.3686  loss_rpn_cls: 0.05936  loss_rpn_loc: 0.1264  time: 0.5215  last_time: 0.5261  data_time: 0.0301  last_data_time: 0.0092   lr: 0.0001  max_mem: 11521M
[02/27 22:22:00 d2.utils.events]:  eta: 1 day, 0:10:32  iter: 17819  total_loss: 1.305  loss_cls: 0.3539  loss_box_reg: 0.2259  loss_mask: 0.3765  loss_rpn_cls: 0.07028  loss_rpn_loc: 0.2075  time: 0.5215  last_time: 0.5239  data_time: 0.0199  last_data_time: 0.0117   lr: 0.0001  max_mem: 11521M
[02/27 22:22:10 d2.utils.events]:  eta: 1 day, 0:10:24  iter: 17839  total_loss: 1.088  loss_cls: 0.3238  loss_box_reg: 0.2069  loss_mask: 0.3772  loss_rpn_cls: 0.06366  loss_rpn_loc: 0.1104  time: 0.5215  last_time: 0.5405  data_time: 0.0141  last_data_time: 0.0134   lr: 0.0001  max_mem: 11521M
[02/27 22:22:21 d2.utils.events]:  eta: 1 day, 0:10:14  iter: 17859  total_loss: 1.112  loss_cls: 0.3637  loss_box_reg: 0.257  loss_mask: 0.3296  loss_rpn_cls: 0.06921  loss_rpn_loc: 0.1002  time: 0.5216  last_time: 0.5546  data_time: 0.0200  last_data_time: 0.0292   lr: 0.0001  max_mem: 11521M
[02/27 22:22:32 d2.utils.events]:  eta: 1 day, 0:08:40  iter: 17879  total_loss: 1.218  loss_cls: 0.3648  loss_box_reg: 0.2373  loss_mask: 0.3466  loss_rpn_cls: 0.05109  loss_rpn_loc: 0.1247  time: 0.5215  last_time: 0.5152  data_time: 0.0133  last_data_time: 0.0098   lr: 0.0001  max_mem: 11521M
[02/27 22:22:42 d2.utils.events]:  eta: 1 day, 0:09:50  iter: 17899  total_loss: 1.285  loss_cls: 0.3745  loss_box_reg: 0.2744  loss_mask: 0.3703  loss_rpn_cls: 0.07037  loss_rpn_loc: 0.1564  time: 0.5216  last_time: 0.5437  data_time: 0.0196  last_data_time: 0.0629   lr: 0.0001  max_mem: 11521M
[02/27 22:22:53 d2.utils.events]:  eta: 1 day, 0:09:39  iter: 17919  total_loss: 1.1  loss_cls: 0.2941  loss_box_reg: 0.2121  loss_mask: 0.353  loss_rpn_cls: 0.05771  loss_rpn_loc: 0.1463  time: 0.5215  last_time: 0.4953  data_time: 0.0107  last_data_time: 0.0033   lr: 0.0001  max_mem: 11521M
[02/27 22:23:03 d2.utils.events]:  eta: 1 day, 0:08:46  iter: 17939  total_loss: 1.117  loss_cls: 0.314  loss_box_reg: 0.2446  loss_mask: 0.3458  loss_rpn_cls: 0.06437  loss_rpn_loc: 0.1036  time: 0.5215  last_time: 0.4970  data_time: 0.0108  last_data_time: 0.0047   lr: 0.0001  max_mem: 11521M
[02/27 22:23:13 d2.utils.events]:  eta: 1 day, 0:07:40  iter: 17959  total_loss: 1.089  loss_cls: 0.3301  loss_box_reg: 0.2795  loss_mask: 0.3442  loss_rpn_cls: 0.06917  loss_rpn_loc: 0.09374  time: 0.5215  last_time: 0.4878  data_time: 0.0150  last_data_time: 0.0084   lr: 0.0001  max_mem: 11521M
[02/27 22:23:24 d2.utils.events]:  eta: 1 day, 0:07:11  iter: 17979  total_loss: 1.219  loss_cls: 0.3144  loss_box_reg: 0.2459  loss_mask: 0.3526  loss_rpn_cls: 0.0619  loss_rpn_loc: 0.1822  time: 0.5215  last_time: 0.4842  data_time: 0.0122  last_data_time: 0.0035   lr: 0.0001  max_mem: 11521M
[02/27 22:23:34 d2.utils.events]:  eta: 1 day, 0:07:01  iter: 17999  total_loss: 1.256  loss_cls: 0.3273  loss_box_reg: 0.2719  loss_mask: 0.3622  loss_rpn_cls: 0.05495  loss_rpn_loc: 0.1585  time: 0.5215  last_time: 0.5151  data_time: 0.0185  last_data_time: 0.0645   lr: 0.0001  max_mem: 11521M
[02/27 22:23:45 d2.utils.events]:  eta: 1 day, 0:05:43  iter: 18019  total_loss: 1.071  loss_cls: 0.2878  loss_box_reg: 0.2137  loss_mask: 0.3301  loss_rpn_cls: 0.06237  loss_rpn_loc: 0.1187  time: 0.5215  last_time: 0.4812  data_time: 0.0105  last_data_time: 0.0043   lr: 0.0001  max_mem: 11521M
[02/27 22:23:55 d2.utils.events]:  eta: 1 day, 0:05:24  iter: 18039  total_loss: 1.188  loss_cls: 0.3624  loss_box_reg: 0.2522  loss_mask: 0.3415  loss_rpn_cls: 0.07407  loss_rpn_loc: 0.1782  time: 0.5215  last_time: 0.6014  data_time: 0.0162  last_data_time: 0.0523   lr: 0.0001  max_mem: 11521M
[02/27 22:24:06 d2.utils.events]:  eta: 1 day, 0:05:23  iter: 18059  total_loss: 1.089  loss_cls: 0.3104  loss_box_reg: 0.2012  loss_mask: 0.3473  loss_rpn_cls: 0.04626  loss_rpn_loc: 0.09198  time: 0.5215  last_time: 0.5383  data_time: 0.0159  last_data_time: 0.0042   lr: 0.0001  max_mem: 11521M
[02/27 22:24:17 d2.utils.events]:  eta: 1 day, 0:05:36  iter: 18079  total_loss: 1.327  loss_cls: 0.3846  loss_box_reg: 0.267  loss_mask: 0.3764  loss_rpn_cls: 0.05294  loss_rpn_loc: 0.1552  time: 0.5215  last_time: 0.5905  data_time: 0.0190  last_data_time: 0.0424   lr: 0.0001  max_mem: 11521M
[02/27 22:24:27 d2.utils.events]:  eta: 1 day, 0:05:46  iter: 18099  total_loss: 1.23  loss_cls: 0.3658  loss_box_reg: 0.2672  loss_mask: 0.371  loss_rpn_cls: 0.05479  loss_rpn_loc: 0.122  time: 0.5215  last_time: 0.5386  data_time: 0.0239  last_data_time: 0.0663   lr: 0.0001  max_mem: 11521M
[02/27 22:24:38 d2.utils.events]:  eta: 1 day, 0:05:36  iter: 18119  total_loss: 1.107  loss_cls: 0.2835  loss_box_reg: 0.2139  loss_mask: 0.3658  loss_rpn_cls: 0.0501  loss_rpn_loc: 0.1294  time: 0.5215  last_time: 0.4996  data_time: 0.0183  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 22:24:48 d2.utils.events]:  eta: 1 day, 0:05:58  iter: 18139  total_loss: 1.055  loss_cls: 0.3023  loss_box_reg: 0.2255  loss_mask: 0.327  loss_rpn_cls: 0.06377  loss_rpn_loc: 0.1214  time: 0.5215  last_time: 0.5001  data_time: 0.0181  last_data_time: 0.0118   lr: 0.0001  max_mem: 11521M
[02/27 22:24:58 d2.utils.events]:  eta: 1 day, 0:05:56  iter: 18159  total_loss: 1.046  loss_cls: 0.3099  loss_box_reg: 0.2333  loss_mask: 0.356  loss_rpn_cls: 0.05463  loss_rpn_loc: 0.08583  time: 0.5215  last_time: 0.5106  data_time: 0.0132  last_data_time: 0.0113   lr: 0.0001  max_mem: 11521M
[02/27 22:25:09 d2.utils.events]:  eta: 1 day, 0:06:00  iter: 18179  total_loss: 1.119  loss_cls: 0.3378  loss_box_reg: 0.243  loss_mask: 0.3929  loss_rpn_cls: 0.06229  loss_rpn_loc: 0.1395  time: 0.5215  last_time: 0.5209  data_time: 0.0179  last_data_time: 0.0107   lr: 0.0001  max_mem: 11521M
[02/27 22:25:20 d2.utils.events]:  eta: 1 day, 0:05:35  iter: 18199  total_loss: 1.123  loss_cls: 0.3129  loss_box_reg: 0.2221  loss_mask: 0.3752  loss_rpn_cls: 0.07054  loss_rpn_loc: 0.1163  time: 0.5215  last_time: 0.5130  data_time: 0.0169  last_data_time: 0.0041   lr: 0.0001  max_mem: 11521M
[02/27 22:25:30 d2.utils.events]:  eta: 1 day, 0:04:57  iter: 18219  total_loss: 1.196  loss_cls: 0.335  loss_box_reg: 0.2482  loss_mask: 0.3929  loss_rpn_cls: 0.07066  loss_rpn_loc: 0.1362  time: 0.5215  last_time: 0.5393  data_time: 0.0115  last_data_time: 0.0175   lr: 0.0001  max_mem: 11521M
[02/27 22:25:40 d2.utils.events]:  eta: 1 day, 0:04:33  iter: 18239  total_loss: 1.179  loss_cls: 0.3442  loss_box_reg: 0.254  loss_mask: 0.3603  loss_rpn_cls: 0.04757  loss_rpn_loc: 0.09768  time: 0.5215  last_time: 0.5199  data_time: 0.0242  last_data_time: 0.0101   lr: 0.0001  max_mem: 11521M
[02/27 22:25:51 d2.utils.events]:  eta: 1 day, 0:04:23  iter: 18259  total_loss: 1.152  loss_cls: 0.3562  loss_box_reg: 0.2978  loss_mask: 0.338  loss_rpn_cls: 0.06006  loss_rpn_loc: 0.09548  time: 0.5215  last_time: 0.5175  data_time: 0.0215  last_data_time: 0.0267   lr: 0.0001  max_mem: 11521M
[02/27 22:26:02 d2.utils.events]:  eta: 1 day, 0:03:40  iter: 18279  total_loss: 0.9955  loss_cls: 0.2979  loss_box_reg: 0.2225  loss_mask: 0.3547  loss_rpn_cls: 0.05096  loss_rpn_loc: 0.08278  time: 0.5215  last_time: 0.5753  data_time: 0.0358  last_data_time: 0.0045   lr: 0.0001  max_mem: 11521M
[02/27 22:26:12 d2.utils.events]:  eta: 1 day, 0:03:40  iter: 18299  total_loss: 1.301  loss_cls: 0.3951  loss_box_reg: 0.2507  loss_mask: 0.3873  loss_rpn_cls: 0.07171  loss_rpn_loc: 0.1153  time: 0.5215  last_time: 0.5181  data_time: 0.0091  last_data_time: 0.0119   lr: 0.0001  max_mem: 11521M
[02/27 22:26:23 d2.utils.events]:  eta: 1 day, 0:03:52  iter: 18319  total_loss: 1.135  loss_cls: 0.345  loss_box_reg: 0.2283  loss_mask: 0.3366  loss_rpn_cls: 0.05509  loss_rpn_loc: 0.1161  time: 0.5215  last_time: 0.5041  data_time: 0.0162  last_data_time: 0.0032   lr: 0.0001  max_mem: 11521M
[02/27 22:26:33 d2.utils.events]:  eta: 1 day, 0:03:02  iter: 18339  total_loss: 1.208  loss_cls: 0.343  loss_box_reg: 0.2554  loss_mask: 0.3784  loss_rpn_cls: 0.05125  loss_rpn_loc: 0.1203  time: 0.5215  last_time: 0.5317  data_time: 0.0084  last_data_time: 0.0097   lr: 0.0001  max_mem: 11521M
[02/27 22:26:44 d2.utils.events]:  eta: 1 day, 0:03:08  iter: 18359  total_loss: 1.17  loss_cls: 0.355  loss_box_reg: 0.2693  loss_mask: 0.3786  loss_rpn_cls: 0.05524  loss_rpn_loc: 0.1124  time: 0.5215  last_time: 0.5290  data_time: 0.0126  last_data_time: 0.0430   lr: 0.0001  max_mem: 11521M
[02/27 22:26:54 d2.utils.events]:  eta: 1 day, 0:02:41  iter: 18379  total_loss: 1.143  loss_cls: 0.3324  loss_box_reg: 0.227  loss_mask: 0.3484  loss_rpn_cls: 0.04932  loss_rpn_loc: 0.09253  time: 0.5215  last_time: 0.5319  data_time: 0.0210  last_data_time: 0.0121   lr: 0.0001  max_mem: 11521M
[02/27 22:27:05 d2.utils.events]:  eta: 1 day, 0:03:01  iter: 18399  total_loss: 1.286  loss_cls: 0.3758  loss_box_reg: 0.2463  loss_mask: 0.372  loss_rpn_cls: 0.08335  loss_rpn_loc: 0.142  time: 0.5215  last_time: 0.4836  data_time: 0.0159  last_data_time: 0.0111   lr: 0.0001  max_mem: 11521M
[02/27 22:27:15 d2.utils.events]:  eta: 1 day, 0:02:59  iter: 18419  total_loss: 1.162  loss_cls: 0.2762  loss_box_reg: 0.2213  loss_mask: 0.3568  loss_rpn_cls: 0.07013  loss_rpn_loc: 0.1162  time: 0.5215  last_time: 0.5501  data_time: 0.0171  last_data_time: 0.0099   lr: 0.0001  max_mem: 11521M
[02/27 22:27:26 d2.utils.events]:  eta: 1 day, 0:03:11  iter: 18439  total_loss: 1.28  loss_cls: 0.3847  loss_box_reg: 0.302  loss_mask: 0.3663  loss_rpn_cls: 0.06449  loss_rpn_loc: 0.1274  time: 0.5215  last_time: 0.4746  data_time: 0.0205  last_data_time: 0.0038   lr: 0.0001  max_mem: 11521M
[02/27 22:27:36 d2.utils.events]:  eta: 1 day, 0:03:01  iter: 18459  total_loss: 1.097  loss_cls: 0.3072  loss_box_reg: 0.2498  loss_mask: 0.3427  loss_rpn_cls: 0.05582  loss_rpn_loc: 0.1321  time: 0.5215  last_time: 0.5456  data_time: 0.0215  last_data_time: 0.0264   lr: 0.0001  max_mem: 11521M
[02/27 22:27:47 d2.utils.events]:  eta: 1 day, 0:02:41  iter: 18479  total_loss: 1.168  loss_cls: 0.3274  loss_box_reg: 0.2526  loss_mask: 0.362  loss_rpn_cls: 0.05942  loss_rpn_loc: 0.0991  time: 0.5215  last_time: 0.5060  data_time: 0.0213  last_data_time: 0.0030   lr: 0.0001  max_mem: 11521M
[02/27 22:27:57 d2.utils.events]:  eta: 1 day, 0:02:03  iter: 18499  total_loss: 1.221  loss_cls: 0.3458  loss_box_reg: 0.2538  loss_mask: 0.3748  loss_rpn_cls: 0.05533  loss_rpn_loc: 0.08118  time: 0.5215  last_time: 0.5459  data_time: 0.0148  last_data_time: 0.0115   lr: 0.0001  max_mem: 11521M
[02/27 22:28:07 d2.utils.events]:  eta: 1 day, 0:02:07  iter: 18519  total_loss: 1.036  loss_cls: 0.2907  loss_box_reg: 0.2373  loss_mask: 0.3411  loss_rpn_cls: 0.06584  loss_rpn_loc: 0.1296  time: 0.5215  last_time: 0.5202  data_time: 0.0141  last_data_time: 0.0121   lr: 0.0001  max_mem: 11521M
[02/27 22:28:18 d2.utils.events]:  eta: 1 day, 0:02:29  iter: 18539  total_loss: 1.087  loss_cls: 0.3256  loss_box_reg: 0.1995  loss_mask: 0.3678  loss_rpn_cls: 0.07063  loss_rpn_loc: 0.1003  time: 0.5215  last_time: 0.5299  data_time: 0.0314  last_data_time: 0.0259   lr: 0.0001  max_mem: 11521M
[02/27 22:28:28 d2.utils.events]:  eta: 1 day, 0:01:38  iter: 18559  total_loss: 1.136  loss_cls: 0.3406  loss_box_reg: 0.233  loss_mask: 0.3713  loss_rpn_cls: 0.05539  loss_rpn_loc: 0.124  time: 0.5215  last_time: 0.5515  data_time: 0.0194  last_data_time: 0.0222   lr: 0.0001  max_mem: 11521M
[02/27 22:28:39 d2.utils.events]:  eta: 1 day, 0:00:42  iter: 18579  total_loss: 1.164  loss_cls: 0.3152  loss_box_reg: 0.2402  loss_mask: 0.3692  loss_rpn_cls: 0.06533  loss_rpn_loc: 0.1671  time: 0.5215  last_time: 0.5171  data_time: 0.0114  last_data_time: 0.0046   lr: 0.0001  max_mem: 11521M
[02/27 22:28:49 d2.utils.events]:  eta: 1 day, 0:00:19  iter: 18599  total_loss: 1.113  loss_cls: 0.3359  loss_box_reg: 0.2257  loss_mask: 0.3436  loss_rpn_cls: 0.04979  loss_rpn_loc: 0.08687  time: 0.5215  last_time: 0.5319  data_time: 0.0207  last_data_time: 0.0121   lr: 0.0001  max_mem: 11521M
[02/27 22:28:59 d2.utils.events]:  eta: 1 day, 0:00:18  iter: 18619  total_loss: 1.142  loss_cls: 0.2911  loss_box_reg: 0.1955  loss_mask: 0.3557  loss_rpn_cls: 0.07418  loss_rpn_loc: 0.1564  time: 0.5215  last_time: 0.5416  data_time: 0.0225  last_data_time: 0.0433   lr: 0.0001  max_mem: 11521M
[02/27 22:29:10 d2.utils.events]:  eta: 1 day, 0:00:04  iter: 18639  total_loss: 1.166  loss_cls: 0.3144  loss_box_reg: 0.2603  loss_mask: 0.3541  loss_rpn_cls: 0.0594  loss_rpn_loc: 0.1436  time: 0.5215  last_time: 0.5446  data_time: 0.0193  last_data_time: 0.0405   lr: 0.0001  max_mem: 11521M
[02/27 22:29:21 d2.utils.events]:  eta: 1 day, 0:00:00  iter: 18659  total_loss: 1.069  loss_cls: 0.2751  loss_box_reg: 0.206  loss_mask: 0.3512  loss_rpn_cls: 0.06029  loss_rpn_loc: 0.1409  time: 0.5215  last_time: 0.5012  data_time: 0.0114  last_data_time: 0.0224   lr: 0.0001  max_mem: 11521M
srun: forcing job termination
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
slurmstepd: error: *** STEP 316298.0 ON euler22 CANCELLED AT 2023-02-27T22:29:23 ***
srun: error: euler22: task 0: Killed
srun: Force Terminated StepId=316298.0
srun: job 316527 queued and waiting for resources
srun: job 316527 has been allocated resources
[02/28 05:11:48 detectron2]: Rank of current process: 0. World size: 4
[02/28 05:11:53 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/28 05:11:53 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/28 05:11:53 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader


model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
train.output_dir = f"/srv/home/pmorgado/yibing/output/mae2cl_detection/bs{dataloader.train.total_batch_size}_mae"
train.init_checkpoint = (
   "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest_detectron2.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
)

train.num_workers = 2
# image_size=640
# 3-4 images per gpu
# 40 ep
# 4 gpus

# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}

[02/28 05:11:53 detectron2]: Full config saved to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/config.yaml
[02/28 05:11:53 d2.utils.env]: Using a generated random seed 56848832
[02/28 05:12:07 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Traceback (most recent call last):
  File "tools/lazyconfig_train_net.py", line 130, in <module>
    args=(args,),
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 81, in launch
    daemon=False,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 123, in _distributed_worker
    main_func(*args)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 119, in main
    do_train(args, cfg)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 71, in do_train
    train_loader = instantiate(cfg.dataloader.train)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/config/instantiate.py", line 67, in instantiate
    cfg = {k: instantiate(v) for k, v in cfg.items()}
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/config/instantiate.py", line 67, in <dictcomp>
    cfg = {k: instantiate(v) for k, v in cfg.items()}
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/config/instantiate.py", line 83, in instantiate
    return cls(**cfg)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/build.py", line 241, in get_detection_dataset_dicts
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in names]
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/build.py", line 241, in <listcomp>
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in names]
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/catalog.py", line 58, in get
    return f()
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/datasets/coco.py", line 500, in <lambda>
    DatasetCatalog.register(name, lambda: load_coco_json(json_file, image_root, name))
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/datasets/coco.py", line 69, in load_coco_json
    coco_api = COCO(json_file)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/pycocotools/coco.py", line 81, in __init__
    with open(annotation_file, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'datasets/coco/annotations/instances_train2017.json'

srun: error: euler22: task 0: Exited with exit code 1
srun: job 316530 queued and waiting for resources
srun: job 316530 has been allocated resources
[02/28 05:19:27 detectron2]: Rank of current process: 0. World size: 4
[02/28 05:19:30 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/28 05:19:30 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/28 05:19:30 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader


model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
train.output_dir = f"/srv/home/pmorgado/yibing/output/mae2cl_detection/bs{dataloader.train.total_batch_size}_mae"
train.init_checkpoint = (
   "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest_detectron2.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
)

train.num_workers = 2
# image_size=640
# 3-4 images per gpu
# 40 ep
# 4 gpus

# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}

[02/28 05:19:30 detectron2]: Full config saved to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs4_mae/config.yaml
[02/28 05:19:30 d2.utils.env]: Using a generated random seed 33640402
[02/28 05:19:36 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[02/28 05:19:48 d2.data.datasets.coco]: Loading /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json takes 12.26 seconds.
[02/28 05:19:49 d2.data.datasets.coco]: Loaded 118287 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json
[02/28 05:19:55 d2.data.build]: Removed 1021 images with no usable annotations. 117266 images left.
[02/28 05:19:58 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |
[02/28 05:19:58 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=640, target_width=640), FixedSizeCrop(crop_size=[640, 640], pad=False)]
[02/28 05:19:58 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/28 05:19:58 d2.data.common]: Serializing 117266 elements to byte tensors and concatenating them all ...
[02/28 05:20:01 d2.data.common]: Serialized dataset takes 453.34 MiB
Traceback (most recent call last):
  File "tools/lazyconfig_train_net.py", line 130, in <module>
    args=(args,),
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 81, in launch
    daemon=False,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 123, in _distributed_worker
    main_func(*args)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 119, in main
    do_train(args, cfg)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 97, in do_train
    checkpointer.resume_or_load(cfg.train.init_checkpoint, resume=args.resume)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/fvcore/common/checkpoint.py", line 227, in resume_or_load
    return self.load(path, checkpointables=[])
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/checkpoint/detection_checkpoint.py", line 62, in load
    ret = super().load(path, *args, **kwargs)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/fvcore/common/checkpoint.py", line 153, in load
    assert os.path.isfile(path), "Checkpoint {} not found!".format(path)
AssertionError: Checkpoint /srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id2/checkpoints/checkpoint_latest_detectron2.pth not found!

srun: error: euler22: task 0: Exited with exit code 1
srun: job 316532 queued and waiting for resources
srun: job 316532 has been allocated resources
[02/28 05:24:21 detectron2]: Rank of current process: 0. World size: 4
[02/28 05:24:23 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/28 05:24:23 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/28 05:24:23 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader


model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

dataloader.train.total_batch_size = 16 # 4 images per gpu #4 gpus

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
train.output_dir = f"/srv/home/pmorgado/yibing/output/mae2cl_detection/bs{dataloader.train.total_batch_size}_mae"
train.init_checkpoint = (
   "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id3/checkpoints/checkpoint_latest_detectron2.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
)

# train.num_workers = 2
# image_size=640
# 3-4 images per gpu
# 40 ep
# 4 gpus

# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}

[02/28 05:24:23 detectron2]: Full config saved to /srv/home/pmorgado/yibing/output/mae2cl_detection/bs16_mae/config.yaml
[02/28 05:24:23 d2.utils.env]: Using a generated random seed 26626949
[02/28 05:24:26 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[02/28 05:24:38 d2.data.datasets.coco]: Loading /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json takes 12.07 seconds.
[02/28 05:24:39 d2.data.datasets.coco]: Loaded 118287 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json
[02/28 05:24:44 d2.data.build]: Removed 1021 images with no usable annotations. 117266 images left.
[02/28 05:24:48 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |
[02/28 05:24:48 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=640, target_width=640), FixedSizeCrop(crop_size=[640, 640], pad=False)]
[02/28 05:24:48 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/28 05:24:48 d2.data.common]: Serializing 117266 elements to byte tensors and concatenating them all ...
[02/28 05:24:50 d2.data.common]: Serialized dataset takes 453.34 MiB
Traceback (most recent call last):
  File "tools/lazyconfig_train_net.py", line 130, in <module>
    args=(args,),
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 81, in launch
    daemon=False,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 123, in _distributed_worker
    main_func(*args)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 119, in main
    do_train(args, cfg)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 97, in do_train
    checkpointer.resume_or_load(cfg.train.init_checkpoint, resume=args.resume)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/fvcore/common/checkpoint.py", line 227, in resume_or_load
    return self.load(path, checkpointables=[])
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/checkpoint/detection_checkpoint.py", line 62, in load
    ret = super().load(path, *args, **kwargs)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/fvcore/common/checkpoint.py", line 153, in load
    assert os.path.isfile(path), "Checkpoint {} not found!".format(path)
AssertionError: Checkpoint /srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id3/checkpoints/checkpoint_latest_detectron2.pth not found!

srun: error: euler22: task 0: Exited with exit code 1
srun: job 316546 queued and waiting for resources
srun: job 316546 has been allocated resources
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:57709 (errno: 1 - Operation not permitted).
[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:57709 (errno: 1 - Operation not permitted).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
Process group URL: tcp://127.0.0.1:57709
Traceback (most recent call last):
  File "tools/lazyconfig_train_net.py", line 131, in <module>
    args=(args,),
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 81, in launch
    daemon=False,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 112, in _distributed_worker
    raise e
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 107, in _distributed_worker
    timeout=timeout,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 754, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/distributed/rendezvous.py", line 201, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/distributed/rendezvous.py", line 178, in _create_c10d_store
    hostname, port, world_size, start_daemon, timeout, multi_tenant=True
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:57709 (errno: 1 - Operation not permitted). The server socket has failed to bind to 0.0.0.0:57709 (errno: 1 - Operation not permitted).

srun: error: euler22: task 0: Exited with exit code 1
srun: job 316547 queued and waiting for resources
srun: job 316547 has been allocated resources
[02/28 06:18:49 detectron2]: Rank of current process: 0. World size: 4
[02/28 06:18:51 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/28 06:18:51 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/28 06:18:51 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader

import os

model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

dataloader.train.total_batch_size = 20 # 4 images per gpu #4 gpus

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True

output_dir = "/srv/home/pmorgado/workspace/mae2cl/checkpoints"
pretrain_job_name = "path1_maefeat_d3t12_compl[0, 0.25]_m0.9_c0.2_epeintrinsic_dpeglobal_blr0.0005_infonce_patches_in100_vitb_bs128x1_ep100_id3"
pretrain_ckpt = os.path.join(output_dir,pretrain_job_name,"checkpoints","checkpoint_latest_detectron2.pth")
train.init_checkpoint = (pretrain_ckpt)
# train.init_checkpoint = (
#    "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id3/checkpoints/checkpoint_latest_detectron2.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
# )

# image_size=640
# 5 images per gpu
# 40 ep
# 4 gpus

# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
# 15 ep = 87949 iters * 20 images/iter / 117266 images/ep # Loaded 118287 images. Removed 1021 images with no usable annotations. 117266 images left.
# 117266 images/ep / 20 images/iter = 5863 iter/ep ~ 44 mins/ep 
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.lr = 8e-5 # ImageNet 8e-5 | None 1.6e-4 | MAE 1e-4
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}


train.output_dir = f"/srv/home/pmorgado/workspace/mae2cl/checkpoints/det__{pretrain_job_name}__bs{dataloader.train.total_batch_size}_blr{optimizer.lr}"
[02/28 06:18:51 detectron2]: Full config saved to /srv/home/pmorgado/workspace/mae2cl/checkpoints/det__path1_maefeat_d3t12_compl[0, 0.25]_m0.9_c0.2_epeintrinsic_dpeglobal_blr0.0005_infonce_patches_in100_vitb_bs128x1_ep100_id3__bs20_blr8e-05/config.yaml
[02/28 06:18:51 d2.utils.env]: Using a generated random seed 55316189
[02/28 06:19:00 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Traceback (most recent call last):
  File "tools/lazyconfig_train_net.py", line 131, in <module>
    args=(args,),
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 81, in launch
    daemon=False,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 123, in _distributed_worker
    main_func(*args)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 120, in main
    do_train(args, cfg)
  File "/srv/home/pmorgado/yibing/detectron2/tools/lazyconfig_train_net.py", line 71, in do_train
    train_loader = instantiate(cfg.dataloader.train)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/config/instantiate.py", line 67, in instantiate
    cfg = {k: instantiate(v) for k, v in cfg.items()}
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/config/instantiate.py", line 67, in <dictcomp>
    cfg = {k: instantiate(v) for k, v in cfg.items()}
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/config/instantiate.py", line 83, in instantiate
    return cls(**cfg)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/build.py", line 241, in get_detection_dataset_dicts
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in names]
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/build.py", line 241, in <listcomp>
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in names]
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/catalog.py", line 58, in get
    return f()
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/datasets/coco.py", line 500, in <lambda>
    DatasetCatalog.register(name, lambda: load_coco_json(json_file, image_root, name))
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/data/datasets/coco.py", line 69, in load_coco_json
    coco_api = COCO(json_file)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/pycocotools/coco.py", line 81, in __init__
    with open(annotation_file, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'datasets/coco/annotations/instances_train2017.json'

srun: error: euler22: task 0: Exited with exit code 1
srun: job 316548 queued and waiting for resources
srun: job 316548 has been allocated resources
[02/28 06:23:19 detectron2]: Rank of current process: 0. World size: 4
[02/28 06:23:21 detectron2]: Environment info:
----------------------  ---------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
numpy                   1.21.5
detectron2              0.6 @/srv/home/pmorgado/yibing/detectron2/detectron2
Compiler                GCC 12.2
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.13.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             NVIDIA RTX A4500 (arch=8.6)
Driver version          525.78.01
CUDA_HOME               /srv/home/pmorgado/anaconda3/envs/pt113
Pillow                  9.3.0
torchvision             0.14.1 @/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     Not found
----------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[02/28 06:23:21 detectron2]: Command line arguments: Namespace(config_file='/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py', dist_url='tcp://127.0.0.1:57709', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/28 06:23:21 detectron2]: Contents of args.config_file=/srv/home/pmorgado/yibing/detectron2/projects/ViTDet/configs/COCO/mask_rcnn_vitdet_b_100ep.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate

from ..common.coco_loader_lsj import dataloader

import os

model = model_zoo.get_config("common/models/mask_rcnn_vitdet.py").model #configs/common/models/mask_rcnn_vitdet.py

dataloader.train.total_batch_size = 20 # 4 images per gpu #4 gpus

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True

output_dir = "/srv/home/pmorgado/workspace/mae2cl/checkpoints"
pretrain_job_name = "path1_maefeat_d3t12_compl[0, 0.25]_m0.9_c0.2_epeintrinsic_dpeglobal_blr0.0005_infonce_patches_in100_vitb_bs128x1_ep100_id3"
pretrain_ckpt = os.path.join(output_dir,pretrain_job_name,"checkpoints","checkpoint_latest_detectron2.pth")
train.init_checkpoint = (pretrain_ckpt)
# train.init_checkpoint = (
#    "/srv/home/pmorgado/workspace/mae2cl/checkpoints/mae_in100_vitb_bs128x1_ep100_id3/checkpoints/checkpoint_latest_detectron2.pth"# "detectron2://ImageNetPretrained/MAE/mae_pretrain_vit_base.pth?matching_heuristics=True" # 
# )

# image_size=640
# 5 images per gpu
# 40 ep
# 4 gpus

# Schedule
# 100 ep = 184375 iters * 64 images/iter / 118000 images/ep
# 15 ep = 87949 iters * 20 images/iter / 117266 images/ep # Loaded 118287 images. Removed 1021 images with no usable annotations. 117266 images left.
# 117266 images/ep / 20 images/iter = 5863 iter/ep ~ 44 mins/ep 
train.max_iter = 184375

lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[163889, 177546],
        num_updates=train.max_iter,
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

# Optimizer
optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.lr = 8e-5 # ImageNet 8e-5 | None 1.6e-4 | MAE 1e-4
optimizer.params.lr_factor_func = partial(get_vit_lr_decay_rate, num_layers=12, lr_decay_rate=0.7)
optimizer.params.overrides = {"pos_embed": {"weight_decay": 0.0}}


train.output_dir = f"/srv/home/pmorgado/workspace/mae2cl/checkpoints/det__{pretrain_job_name}__bs{dataloader.train.total_batch_size}_blr{optimizer.lr}"
[02/28 06:23:21 detectron2]: Full config saved to /srv/home/pmorgado/workspace/mae2cl/checkpoints/det__path1_maefeat_d3t12_compl[0, 0.25]_m0.9_c0.2_epeintrinsic_dpeglobal_blr0.0005_infonce_patches_in100_vitb_bs128x1_ep100_id3__bs20_blr8e-05/config.yaml
[02/28 06:23:21 d2.utils.env]: Using a generated random seed 25107183
[02/28 06:23:24 detectron2]: Model:
GeneralizedRCNN(
  (backbone): SimpleFeaturePyramid(
    (simfp_2): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))
      (4): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (5): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_3): Sequential(
      (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))
      (1): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_4): Sequential(
      (0): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (simfp_5): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): LayerNorm()
      )
      (2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
      )
    )
    (net): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      )
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (top_block): LastLevelMaxPool()
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (conv4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): LayerNorm()
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[02/28 06:23:36 d2.data.datasets.coco]: Loading /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json takes 11.89 seconds.
[02/28 06:23:36 d2.data.datasets.coco]: Loaded 118287 images in COCO format from /srv/home/pmorgado/datasets/coco/annotations/instances_train2017.json
[02/28 06:23:42 d2.data.build]: Removed 1021 images with no usable annotations. 117266 images left.
[02/28 06:23:45 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |
[02/28 06:23:45 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=640, target_width=640), FixedSizeCrop(crop_size=[640, 640], pad=False)]
[02/28 06:23:45 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[02/28 06:23:45 d2.data.common]: Serializing 117266 elements to byte tensors and concatenating them all ...
[02/28 06:23:48 d2.data.common]: Serialized dataset takes 453.34 MiB
[02/28 06:23:54 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /srv/home/pmorgado/workspace/mae2cl/checkpoints/path1_maefeat_d3t12_compl[0, 0.25]_m0.9_c0.2_epeintrinsic_dpeglobal_blr0.0005_infonce_patches_in100_vitb_bs128x1_ep100_id3/checkpoints/checkpoint_latest_detectron2.pth ...
[02/28 06:23:54 fvcore.common.checkpoint]: [Checkpointer] Loading from /srv/home/pmorgado/workspace/mae2cl/checkpoints/path1_maefeat_d3t12_compl[0, 0.25]_m0.9_c0.2_epeintrinsic_dpeglobal_blr0.0005_infonce_patches_in100_vitb_bs128x1_ep100_id3/checkpoints/checkpoint_latest_detectron2.pth ...
checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['cls_token', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.cls_token', 'backbone.net.pos_embed', 'backbone.net.mask_token', 'backbone.net.patch_embed.proj.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.norm.weight', 'backbone.net.norm.bias']checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['cls_token', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.cls_token', 'backbone.net.pos_embed', 'backbone.net.mask_token', 'backbone.net.patch_embed.proj.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.norm.weight', 'backbone.net.norm.bias']checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['cls_token', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.cls_token', 'backbone.net.pos_embed', 'backbone.net.mask_token', 'backbone.net.patch_embed.proj.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.norm.weight', 'backbone.net.norm.bias']checkpoint.get("matching_heuristics", False) False
checkpoint_state_dict ['cls_token', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']
after checkpoint_state_dict ['backbone.net.cls_token', 'backbone.net.pos_embed', 'backbone.net.mask_token', 'backbone.net.patch_embed.proj.weight', 'backbone.net.patch_embed.proj.bias', 'backbone.net.blocks.0.norm1.weight', 'backbone.net.blocks.0.norm1.bias', 'backbone.net.blocks.0.attn.qkv.weight', 'backbone.net.blocks.0.attn.qkv.bias', 'backbone.net.blocks.0.attn.proj.weight', 'backbone.net.blocks.0.attn.proj.bias', 'backbone.net.blocks.0.norm2.weight', 'backbone.net.blocks.0.norm2.bias', 'backbone.net.blocks.0.mlp.fc1.weight', 'backbone.net.blocks.0.mlp.fc1.bias', 'backbone.net.blocks.0.mlp.fc2.weight', 'backbone.net.blocks.0.mlp.fc2.bias', 'backbone.net.blocks.1.norm1.weight', 'backbone.net.blocks.1.norm1.bias', 'backbone.net.blocks.1.attn.qkv.weight', 'backbone.net.blocks.1.attn.qkv.bias', 'backbone.net.blocks.1.attn.proj.weight', 'backbone.net.blocks.1.attn.proj.bias', 'backbone.net.blocks.1.norm2.weight', 'backbone.net.blocks.1.norm2.bias', 'backbone.net.blocks.1.mlp.fc1.weight', 'backbone.net.blocks.1.mlp.fc1.bias', 'backbone.net.blocks.1.mlp.fc2.weight', 'backbone.net.blocks.1.mlp.fc2.bias', 'backbone.net.blocks.2.norm1.weight', 'backbone.net.blocks.2.norm1.bias', 'backbone.net.blocks.2.attn.qkv.weight', 'backbone.net.blocks.2.attn.qkv.bias', 'backbone.net.blocks.2.attn.proj.weight', 'backbone.net.blocks.2.attn.proj.bias', 'backbone.net.blocks.2.norm2.weight', 'backbone.net.blocks.2.norm2.bias', 'backbone.net.blocks.2.mlp.fc1.weight', 'backbone.net.blocks.2.mlp.fc1.bias', 'backbone.net.blocks.2.mlp.fc2.weight', 'backbone.net.blocks.2.mlp.fc2.bias', 'backbone.net.blocks.3.norm1.weight', 'backbone.net.blocks.3.norm1.bias', 'backbone.net.blocks.3.attn.qkv.weight', 'backbone.net.blocks.3.attn.qkv.bias', 'backbone.net.blocks.3.attn.proj.weight', 'backbone.net.blocks.3.attn.proj.bias', 'backbone.net.blocks.3.norm2.weight', 'backbone.net.blocks.3.norm2.bias', 'backbone.net.blocks.3.mlp.fc1.weight', 'backbone.net.blocks.3.mlp.fc1.bias', 'backbone.net.blocks.3.mlp.fc2.weight', 'backbone.net.blocks.3.mlp.fc2.bias', 'backbone.net.blocks.4.norm1.weight', 'backbone.net.blocks.4.norm1.bias', 'backbone.net.blocks.4.attn.qkv.weight', 'backbone.net.blocks.4.attn.qkv.bias', 'backbone.net.blocks.4.attn.proj.weight', 'backbone.net.blocks.4.attn.proj.bias', 'backbone.net.blocks.4.norm2.weight', 'backbone.net.blocks.4.norm2.bias', 'backbone.net.blocks.4.mlp.fc1.weight', 'backbone.net.blocks.4.mlp.fc1.bias', 'backbone.net.blocks.4.mlp.fc2.weight', 'backbone.net.blocks.4.mlp.fc2.bias', 'backbone.net.blocks.5.norm1.weight', 'backbone.net.blocks.5.norm1.bias', 'backbone.net.blocks.5.attn.qkv.weight', 'backbone.net.blocks.5.attn.qkv.bias', 'backbone.net.blocks.5.attn.proj.weight', 'backbone.net.blocks.5.attn.proj.bias', 'backbone.net.blocks.5.norm2.weight', 'backbone.net.blocks.5.norm2.bias', 'backbone.net.blocks.5.mlp.fc1.weight', 'backbone.net.blocks.5.mlp.fc1.bias', 'backbone.net.blocks.5.mlp.fc2.weight', 'backbone.net.blocks.5.mlp.fc2.bias', 'backbone.net.blocks.6.norm1.weight', 'backbone.net.blocks.6.norm1.bias', 'backbone.net.blocks.6.attn.qkv.weight', 'backbone.net.blocks.6.attn.qkv.bias', 'backbone.net.blocks.6.attn.proj.weight', 'backbone.net.blocks.6.attn.proj.bias', 'backbone.net.blocks.6.norm2.weight', 'backbone.net.blocks.6.norm2.bias', 'backbone.net.blocks.6.mlp.fc1.weight', 'backbone.net.blocks.6.mlp.fc1.bias', 'backbone.net.blocks.6.mlp.fc2.weight', 'backbone.net.blocks.6.mlp.fc2.bias', 'backbone.net.blocks.7.norm1.weight', 'backbone.net.blocks.7.norm1.bias', 'backbone.net.blocks.7.attn.qkv.weight', 'backbone.net.blocks.7.attn.qkv.bias', 'backbone.net.blocks.7.attn.proj.weight', 'backbone.net.blocks.7.attn.proj.bias', 'backbone.net.blocks.7.norm2.weight', 'backbone.net.blocks.7.norm2.bias', 'backbone.net.blocks.7.mlp.fc1.weight', 'backbone.net.blocks.7.mlp.fc1.bias', 'backbone.net.blocks.7.mlp.fc2.weight', 'backbone.net.blocks.7.mlp.fc2.bias', 'backbone.net.blocks.8.norm1.weight', 'backbone.net.blocks.8.norm1.bias', 'backbone.net.blocks.8.attn.qkv.weight', 'backbone.net.blocks.8.attn.qkv.bias', 'backbone.net.blocks.8.attn.proj.weight', 'backbone.net.blocks.8.attn.proj.bias', 'backbone.net.blocks.8.norm2.weight', 'backbone.net.blocks.8.norm2.bias', 'backbone.net.blocks.8.mlp.fc1.weight', 'backbone.net.blocks.8.mlp.fc1.bias', 'backbone.net.blocks.8.mlp.fc2.weight', 'backbone.net.blocks.8.mlp.fc2.bias', 'backbone.net.blocks.9.norm1.weight', 'backbone.net.blocks.9.norm1.bias', 'backbone.net.blocks.9.attn.qkv.weight', 'backbone.net.blocks.9.attn.qkv.bias', 'backbone.net.blocks.9.attn.proj.weight', 'backbone.net.blocks.9.attn.proj.bias', 'backbone.net.blocks.9.norm2.weight', 'backbone.net.blocks.9.norm2.bias', 'backbone.net.blocks.9.mlp.fc1.weight', 'backbone.net.blocks.9.mlp.fc1.bias', 'backbone.net.blocks.9.mlp.fc2.weight', 'backbone.net.blocks.9.mlp.fc2.bias', 'backbone.net.blocks.10.norm1.weight', 'backbone.net.blocks.10.norm1.bias', 'backbone.net.blocks.10.attn.qkv.weight', 'backbone.net.blocks.10.attn.qkv.bias', 'backbone.net.blocks.10.attn.proj.weight', 'backbone.net.blocks.10.attn.proj.bias', 'backbone.net.blocks.10.norm2.weight', 'backbone.net.blocks.10.norm2.bias', 'backbone.net.blocks.10.mlp.fc1.weight', 'backbone.net.blocks.10.mlp.fc1.bias', 'backbone.net.blocks.10.mlp.fc2.weight', 'backbone.net.blocks.10.mlp.fc2.bias', 'backbone.net.blocks.11.norm1.weight', 'backbone.net.blocks.11.norm1.bias', 'backbone.net.blocks.11.attn.qkv.weight', 'backbone.net.blocks.11.attn.qkv.bias', 'backbone.net.blocks.11.attn.proj.weight', 'backbone.net.blocks.11.attn.proj.bias', 'backbone.net.blocks.11.norm2.weight', 'backbone.net.blocks.11.norm2.bias', 'backbone.net.blocks.11.mlp.fc1.weight', 'backbone.net.blocks.11.mlp.fc1.bias', 'backbone.net.blocks.11.mlp.fc2.weight', 'backbone.net.blocks.11.mlp.fc2.bias', 'backbone.net.norm.weight', 'backbone.net.norm.bias']
incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.weight', 'backbone.net.norm.bias'], incorrect_shapes=[])
WARNING [02/28 06:23:56 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
backbone.net.blocks.0.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.1.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.10.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.11.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.2.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.3.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.4.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.5.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.6.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.7.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.8.attn.{rel_pos_h, rel_pos_w}
backbone.net.blocks.9.attn.{rel_pos_h, rel_pos_w}
backbone.simfp_2.0.{bias, weight}
backbone.simfp_2.1.{bias, weight}
backbone.simfp_2.3.{bias, weight}
backbone.simfp_2.4.norm.{bias, weight}
backbone.simfp_2.4.weight
backbone.simfp_2.5.norm.{bias, weight}
backbone.simfp_2.5.weight
backbone.simfp_3.0.{bias, weight}
backbone.simfp_3.1.norm.{bias, weight}
backbone.simfp_3.1.weight
backbone.simfp_3.2.norm.{bias, weight}
backbone.simfp_3.2.weight
backbone.simfp_4.0.norm.{bias, weight}
backbone.simfp_4.0.weight
backbone.simfp_4.1.norm.{bias, weight}
backbone.simfp_4.1.weight
backbone.simfp_5.1.norm.{bias, weight}
backbone.simfp_5.1.weight
backbone.simfp_5.2.norm.{bias, weight}
backbone.simfp_5.2.weight
proposal_generator.rpn_head.anchor_deltas.{bias, weight}
proposal_generator.rpn_head.conv.conv0.{bias, weight}
proposal_generator.rpn_head.conv.conv1.{bias, weight}
proposal_generator.rpn_head.objectness_logits.{bias, weight}
roi_heads.box_head.conv1.norm.{bias, weight}
roi_heads.box_head.conv1.weight
roi_heads.box_head.conv2.norm.{bias, weight}
roi_heads.box_head.conv2.weight
roi_heads.box_head.conv3.norm.{bias, weight}
roi_heads.box_head.conv3.weight
roi_heads.box_head.conv4.norm.{bias, weight}
roi_heads.box_head.conv4.weight
roi_heads.box_head.fc1.{bias, weight}
roi_heads.box_predictor.bbox_pred.{bias, weight}
roi_heads.box_predictor.cls_score.{bias, weight}
roi_heads.mask_head.deconv.{bias, weight}
roi_heads.mask_head.mask_fcn1.norm.{bias, weight}
roi_heads.mask_head.mask_fcn1.weight
roi_heads.mask_head.mask_fcn2.norm.{bias, weight}
roi_heads.mask_head.mask_fcn2.weight
roi_heads.mask_head.mask_fcn3.norm.{bias, weight}
roi_heads.mask_head.mask_fcn3.weight
roi_heads.mask_head.mask_fcn4.norm.{bias, weight}
roi_heads.mask_head.mask_fcn4.weight
roi_heads.mask_head.predictor.{bias, weight}
WARNING [02/28 06:23:56 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  backbone.net.{cls_token, mask_token}
  backbone.net.norm.{bias, weight}
[02/28 06:23:56 d2.engine.train_loop]: Starting training from iteration 0

incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.weight', 'backbone.net.norm.bias'], incorrect_shapes=[])

incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.weight', 'backbone.net.norm.bias'], incorrect_shapes=[])

incompatible _IncompatibleKeys(missing_keys=['backbone.simfp_2.0.weight', 'backbone.simfp_2.0.bias', 'backbone.simfp_2.1.weight', 'backbone.simfp_2.1.bias', 'backbone.simfp_2.3.weight', 'backbone.simfp_2.3.bias', 'backbone.simfp_2.4.weight', 'backbone.simfp_2.4.norm.weight', 'backbone.simfp_2.4.norm.bias', 'backbone.simfp_2.5.weight', 'backbone.simfp_2.5.norm.weight', 'backbone.simfp_2.5.norm.bias', 'backbone.simfp_3.0.weight', 'backbone.simfp_3.0.bias', 'backbone.simfp_3.1.weight', 'backbone.simfp_3.1.norm.weight', 'backbone.simfp_3.1.norm.bias', 'backbone.simfp_3.2.weight', 'backbone.simfp_3.2.norm.weight', 'backbone.simfp_3.2.norm.bias', 'backbone.simfp_4.0.weight', 'backbone.simfp_4.0.norm.weight', 'backbone.simfp_4.0.norm.bias', 'backbone.simfp_4.1.weight', 'backbone.simfp_4.1.norm.weight', 'backbone.simfp_4.1.norm.bias', 'backbone.simfp_5.1.weight', 'backbone.simfp_5.1.norm.weight', 'backbone.simfp_5.1.norm.bias', 'backbone.simfp_5.2.weight', 'backbone.simfp_5.2.norm.weight', 'backbone.simfp_5.2.norm.bias', 'backbone.net.blocks.0.attn.rel_pos_h', 'backbone.net.blocks.0.attn.rel_pos_w', 'backbone.net.blocks.1.attn.rel_pos_h', 'backbone.net.blocks.1.attn.rel_pos_w', 'backbone.net.blocks.2.attn.rel_pos_h', 'backbone.net.blocks.2.attn.rel_pos_w', 'backbone.net.blocks.3.attn.rel_pos_h', 'backbone.net.blocks.3.attn.rel_pos_w', 'backbone.net.blocks.4.attn.rel_pos_h', 'backbone.net.blocks.4.attn.rel_pos_w', 'backbone.net.blocks.5.attn.rel_pos_h', 'backbone.net.blocks.5.attn.rel_pos_w', 'backbone.net.blocks.6.attn.rel_pos_h', 'backbone.net.blocks.6.attn.rel_pos_w', 'backbone.net.blocks.7.attn.rel_pos_h', 'backbone.net.blocks.7.attn.rel_pos_w', 'backbone.net.blocks.8.attn.rel_pos_h', 'backbone.net.blocks.8.attn.rel_pos_w', 'backbone.net.blocks.9.attn.rel_pos_h', 'backbone.net.blocks.9.attn.rel_pos_w', 'backbone.net.blocks.10.attn.rel_pos_h', 'backbone.net.blocks.10.attn.rel_pos_w', 'backbone.net.blocks.11.attn.rel_pos_h', 'backbone.net.blocks.11.attn.rel_pos_w', 'proposal_generator.rpn_head.conv.conv0.weight', 'proposal_generator.rpn_head.conv.conv0.bias', 'proposal_generator.rpn_head.conv.conv1.weight', 'proposal_generator.rpn_head.conv.conv1.bias', 'proposal_generator.rpn_head.objectness_logits.weight', 'proposal_generator.rpn_head.objectness_logits.bias', 'proposal_generator.rpn_head.anchor_deltas.weight', 'proposal_generator.rpn_head.anchor_deltas.bias', 'roi_heads.box_head.conv1.weight', 'roi_heads.box_head.conv1.norm.weight', 'roi_heads.box_head.conv1.norm.bias', 'roi_heads.box_head.conv2.weight', 'roi_heads.box_head.conv2.norm.weight', 'roi_heads.box_head.conv2.norm.bias', 'roi_heads.box_head.conv3.weight', 'roi_heads.box_head.conv3.norm.weight', 'roi_heads.box_head.conv3.norm.bias', 'roi_heads.box_head.conv4.weight', 'roi_heads.box_head.conv4.norm.weight', 'roi_heads.box_head.conv4.norm.bias', 'roi_heads.box_head.fc1.weight', 'roi_heads.box_head.fc1.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias', 'roi_heads.mask_head.mask_fcn1.weight', 'roi_heads.mask_head.mask_fcn1.norm.weight', 'roi_heads.mask_head.mask_fcn1.norm.bias', 'roi_heads.mask_head.mask_fcn2.weight', 'roi_heads.mask_head.mask_fcn2.norm.weight', 'roi_heads.mask_head.mask_fcn2.norm.bias', 'roi_heads.mask_head.mask_fcn3.weight', 'roi_heads.mask_head.mask_fcn3.norm.weight', 'roi_heads.mask_head.mask_fcn3.norm.bias', 'roi_heads.mask_head.mask_fcn4.weight', 'roi_heads.mask_head.mask_fcn4.norm.weight', 'roi_heads.mask_head.mask_fcn4.norm.bias', 'roi_heads.mask_head.deconv.weight', 'roi_heads.mask_head.deconv.bias', 'roi_heads.mask_head.predictor.weight', 'roi_heads.mask_head.predictor.bias'], unexpected_keys=['backbone.net.cls_token', 'backbone.net.mask_token', 'backbone.net.norm.weight', 'backbone.net.norm.bias'], incorrect_shapes=[])
[02/28 06:24:21 d2.utils.events]:  eta: 1 day, 12:03:31  iter: 19  total_loss: 5.701  loss_cls: 4.056  loss_box_reg: 0.03771  loss_mask: 0.6931  loss_rpn_cls: 0.6889  loss_rpn_loc: 0.2324  time: 0.7068  last_time: 0.6773  data_time: 0.3075  last_data_time: 0.0078   lr: 6.1539e-06  max_mem: 15030M
[02/28 06:24:35 d2.utils.events]:  eta: 1 day, 12:25:39  iter: 39  total_loss: 2.819  loss_cls: 1.106  loss_box_reg: 0.06148  loss_mask: 0.6929  loss_rpn_cls: 0.6748  loss_rpn_loc: 0.2348  time: 0.7131  last_time: 0.7424  data_time: 0.0284  last_data_time: 0.0101   lr: 1.2548e-05  max_mem: 15348M
[02/28 06:24:50 d2.utils.events]:  eta: 1 day, 12:59:29  iter: 59  total_loss: 2.187  loss_cls: 0.4481  loss_box_reg: 0.1735  loss_mask: 0.6924  loss_rpn_cls: 0.619  loss_rpn_loc: 0.2406  time: 0.7246  last_time: 0.7687  data_time: 0.0265  last_data_time: 0.0296   lr: 1.8941e-05  max_mem: 16070M
[02/28 06:25:05 d2.utils.events]:  eta: 1 day, 13:25:58  iter: 79  total_loss: 1.989  loss_cls: 0.4224  loss_box_reg: 0.1943  loss_mask: 0.6915  loss_rpn_cls: 0.4253  loss_rpn_loc: 0.218  time: 0.7304  last_time: 0.7375  data_time: 0.0271  last_data_time: 0.0146   lr: 2.5335e-05  max_mem: 16103M
[02/28 06:25:20 d2.utils.events]:  eta: 1 day, 13:34:15  iter: 99  total_loss: 1.87  loss_cls: 0.4373  loss_box_reg: 0.2219  loss_mask: 0.6901  loss_rpn_cls: 0.3066  loss_rpn_loc: 0.2273  time: 0.7339  last_time: 0.7308  data_time: 0.0211  last_data_time: 0.0364   lr: 3.1728e-05  max_mem: 16103M
[02/28 06:25:35 d2.utils.events]:  eta: 1 day, 13:40:18  iter: 119  total_loss: 1.9  loss_cls: 0.4601  loss_box_reg: 0.2352  loss_mask: 0.6879  loss_rpn_cls: 0.2637  loss_rpn_loc: 0.2608  time: 0.7361  last_time: 0.7493  data_time: 0.0281  last_data_time: 0.0380   lr: 3.8122e-05  max_mem: 16228M
[02/28 06:25:50 d2.utils.events]:  eta: 1 day, 13:48:09  iter: 139  total_loss: 1.797  loss_cls: 0.4366  loss_box_reg: 0.2306  loss_mask: 0.6813  loss_rpn_cls: 0.2158  loss_rpn_loc: 0.2238  time: 0.7386  last_time: 0.7559  data_time: 0.0375  last_data_time: 0.0594   lr: 4.4516e-05  max_mem: 16228M
[02/28 06:26:05 d2.utils.events]:  eta: 1 day, 13:44:43  iter: 159  total_loss: 1.628  loss_cls: 0.3802  loss_box_reg: 0.196  loss_mask: 0.6701  loss_rpn_cls: 0.1959  loss_rpn_loc: 0.1893  time: 0.7386  last_time: 0.7346  data_time: 0.0211  last_data_time: 0.0117   lr: 5.0909e-05  max_mem: 16228M
[02/28 06:26:19 d2.utils.events]:  eta: 1 day, 13:43:59  iter: 179  total_loss: 1.465  loss_cls: 0.3169  loss_box_reg: 0.1468  loss_mask: 0.6506  loss_rpn_cls: 0.1772  loss_rpn_loc: 0.1823  time: 0.7378  last_time: 0.6996  data_time: 0.0416  last_data_time: 0.0100   lr: 5.7303e-05  max_mem: 16228M
[02/28 06:26:34 d2.utils.events]:  eta: 1 day, 13:41:42  iter: 199  total_loss: 1.606  loss_cls: 0.3755  loss_box_reg: 0.1961  loss_mask: 0.6302  loss_rpn_cls: 0.179  loss_rpn_loc: 0.2081  time: 0.7375  last_time: 0.7260  data_time: 0.0306  last_data_time: 0.0089   lr: 6.3696e-05  max_mem: 16263M
[02/28 06:26:49 d2.utils.events]:  eta: 1 day, 13:41:38  iter: 219  total_loss: 1.525  loss_cls: 0.3732  loss_box_reg: 0.1838  loss_mask: 0.5999  loss_rpn_cls: 0.171  loss_rpn_loc: 0.1954  time: 0.7381  last_time: 0.7195  data_time: 0.0393  last_data_time: 0.0073   lr: 7.009e-05  max_mem: 16263M
[02/28 06:27:04 d2.utils.events]:  eta: 1 day, 13:42:29  iter: 239  total_loss: 1.576  loss_cls: 0.3947  loss_box_reg: 0.2172  loss_mask: 0.5935  loss_rpn_cls: 0.175  loss_rpn_loc: 0.2093  time: 0.7385  last_time: 0.7287  data_time: 0.0304  last_data_time: 0.0229   lr: 7.6484e-05  max_mem: 16263M
[02/28 06:27:19 d2.utils.events]:  eta: 1 day, 13:44:11  iter: 259  total_loss: 1.533  loss_cls: 0.4007  loss_box_reg: 0.2232  loss_mask: 0.5701  loss_rpn_cls: 0.1385  loss_rpn_loc: 0.1604  time: 0.7394  last_time: 0.7827  data_time: 0.0524  last_data_time: 0.1698   lr: 8e-05  max_mem: 16424M
[02/28 06:27:34 d2.utils.events]:  eta: 1 day, 13:44:51  iter: 279  total_loss: 1.528  loss_cls: 0.4064  loss_box_reg: 0.2148  loss_mask: 0.5623  loss_rpn_cls: 0.1485  loss_rpn_loc: 0.1975  time: 0.7399  last_time: 0.7153  data_time: 0.0245  last_data_time: 0.0071   lr: 8e-05  max_mem: 16424M
[02/28 06:27:49 d2.utils.events]:  eta: 1 day, 13:45:47  iter: 299  total_loss: 1.47  loss_cls: 0.3784  loss_box_reg: 0.2082  loss_mask: 0.5638  loss_rpn_cls: 0.1423  loss_rpn_loc: 0.1753  time: 0.7400  last_time: 0.7306  data_time: 0.0294  last_data_time: 0.0277   lr: 8e-05  max_mem: 16518M
[02/28 06:28:04 d2.utils.events]:  eta: 1 day, 13:48:23  iter: 319  total_loss: 1.576  loss_cls: 0.4042  loss_box_reg: 0.2294  loss_mask: 0.5603  loss_rpn_cls: 0.1478  loss_rpn_loc: 0.2051  time: 0.7406  last_time: 0.7837  data_time: 0.0283  last_data_time: 0.1010   lr: 8e-05  max_mem: 16518M
[02/28 06:28:19 d2.utils.events]:  eta: 1 day, 13:49:50  iter: 339  total_loss: 1.567  loss_cls: 0.3963  loss_box_reg: 0.2262  loss_mask: 0.5501  loss_rpn_cls: 0.1541  loss_rpn_loc: 0.198  time: 0.7406  last_time: 0.7431  data_time: 0.0335  last_data_time: 0.0392   lr: 8e-05  max_mem: 16518M
[02/28 06:28:34 d2.utils.events]:  eta: 1 day, 13:52:16  iter: 359  total_loss: 1.522  loss_cls: 0.3821  loss_box_reg: 0.2397  loss_mask: 0.539  loss_rpn_cls: 0.1515  loss_rpn_loc: 0.2389  time: 0.7414  last_time: 0.7387  data_time: 0.0414  last_data_time: 0.0090   lr: 8e-05  max_mem: 16518M
[02/28 06:28:49 d2.utils.events]:  eta: 1 day, 13:53:06  iter: 379  total_loss: 1.509  loss_cls: 0.3902  loss_box_reg: 0.2351  loss_mask: 0.5464  loss_rpn_cls: 0.1395  loss_rpn_loc: 0.1652  time: 0.7419  last_time: 0.7531  data_time: 0.0240  last_data_time: 0.0308   lr: 8e-05  max_mem: 16518M
[02/28 06:29:04 d2.utils.events]:  eta: 1 day, 13:53:21  iter: 399  total_loss: 1.544  loss_cls: 0.411  loss_box_reg: 0.2401  loss_mask: 0.5245  loss_rpn_cls: 0.154  loss_rpn_loc: 0.1769  time: 0.7422  last_time: 0.7739  data_time: 0.0327  last_data_time: 0.0110   lr: 8e-05  max_mem: 16518M
[02/28 06:29:19 d2.utils.events]:  eta: 1 day, 13:54:25  iter: 419  total_loss: 1.556  loss_cls: 0.4074  loss_box_reg: 0.2483  loss_mask: 0.5129  loss_rpn_cls: 0.1441  loss_rpn_loc: 0.2066  time: 0.7424  last_time: 0.7880  data_time: 0.0194  last_data_time: 0.0484   lr: 8e-05  max_mem: 16518M
[02/28 06:29:34 d2.utils.events]:  eta: 1 day, 13:54:29  iter: 439  total_loss: 1.458  loss_cls: 0.4185  loss_box_reg: 0.2634  loss_mask: 0.506  loss_rpn_cls: 0.1326  loss_rpn_loc: 0.16  time: 0.7427  last_time: 0.7030  data_time: 0.0247  last_data_time: 0.0492   lr: 8e-05  max_mem: 16518M
[02/28 06:29:48 d2.utils.events]:  eta: 1 day, 13:53:39  iter: 459  total_loss: 1.452  loss_cls: 0.4107  loss_box_reg: 0.265  loss_mask: 0.5109  loss_rpn_cls: 0.1387  loss_rpn_loc: 0.1827  time: 0.7422  last_time: 0.7166  data_time: 0.0201  last_data_time: 0.0312   lr: 8e-05  max_mem: 16518M
[02/28 06:30:03 d2.utils.events]:  eta: 1 day, 13:53:25  iter: 479  total_loss: 1.451  loss_cls: 0.3674  loss_box_reg: 0.2342  loss_mask: 0.5054  loss_rpn_cls: 0.1392  loss_rpn_loc: 0.1731  time: 0.7421  last_time: 0.7431  data_time: 0.0265  last_data_time: 0.0212   lr: 8e-05  max_mem: 16518M
[02/28 06:30:18 d2.utils.events]:  eta: 1 day, 13:54:17  iter: 499  total_loss: 1.498  loss_cls: 0.3841  loss_box_reg: 0.2447  loss_mask: 0.492  loss_rpn_cls: 0.1433  loss_rpn_loc: 0.1711  time: 0.7424  last_time: 0.7253  data_time: 0.0362  last_data_time: 0.0483   lr: 8e-05  max_mem: 16518M
[02/28 06:30:33 d2.utils.events]:  eta: 1 day, 13:53:44  iter: 519  total_loss: 1.45  loss_cls: 0.3931  loss_box_reg: 0.257  loss_mask: 0.5031  loss_rpn_cls: 0.1222  loss_rpn_loc: 0.1676  time: 0.7424  last_time: 0.7600  data_time: 0.0279  last_data_time: 0.0198   lr: 8e-05  max_mem: 16518M
[02/28 06:30:48 d2.utils.events]:  eta: 1 day, 13:53:47  iter: 539  total_loss: 1.416  loss_cls: 0.3751  loss_box_reg: 0.2467  loss_mask: 0.4925  loss_rpn_cls: 0.1373  loss_rpn_loc: 0.1943  time: 0.7426  last_time: 0.7156  data_time: 0.0356  last_data_time: 0.0147   lr: 8e-05  max_mem: 16518M
[02/28 06:31:03 d2.utils.events]:  eta: 1 day, 13:54:50  iter: 559  total_loss: 1.501  loss_cls: 0.4076  loss_box_reg: 0.2773  loss_mask: 0.5015  loss_rpn_cls: 0.1346  loss_rpn_loc: 0.1844  time: 0.7431  last_time: 0.7632  data_time: 0.0413  last_data_time: 0.0493   lr: 8e-05  max_mem: 17110M
[02/28 06:31:18 d2.utils.events]:  eta: 1 day, 13:55:17  iter: 579  total_loss: 1.381  loss_cls: 0.3541  loss_box_reg: 0.231  loss_mask: 0.4709  loss_rpn_cls: 0.1373  loss_rpn_loc: 0.1744  time: 0.7436  last_time: 0.7743  data_time: 0.0320  last_data_time: 0.0562   lr: 8e-05  max_mem: 17110M
[02/28 06:31:33 d2.utils.events]:  eta: 1 day, 13:55:40  iter: 599  total_loss: 1.449  loss_cls: 0.3994  loss_box_reg: 0.263  loss_mask: 0.493  loss_rpn_cls: 0.1286  loss_rpn_loc: 0.1858  time: 0.7441  last_time: 0.7460  data_time: 0.0365  last_data_time: 0.0223   lr: 8e-05  max_mem: 17110M
[02/28 06:31:48 d2.utils.events]:  eta: 1 day, 13:55:25  iter: 619  total_loss: 1.381  loss_cls: 0.3488  loss_box_reg: 0.2288  loss_mask: 0.4716  loss_rpn_cls: 0.1236  loss_rpn_loc: 0.179  time: 0.7442  last_time: 0.7463  data_time: 0.0288  last_data_time: 0.0108   lr: 8e-05  max_mem: 17110M
[02/28 06:32:03 d2.utils.events]:  eta: 1 day, 13:55:10  iter: 639  total_loss: 1.424  loss_cls: 0.3569  loss_box_reg: 0.2348  loss_mask: 0.4958  loss_rpn_cls: 0.1253  loss_rpn_loc: 0.2099  time: 0.7444  last_time: 0.7213  data_time: 0.0257  last_data_time: 0.0441   lr: 8e-05  max_mem: 17110M
[02/28 06:32:19 d2.utils.events]:  eta: 1 day, 13:55:38  iter: 659  total_loss: 1.437  loss_cls: 0.3535  loss_box_reg: 0.2433  loss_mask: 0.4821  loss_rpn_cls: 0.1325  loss_rpn_loc: 0.1981  time: 0.7448  last_time: 0.7895  data_time: 0.0436  last_data_time: 0.0342   lr: 8e-05  max_mem: 17110M
[02/28 06:32:34 d2.utils.events]:  eta: 1 day, 13:56:32  iter: 679  total_loss: 1.447  loss_cls: 0.3855  loss_box_reg: 0.2633  loss_mask: 0.473  loss_rpn_cls: 0.1276  loss_rpn_loc: 0.1992  time: 0.7454  last_time: 0.7976  data_time: 0.0356  last_data_time: 0.0226   lr: 8e-05  max_mem: 17110M
[02/28 06:32:49 d2.utils.events]:  eta: 1 day, 13:58:13  iter: 699  total_loss: 1.457  loss_cls: 0.3831  loss_box_reg: 0.2621  loss_mask: 0.4752  loss_rpn_cls: 0.1333  loss_rpn_loc: 0.1874  time: 0.7458  last_time: 0.7416  data_time: 0.0461  last_data_time: 0.0189   lr: 8e-05  max_mem: 17110M
[02/28 06:33:04 d2.utils.events]:  eta: 1 day, 13:57:29  iter: 719  total_loss: 1.342  loss_cls: 0.3573  loss_box_reg: 0.2304  loss_mask: 0.4707  loss_rpn_cls: 0.1112  loss_rpn_loc: 0.1815  time: 0.7457  last_time: 0.7566  data_time: 0.0288  last_data_time: 0.0239   lr: 8e-05  max_mem: 17110M
[02/28 06:33:19 d2.utils.events]:  eta: 1 day, 13:58:03  iter: 739  total_loss: 1.436  loss_cls: 0.3796  loss_box_reg: 0.2542  loss_mask: 0.4728  loss_rpn_cls: 0.1146  loss_rpn_loc: 0.1828  time: 0.7460  last_time: 0.7544  data_time: 0.0360  last_data_time: 0.0576   lr: 8e-05  max_mem: 17110M
[02/28 06:33:35 d2.utils.events]:  eta: 1 day, 13:59:55  iter: 759  total_loss: 1.45  loss_cls: 0.4055  loss_box_reg: 0.2723  loss_mask: 0.4635  loss_rpn_cls: 0.1246  loss_rpn_loc: 0.1569  time: 0.7465  last_time: 0.7851  data_time: 0.0329  last_data_time: 0.0380   lr: 8e-05  max_mem: 17110M
[02/28 06:33:50 d2.utils.events]:  eta: 1 day, 14:00:12  iter: 779  total_loss: 1.421  loss_cls: 0.3582  loss_box_reg: 0.2471  loss_mask: 0.4616  loss_rpn_cls: 0.1225  loss_rpn_loc: 0.1758  time: 0.7467  last_time: 0.7493  data_time: 0.0450  last_data_time: 0.0185   lr: 8e-05  max_mem: 17110M
[02/28 06:34:05 d2.utils.events]:  eta: 1 day, 14:02:27  iter: 799  total_loss: 1.377  loss_cls: 0.3756  loss_box_reg: 0.2505  loss_mask: 0.4592  loss_rpn_cls: 0.1201  loss_rpn_loc: 0.156  time: 0.7472  last_time: 0.7783  data_time: 0.0497  last_data_time: 0.0208   lr: 8e-05  max_mem: 17110M
[02/28 06:34:20 d2.utils.events]:  eta: 1 day, 14:03:14  iter: 819  total_loss: 1.407  loss_cls: 0.3929  loss_box_reg: 0.277  loss_mask: 0.454  loss_rpn_cls: 0.1251  loss_rpn_loc: 0.1764  time: 0.7477  last_time: 0.7553  data_time: 0.0417  last_data_time: 0.0498   lr: 8e-05  max_mem: 17110M
[02/28 06:34:36 d2.utils.events]:  eta: 1 day, 14:03:37  iter: 839  total_loss: 1.403  loss_cls: 0.3783  loss_box_reg: 0.261  loss_mask: 0.4632  loss_rpn_cls: 0.1205  loss_rpn_loc: 0.1894  time: 0.7480  last_time: 0.7034  data_time: 0.0379  last_data_time: 0.0229   lr: 8e-05  max_mem: 17110M
[02/28 06:34:51 d2.utils.events]:  eta: 1 day, 14:03:29  iter: 859  total_loss: 1.415  loss_cls: 0.3866  loss_box_reg: 0.2684  loss_mask: 0.4577  loss_rpn_cls: 0.1139  loss_rpn_loc: 0.1779  time: 0.7482  last_time: 0.7606  data_time: 0.0348  last_data_time: 0.0502   lr: 8e-05  max_mem: 17110M
[02/28 06:35:06 d2.utils.events]:  eta: 1 day, 14:04:02  iter: 879  total_loss: 1.344  loss_cls: 0.3615  loss_box_reg: 0.2392  loss_mask: 0.447  loss_rpn_cls: 0.1088  loss_rpn_loc: 0.1564  time: 0.7486  last_time: 0.7718  data_time: 0.0318  last_data_time: 0.0111   lr: 8e-05  max_mem: 17110M
[02/28 06:35:21 d2.utils.events]:  eta: 1 day, 14:05:46  iter: 899  total_loss: 1.418  loss_cls: 0.3725  loss_box_reg: 0.2665  loss_mask: 0.4652  loss_rpn_cls: 0.1135  loss_rpn_loc: 0.1616  time: 0.7490  last_time: 0.7658  data_time: 0.0362  last_data_time: 0.0142   lr: 8e-05  max_mem: 17110M
[02/28 06:35:36 d2.utils.events]:  eta: 1 day, 14:05:53  iter: 919  total_loss: 1.353  loss_cls: 0.3552  loss_box_reg: 0.2437  loss_mask: 0.4625  loss_rpn_cls: 0.1108  loss_rpn_loc: 0.1572  time: 0.7491  last_time: 0.7912  data_time: 0.0415  last_data_time: 0.0148   lr: 8e-05  max_mem: 17110M
[02/28 06:35:52 d2.utils.events]:  eta: 1 day, 14:06:37  iter: 939  total_loss: 1.409  loss_cls: 0.381  loss_box_reg: 0.2623  loss_mask: 0.4401  loss_rpn_cls: 0.1195  loss_rpn_loc: 0.1888  time: 0.7494  last_time: 0.7735  data_time: 0.0371  last_data_time: 0.0180   lr: 8e-05  max_mem: 17110M
[02/28 06:36:07 d2.utils.events]:  eta: 1 day, 14:08:18  iter: 959  total_loss: 1.422  loss_cls: 0.3699  loss_box_reg: 0.2685  loss_mask: 0.4495  loss_rpn_cls: 0.1316  loss_rpn_loc: 0.1875  time: 0.7496  last_time: 0.7191  data_time: 0.0324  last_data_time: 0.0182   lr: 8e-05  max_mem: 17110M
[02/28 06:36:22 d2.utils.events]:  eta: 1 day, 14:08:37  iter: 979  total_loss: 1.347  loss_cls: 0.3774  loss_box_reg: 0.2718  loss_mask: 0.4425  loss_rpn_cls: 0.1138  loss_rpn_loc: 0.1736  time: 0.7498  last_time: 0.7995  data_time: 0.0270  last_data_time: 0.0591   lr: 8e-05  max_mem: 17110M
[02/28 06:36:37 d2.utils.events]:  eta: 1 day, 14:08:17  iter: 999  total_loss: 1.344  loss_cls: 0.3815  loss_box_reg: 0.2715  loss_mask: 0.4528  loss_rpn_cls: 0.1021  loss_rpn_loc: 0.1439  time: 0.7497  last_time: 0.7454  data_time: 0.0313  last_data_time: 0.0169   lr: 8e-05  max_mem: 17110M
[02/28 06:36:53 d2.utils.events]:  eta: 1 day, 14:10:39  iter: 1019  total_loss: 1.313  loss_cls: 0.3639  loss_box_reg: 0.2601  loss_mask: 0.4396  loss_rpn_cls: 0.1195  loss_rpn_loc: 0.1785  time: 0.7502  last_time: 0.8255  data_time: 0.0321  last_data_time: 0.0511   lr: 8e-05  max_mem: 17110M
[02/28 06:37:08 d2.utils.events]:  eta: 1 day, 14:11:57  iter: 1039  total_loss: 1.35  loss_cls: 0.3641  loss_box_reg: 0.2761  loss_mask: 0.4423  loss_rpn_cls: 0.1151  loss_rpn_loc: 0.1456  time: 0.7503  last_time: 0.7229  data_time: 0.0263  last_data_time: 0.0123   lr: 8e-05  max_mem: 17110M
[02/28 06:37:23 d2.utils.events]:  eta: 1 day, 14:12:01  iter: 1059  total_loss: 1.353  loss_cls: 0.357  loss_box_reg: 0.2608  loss_mask: 0.4461  loss_rpn_cls: 0.09575  loss_rpn_loc: 0.1539  time: 0.7503  last_time: 0.7357  data_time: 0.0311  last_data_time: 0.0258   lr: 8e-05  max_mem: 17110M
[02/28 06:37:38 d2.utils.events]:  eta: 1 day, 14:12:33  iter: 1079  total_loss: 1.293  loss_cls: 0.3775  loss_box_reg: 0.2622  loss_mask: 0.4453  loss_rpn_cls: 0.1067  loss_rpn_loc: 0.1574  time: 0.7505  last_time: 0.7238  data_time: 0.0282  last_data_time: 0.0363   lr: 8e-05  max_mem: 17156M
[02/28 06:37:53 d2.utils.events]:  eta: 1 day, 14:13:06  iter: 1099  total_loss: 1.308  loss_cls: 0.3581  loss_box_reg: 0.2505  loss_mask: 0.4412  loss_rpn_cls: 0.1158  loss_rpn_loc: 0.1491  time: 0.7505  last_time: 0.7606  data_time: 0.0257  last_data_time: 0.0214   lr: 8e-05  max_mem: 17156M
[02/28 06:38:08 d2.utils.events]:  eta: 1 day, 14:13:09  iter: 1119  total_loss: 1.381  loss_cls: 0.3674  loss_box_reg: 0.2616  loss_mask: 0.4418  loss_rpn_cls: 0.1196  loss_rpn_loc: 0.173  time: 0.7507  last_time: 0.7476  data_time: 0.0349  last_data_time: 0.0319   lr: 8e-05  max_mem: 17156M
srun: job 316553 queued and waiting for resources
srun: job 316553 has been allocated resources
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:57709 (errno: 1 - Operation not permitted).
[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:57709 (errno: 1 - Operation not permitted).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
Process group URL: tcp://127.0.0.1:57709
Traceback (most recent call last):
  File "tools/lazyconfig_train_net.py", line 131, in <module>
    args=(args,),
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 81, in launch
    daemon=False,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 112, in _distributed_worker
    raise e
  File "/srv/home/pmorgado/yibing/detectron2/detectron2/engine/launch.py", line 107, in _distributed_worker
    timeout=timeout,
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 754, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/distributed/rendezvous.py", line 201, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/srv/home/pmorgado/anaconda3/envs/pt113/lib/python3.7/site-packages/torch/distributed/rendezvous.py", line 178, in _create_c10d_store
    hostname, port, world_size, start_daemon, timeout, multi_tenant=True
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:57709 (errno: 1 - Operation not permitted). The server socket has failed to bind to 0.0.0.0:57709 (errno: 1 - Operation not permitted).

srun: error: euler22: task 0: Exited with exit code 1
[02/28 06:38:23 d2.utils.events]:  eta: 1 day, 14:12:54  iter: 1139  total_loss: 1.35  loss_cls: 0.3676  loss_box_reg: 0.2573  loss_mask: 0.4452  loss_rpn_cls: 0.1139  loss_rpn_loc: 0.1716  time: 0.7509  last_time: 0.7510  data_time: 0.0290  last_data_time: 0.0210   lr: 8e-05  max_mem: 17156M
[02/28 06:38:39 d2.utils.events]:  eta: 1 day, 14:13:45  iter: 1159  total_loss: 1.388  loss_cls: 0.4006  loss_box_reg: 0.2732  loss_mask: 0.437  loss_rpn_cls: 0.1145  loss_rpn_loc: 0.1829  time: 0.7510  last_time: 0.7819  data_time: 0.0339  last_data_time: 0.1373   lr: 8e-05  max_mem: 17156M
[02/28 06:38:54 d2.utils.events]:  eta: 1 day, 14:14:26  iter: 1179  total_loss: 1.455  loss_cls: 0.3687  loss_box_reg: 0.2703  loss_mask: 0.449  loss_rpn_cls: 0.1196  loss_rpn_loc: 0.1967  time: 0.7510  last_time: 0.7652  data_time: 0.0344  last_data_time: 0.0703   lr: 8e-05  max_mem: 17156M
[02/28 06:39:09 d2.utils.events]:  eta: 1 day, 14:15:01  iter: 1199  total_loss: 1.359  loss_cls: 0.3313  loss_box_reg: 0.2387  loss_mask: 0.4306  loss_rpn_cls: 0.1167  loss_rpn_loc: 0.1749  time: 0.7511  last_time: 0.7710  data_time: 0.0384  last_data_time: 0.0198   lr: 8e-05  max_mem: 17156M
[02/28 06:39:24 d2.utils.events]:  eta: 1 day, 14:14:48  iter: 1219  total_loss: 1.323  loss_cls: 0.3495  loss_box_reg: 0.2535  loss_mask: 0.4437  loss_rpn_cls: 0.09774  loss_rpn_loc: 0.1556  time: 0.7511  last_time: 0.7559  data_time: 0.0296  last_data_time: 0.0230   lr: 8e-05  max_mem: 17156M
[02/28 06:39:39 d2.utils.events]:  eta: 1 day, 14:15:55  iter: 1239  total_loss: 1.291  loss_cls: 0.3334  loss_box_reg: 0.2218  loss_mask: 0.4249  loss_rpn_cls: 0.1229  loss_rpn_loc: 0.1726  time: 0.7512  last_time: 0.7647  data_time: 0.0377  last_data_time: 0.0221   lr: 8e-05  max_mem: 17156M
srun: forcing job termination
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
slurmstepd: error: *** STEP 316548.0 ON euler22 CANCELLED AT 2023-02-28T06:39:44 ***
srun: error: euler22: task 0: Killed
srun: job 316597 queued and waiting for resources
